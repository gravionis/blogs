<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/content/posts/ai/llm_strategies_and_dps.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/ai/llm_strategies_and_dps.md" />
              <option name="originalContent" value="+++&#10;date = '2025-05-20T12:44:47+10:00'&#10;draft = false&#10;title = 'LLM Strategies and Design Patterns'&#10;tags = ['LLM', 'Design Patterns']&#10;+++&#10;&#10;LLM design patterns are reusable strategies for building robust, efficient, and scalable AI applications. They help developers structure retrieval, reading, rewriting, memory, agent, and orchestration workflows for large language models. These patterns improve performance, maintainability accuracy, cost and security.&#10;&#10;## Table of Contents&#10;1. [Introduction](#introduction)&#10;2. [Concepts](#concepts)&#10;    1. [LLMs](#llms)&#10;    2. [Transformer Architecture](#transformer-architecture)&#10;    3. [Tokens](#tokens)&#10;    4. [LLM Model Variants and Techniques](#llm-model-variants-and-techniques)&#10;    5. [Chat Models](#chat-models)&#10;    6. [Model Parameters](#model-paramaters)&#10;    7. [Popular LLM Tools and Frameworks](#popular-llm-tools-and-frameworks)&#10;    8. [Prompt](#prompt)&#10;    9. [Prompt Engineering and Patterns](#prompt-engineering-and-patterns)&#10;    10. [Roles](#roles)&#10;    11. [Structured Output](#structured-output)&#10;    12. [Common Issues](#common-issues)&#10;    13. [Output &amp; Performance](#output--performance)&#10;    14. [Embeddings](#embeddings)&#10;    15. [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)&#10;    16. [Vector Store](#vector-store)&#10;    17. [Vector Stores and Embeddings: Problems Solved &amp; Techniques](#vector-stores-and-embeddings-problems-solved--techniques)&#10;3. [Strategies for Loading Documents into Chat Model Prompts](#strategies-for-loading-documents-into-chat-model-prompts)&#10;    1. [Current Knowledge Base](#1-current-knowledge-base)&#10;    2. [Representing Information](#2-representing-information)&#10;4. [Strategies for Splitting Text into Meaningful Chunks](#strategies-for-splitting-text-into-meaningful-chunks)&#10;    1. [Chunking with Overlap](#1-chunking-with-overlap)&#10;    2. [Language or Format-Aware Chunking](#2-language-or-format-aware-chunking)&#10;    3. [Maintaining References](#3-maintaining-references)&#10;    4. [Using PgVector with Embedding Models](#using-pgvector-with-embedding-models)&#10;    5. [Vector DB Issues](#vector-db-issues)&#10;5. [Vector Stores](#vector-stores)&#10;6. [Patterns](#patterns)&#10;    1. [MultiVector Retrieval](#multivector-retrieval)&#10;    2. [Strategies for Mathematical Operations](#strategies-for-mathematical-operations)&#10;    3. [Strategies for Security Compliance](#strategies-for-security-compliance)&#10;    4. [Recursive Abstractive Processing for Tree-Organized Retrieval (RAPTOR)](#recursive-abstractive-processing-for-tree-organized-retrieval-raptor)&#10;    5. [ColBERT: Optimizing Embeddings](#colbert-optimizing-embeddings)&#10;    6. [Effective Strategies Using RAG](#effective-strategies-using-rag)&#10;    7. [Query Transformation](#query-transformation)&#10;    8. [Step-Back Prompting](#step-back-prompting)&#10;    9. [Subquestion Prompting](#subquestion-prompting)&#10;    10. [Rewritten Prompting aka Rewrite-Retrieve-Read](#rewritten-prompting-aka-rewrite-retrieve-read)&#10;    11. [Multi-Query Retrieval](#multi-query-retrieval)&#10;    12. [RAG-Fusion](#rag-fusion)&#10;    13. [Hypothetical Document Embeddings (HyDE)](#hypothetical-document-embeddings-hyde)&#10;    14. [Query Routing](#query-routing)&#10;    15. [Query Construction](#query-construction)&#10;        - [Text-to-Metadata Conversion](#1-text-to-metadata-conversion)&#10;        - [Text-to-SQL](#2-text-to-sql)&#10;        - [Summary](#summary)&#10;        &#10;## Introduction&#10;Instruction were to keep it basic &amp;  AI WG experiences 3 usecases are in progress. we will be sharing some basics wrt to the learnings as strategies or design patterns here. if haven't already will go through.&#10;* security arch approvals&#10;* cost&#10;* availablility of an appropriate model &#10;* model support for context size&#10;* amount of data in the knowledge base.&#10;* format of the data (e.g table, plantuml xmls)&#10;* challenges with the quality of context data.&#10;* Speed of models&#10;&#10;Before we proceed setting the stage.&#10;## Concepts &#10;### LLMs&#10;- GPT-3: 175B Parameters, ~350B tokens. GPT-4:is 6x bigger. GPT-5 was supposed to be 20x bigger but is counter intuitive 300B Parameters.&#10;- Chinchilla's Law: model performance optimal when training tokens ≈ 20 × model parameters.&#10;    - Example: 70B parameters → ~1.4T tokens needed.&#10;    - hundreds of billions/trillions of parameters requires enormous data more than available high-quality text.&#10;  - GPT-5 leverages on many architectural innovations like:&#10;    - Unified Intelligence System: integrates multiple reasoning and perception modules for more general intelligence.&#10;    - Mixture-of-Experts: routes inputs to specialized sub-models for improved efficiency and accuracy.&#10;    - Multimodal Integration: processes and understands text, images, and other data types together.&#10;- Sometimes we have bigger models but often we have restrictions using best available model due reasons mentioned before. Hence we have to compensate with design patterns. we will be going to few models available to atleast the Fintech and how do we get past some of the common issues.&#10;&#10;### Transformer Architecture: &#10;  - LLMs use the transformer neural network architecture.&#10;  - Transformers understand relationships between words, regardless of their position in the text.&#10;  - The architecture enables prediction of the next word in a sequence. that is to say they should not expected to perform for accurate mathematical computation; lot of our AI usecases some have some math involved.&#10;&#10;### Tokens:&#10;  - A token is a chunk of text, it could be entire word or part.&#10;  - Roughly, one token is about 4 characters or for calculation sake 75% of  words in English language.&#10;  - why important? - for us to estimate monthly cost: (tokens used per month) × (API cost per token).&#10;  - If transaction rate is T tokens/sec, that's about (T × 0.75) words/sec. (include ss)&#10;&#10;### LLM Model Variants and Techniques:&#10;  - Two main techniques:&#10;    - Predict next word (causal language modeling): Used by models like GPT; generates text by predicting the next token in a sequence.&#10;    - Predict masked word: Used by models like BERT; predicts masked tokens within a sentence for better understanding of context.&#10;  - GPT models: Focus on text generation, conversation, and completion tasks.&#10;  - BERT models: Focus on understanding, classification, and extracting information from text.&#10;  - Other flavours:&#10;    - Encoder-only (BERT): Good for understanding and classification.&#10;    - Decoder-only (GPT): Good for generation and completion.&#10;    - Encoder-decoder (T5, BART): Good for translation, summarization, and more complex tasks.&#10;  - Many LLMs are specialized for tasks like code generation, multimodal input (text + images), or domain-specific knowledge.&#10;&#10;&#10;### Chat Models&#10;- Chat models are specialized LLMs designed for conversational interactions.&#10;&#10;| Model                                                                                                        | Provider       | Strengths                                   | Typical Use Cases                                 |&#10;|--------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------|---------------------------------------------------|&#10;| Copilot                                                                                        |  Copilot utilizes Mixture-of-Experts - uses variety of AI models to provide coding assistance, adapting to different tasks and user needs. OpenAI and Microsoft's property models and tools      | Text generation, conversation, code, summarization | Chatbots, assistants, writing, code, content      |&#10;| LLaMA                                                                                                        | Meta           | Open-source, efficient, adaptable           | Research, custom training, open-source apps       |&#10;| Claude                                                                                                       | Anthropic      | Safety-focused, reliable, conversational    | Support, safe chatbots, document analysis         |&#10;| Mistral                                                                                                      | Mistral AI     | Kind of something from all aspects - Efficient, open weights, strong benchmarks  | Research, production NLP, open-source projects    |&#10;| Gemini                                                                                                       | Google DeepMind | Multimodal (text, image, audio), reasoning  | Multimodal assistants, content analysis           |&#10;| Anthropic (Claude), Amazon (Titan), Meta (Llama),&lt;br/&gt;Mistral, AI21 Labs (Jurassic), Cohere (Command) | Amazon Bedrock | Wide model selection, enterprise integration, scalable APIs | Chatbots, search, summarization, enterprise AI    |&#10;&#10;&#10;### Model Paramaters&#10;| Parameter            | Description                                                                                     | Typical Range        | Impact                                                                 |&#10;|----------------------|-------------------------------------------------------------------------------------------------|----------------------|------------------------------------------------------------------------|&#10;| **temperature**      | Controls randomness. Lower = deterministic, higher = creative/unpredictable.                  | `0.0` – `2.0`       | `0.0` = deterministic, `1.0` = balanced, `&gt;1.2` = very random.        |&#10;| **top_p**            | Nucleus sampling. keep adding tokens from smallest set until the cumulative probability ≥ p.           | `0.1` – `1.0`       | Lower = more focused, 1.0 = no restriction.                           |&#10;| **top_k**            | Limits token selection to the k most probable tokens at each step.   | `1` – `100`         | Low values reduce creativity, high values allow more variation.       |&#10;| **max_tokens**       | Maximum tokens to generate in the response.                                                   | Depends on model    | Controls output length.                                               |&#10;| **min_tokens**       | Minimum tokens to generate before stopping.                                                   | Depends on model    | Ensures longer answers if needed.                                     |&#10;| **presence_penalty** | Penalizes tokens already present in the text (encourages new topics).                          | `-2.0` – `2.0`      | Higher = more diversity.                                              |&#10;| **frequency_penalty**| Penalizes frequent tokens to reduce repetition.                                               | `-2.0` – `2.0`      | Higher = less repetition.                                             |&#10;| **stop**             | List of stop sequences where generation should halt.                                          | Strings / tokens    | Useful for structured responses.                                      |&#10;| **seed**             | Fixes randomness for reproducibility.                                                         | Integer             | Same seed = same output (with sampling).                              |&#10;| **length_penalty**   | Adjusts likelihood for longer sequences in beam search.                                       | `&gt;0`                | Higher = longer outputs favored.                                      |&#10;| **early_stopping**   | Stops beam search when best candidates are found.                                             | `true` / `false`    | Speeds up generation, may miss better completions.                    |&#10;| **logit_bias**       | dictionary to control probability of certain token ids   | Useful for forcing or avoiding words.                                 |&#10;| **echo**             | Return the prompt along with the completion.                                                  | `true` / `false`    | For debugging or prompt reconstruction.                                |&#10;&#10;#### Top p&#10;&#10;| Token | Probability |&#10;|-------|------------|&#10;| the   | 0.4        |&#10;| a     | 0.3        |&#10;| an    | 0.1        |&#10;| dog   | 0.05       |&#10;| ...   | ...        |&#10;&#10;If `p = 0.8`:&#10;- Cumulative probabilities:&#10;  - `&quot;the&quot; (0.4) + &quot;a&quot; (0.3) + &quot;an&quot; (0.1) = 0.8`&#10;- Nucleus = `{&quot;the&quot;, &quot;a&quot;, &quot;an&quot;}`&#10;- Next token is sampled **only** from this set.&#10;&#10;#### Top k &#10;&#10;Suppose the token probabilities are:&#10;&#10;| Token | Probability |&#10;|-------|------------|&#10;| the   | 0.4        |&#10;| a     | 0.3        |&#10;| an    | 0.1        |&#10;| dog   | 0.05       |&#10;| ...   | ...        |&#10;&#10;If `k = 3`:&#10;&#10;- Top 3 tokens = `{&quot;the&quot;, &quot;a&quot;, &quot;an&quot;}`&#10;- Next token is sampled **only** from this set.&#10;&#10;### Popular LLM Tools and Frameworks&#10;&#10;| Tool/Framework         | Language   | Focus/Strengths                  | Integration      | Use Case Examples              |&#10;|-----------------------|------------|----------------------------------|------------------|-------------------------------|&#10;| LangChain             | Python     | Chaining, agents, retrieval      | Many LLM APIs    | Chatbots, RAG, workflows      |&#10;| LangGraph             | Python     | Multi-agent, graph orchestration | LangChain        | Reasoning, modular agents     |&#10;| Spring AI             | Java       | Enterprise, abstraction          | Spring ecosystem | Business apps, backend        |&#10;| HF Transformers       | Python     | Model training, deployment       | Model hub, APIs  | NLP tasks, research, prod     |&#10;| Agentic AI (AutoGPT, CrewAI, N8N) | Python/JS | Autonomous agents, automation | Various          | Task automation, multi-agent  |&#10;&#10;### Prompt&#10;- A prompt is the input text or set of instruction given to an LLM to guide its response. &#10;- Considerations for prompts Common Issues:&#10;  - **Clarity**: Avoiding ambiguity to get accurate results. Cannot use our own DSLs. &#10;  - **Context**: Provide relevant background or examples.&#10;  - **Length**: Too short may lack detail; too long may confuse or dilute intent.&#10;  - **Structure**: Use formatting, lists, or step-by-step instructions for better outputs.&#10;  - **Constraints**: Specify requirements, style, or limitations as needed.&#10;  - **Iteration**: Refine prompts based on model responses to improve outcomes.&#10;  - **Inconsistent results**: Due to different models and also different model parameters.&#10;  - Output format varies by model:&#10;    - Some prepend `assistant:` or add Markdown fences (```json```).&#10;    - Some omit required fields or slightly alter the structure.&#10;    - Nested structures may be inconsistent or flattened unexpectedly. This was particularly the case with Mistral.&#10;  - Models may generate **extra explanations** or comments alongside the data.&#10;  - Inconsistent **data types** (numbers as strings, null vs missing fields).&#10;  - Missing **mandatory keys** in JSON or headers in CSV.&#10;  - Some models **truncate long outputs**, breaking the structure.&#10;  - Free-text injections may appear despite formatting instructions.&#10;&#10;#### Solutions&#10;- Use **prompt engeinnering techniques** with examples to guide the structure.&#10;- **Use of Strong models**: Models such as OpenAI or use of CoPilot Strong support via JSON mode or function calling.&#10;&#10;### Prompt Engineering and Patterns&#10;- Prompt engineering is the practice of designing and refining prompts to optimize LLM outputs.&#10;- Prompt engineers experiment with wording, structure, and context to achieve desired results.&#10;- Common patterns:&#10;&#10;| **Type**                       | **Definition**                                                       | **Example**                                                                 |&#10;|--------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------|&#10;| **Single-String Prompting**    | One large text prompt, meant to be completed.             | `Translate to French: &quot;How are you?&quot;`                                      |&#10;| **Chat-Based Prompting**       | Structured messages with roles (`system`, `user`, `assistant`).     | System: &quot;You are helpful.&quot; User: &quot;Translate 'How are you?' to French.&quot;     |&#10;| **Zero-Shot**                  | No examples provided; just the instruction.                         | `Summarize this text in one sentence: ...`                                  |&#10;| **Few-Shot**                   | Include a few examples to guide the output.                         | `Q: Capital of France? A: Paris. Q: Capital of Germany? A:`                |&#10;| **Chain-of-Thought (CoT)**     | Ask model to reason step-by-step before answering.                  | `Explain step by step: If you have 3 apples and eat 1, how many are left?` |&#10;| **Self-Consistency**           | Model generates multiple solutions, picks most consistent.          |  reasoning-heavy tasks. Why is the balance showing at $200 instead of $100? Generate multiple reasoning paths                                              |&#10;| **Role-Based / Persona**       | Assign a role or persona for better context.                        | `Act as an expert Python tutor. Explain decorators.`                        |&#10;| **Instruction-Based**          | Direct task-oriented commands without dialogue.                     | `Extract all email addresses from the text.`                                |&#10;| **Structured Output**          | Request output in JSON, table, or specific format.                  | `Return the answer in JSON with keys: name, age.`                           |&#10;| **ReAct (Reason + Act)**       | Model reasons, then calls tools or APIs.                            | `Thought: I need weather info. Action: call weather API.`                   |&#10;| **Retrieval-Augmented (RAG)**  | Combine prompt with retrieved external knowledge.                   | `[Context] Answer based on above.`                                          |&#10;| **Multimodal Prompting**       | Combines text + images (or other modalities).                       | `Describe the scene in this image.`                                         |&#10;| **Prompt Assiting**       | Similar to meta prompting but using copilot to generate prompts for mistral                       | `Describe the scene in this image.`                                         |&#10;| **Prompt Chaining**            | Splits a task into multiple linked prompts.                         | Step 1: Summarize → Step 2: Extract entities → Step 3: Generate questions. |&#10;| **Reflection / Deliberation**  | Ask model to review or critique its own response.                   | `Check your previous answer and improve it.`                                |&#10;| **Dynamic Prompt Composition** | Build prompts programmatically for personalization.                 | Insert user name, context dynamically from a database.                      |&#10;| **Tool Calling**               | Enable model to trigger external tools for answers.                 | `If math needed, call calculator function.`                                  |&#10;| **Function Calling**           | Model outputs structured JSON to call specific function.            | `{ &quot;function&quot;: &quot;get_weather&quot;, &quot;location&quot;: &quot;Paris&quot; }`                        |&#10;&#10;### Roles&#10;| Role        | Purpose                                                       | Example                                                                 |&#10;|-------------|--------------------------------------------------------------|------------------------------------------------------------------------|&#10;| **system**  | Sets high-level instructions, behavior, or persona for model | &quot;You are an expert software engineer. Respond concisely with examples.&quot; |&#10;| **user**    | Represents the end-user’s input or question                  | &quot;Write a Python function to reverse a string.&quot;                         |&#10;| **assistant**| Represents the model’s own response                         | &quot;Here is the Python code:\n```python\ndef reverse_string(s): return s[::-1]\n```&quot; |&#10;| **tool**    | Output from an external tool or function call                | `{&quot;result&quot;: &quot;42&quot;}` (after a calculator function call)                  |&#10;| **function**| (Older term for tool) Shows result from a function           | Same as tool                                                           |&#10;| **critic**  | Used for self-evaluation or reinforcement learning loops     | &quot;Check if the previous answer followed all constraints.&quot;               |&#10;| **developer** (optional) | Provides additional instructions (multi-role systems) | &quot;Add unit tests for the function above.&quot;                               |&#10;&#10;apart from the system, user and assistant roles which are the default roles in some frameworks most frameworks support custom roles.&#10;```python&#10;from langchain.schema import ChatMessage&#10;&#10;messages = [&#10;    ChatMessage(role=&quot;system&quot;, content=&quot;You are an expert AI.&quot;),&#10;    ChatMessage(role=&quot;developer&quot;, content=&quot;Always include type hints in Python code.&quot;),&#10;    ChatMessage(role=&quot;user&quot;, content=&quot;Write a function to add two numbers.&quot;)&#10;]&#10;```&#10;&#10;### Structured Output&#10;Structured output model returns data in a **specific or predefined, machine-readable format** like JSON or CSV, instead of free-form text.&#10;&#10;### Output &amp; performance&#10;&#10;| Strategy                   | Description / Tips                                                                                       |&#10;|-----------------------------|---------------------------------------------------------------------------------------------------------|&#10;| **Streaming vs Batch**      | Stream for user-facing outputs (low latency). Batch multiple prompts with templates for high throughput. |&#10;| **Token Management**        | Limit `max_tokens`. Use `stop` sequences to avoid unnecessary generation.                               |&#10;| **Parallelization**         | Run independent requests concurrently. Use async frameworks or thread pools.                             |&#10;| **Prompt Caching**                 | Cache repeated prompts/responses to reduce API calls and latency.                                       |&#10;| **Model Selection**         | Use smaller/faster models for simple tasks; larger models for complex reasoning or high-quality output. |&#10;| **Tooling Integration**     | Use RAG or vector databases to reduce redundant computation.                                           |&#10;| **Function/Structured Output** | Use function calls or structured outputs to reduce parsing and token overhead.                        |&#10;&#10;* Writing templates for prompts help reusable prompts.&#10;* Some frameworks support, **Imperative or Declarative Composition** which helps in controlling the sequence and flow of operations with LLMs, tools, and data. precise control over prompts, model calls, or conditional logic.&#10;&#10;&#10;### Embeddings&#10;- Embeddings are dense vector representations of text (words, sentences, or documents) generated by LLMs.&#10;- advantage is they capture semantic meaning, allowing for usecases - **semantic search, recommendation systems, document classification, clustering, and information retrieval**.&#10;- Common models for embeddings: **OpenAI Embeddings, Cohere, Sentence Transformers, BERT.**&#10;&#10;  - An embedding model is an algorithm that converts a text into a numerical representation of its meaning.  &#10;  - The output is a long array of floating-point numbers called dimensions.  &#10;  - These are called **dense embeddings**, where most dimensions have non-zero values.  &#10;  - Each number is a floating-point value representing a **semantic dimension** of the text.&#10;  - **traveling the meaning** in space by using the elementary math operations of addition and subtraction: for instance, the operation king – man + woman = queen. take the meaning (or semantic embedding) of king, subtract the meaning of man, I guess you reach something like a ruler, add woman, would've arrived close to word queen. &#10;  &#10;- Embedding models produce vectors for **semantic understanding**, while chat models produce **human-readable text**.&#10;&#10;### RAG (Retrieval-Augmented Generation)&#10;- Models have limited knowledge in the context of a specific business use case or problem, augmenting it with Business related knowledge base is essential. RAG Combines retrieval of relevant documents with LLM generation. Helps LLMs answer questions using external knowledge beyond their training data.&#10;  &#10;- **3 stages**&#10;  - **Indexing:** create and storing embeddings of data in a vector store.&#10;  - **Retrieval:** retrieving the relevant embeddings and data from vector store&#10;    - **Techniques / Similarity Measures:**  &#10;      - **Cosine Similarity:** Measures angle between vectors; commonly used for semantic similarity.  &#10;      - **Euclidean Distance:** Measures straight-line distance in embedding space; good for spatial closeness.  &#10;      - **Dot Product / Inner Product:** Measures magnitude of similarity; &#10;      - **Approximate Nearest Neighbors (KNN):** Efficient search in large vector stores for top-k similar vectors.  &#10;  - **Generation:** Augmenting the original prompt with the relevant retrieved documents. &#10;&#10;- **Some issues you may face with RAG:**  &#10;  - Large context data cause incorrect results.  &#10;  - data will be split across multiple documents will cause incorrect results.  &#10;  - Too much context requires the model to filter irrelevant info → risk of hallucination.&#10;&#10;### Vector Store&#10;- A vector store is a database for storing and searching embeddings.&#10;- good for similarity search and retrieval of relevant documents or data points based on vector operations.&#10;- Popular vector stores: Pinecone, FAISS, PgVector.&#10;- Use cases: retrieval-augmented generation (RAG), semantic search and knowledge management.&#10;&#10;### Vector Stores and Embeddings: Problems Solved &amp; Techniques&#10;&#10;- **Problems They Solve:**  &#10;  - **Semantic Search:** Find documents or text that are meaningfully similar to a query.&#10;  - **Classification:** Assigning a new document to a previously identified group or label (for instance, a topic)&#10;  - **Question Answering:** Retrieve relevant context for LLMs to answer questions accurately.  &#10;  - **Recommendation Systems:** Suggest items based on similarity of embeddings.  &#10;  - **Clustering &amp; Topic Analysis:** Group similar content or detect topics.  &#10;  - **Anomaly Detection:** Identify outliers by distance from typical embeddings.&#10;  &#10;```python&#10;# 1. Semantic Search&#10;query = &quot;Find documents about long documents.&quot;&#10;results = pgvector_store.similarity_search(query, k=3)&#10;&#10;# 2. Question Answering&#10;answer = qa.run(&quot;What does the document talk about?&quot;)&#10;&#10;# 3. Classification&#10;nearest_docs = pgvector_store.similarity_search(&quot;New text to classify&quot;, k=1)&#10;predicted_label = nearest_docs[0].metadata.get(&quot;label&quot;)&#10;&#10;# 4. Recommendation&#10;recommendations = pgvector_store.similarity_search(&quot;Guide on API design&quot;, k=5)&#10;&#10;# 5. Clustering&#10;all_embeddings = pgvector_store.get_embeddings()  # then apply KMeans externally&#10;&#10;# 6. Anomaly Detection&#10;new_embedding = embeddings.embed_query(&quot;Some unusual text.&quot;)&#10;nearest = pgvector_store.similarity_search_by_vector(new_embedding, k=1)&#10;```&#10;&#10;## Strategis for Loading Documents into Chat Model Prompts &#10;&#10;### 1. Current Knowledge Base&#10;- Use of structured templates and formats for knowledge base - serve as **context needed for the LLM**.&#10;- follow best practices of prompt engineering check results in Copilot after generation.&#10;- Knowledge can be provided as plain text, PDF, or **Markdown** format.  &#10;- Markdown format easily converted to and from Confluence tools like `pandoc`.  &#10;&#10;### 2. Representing Information based on Retrieval strategy&#10;- Representing information in **tables** is effective.  &#10;- Each cell corresponds to a **row and column**, making structured retrieval easier for the model.  &#10;- tables might sometimes be stored in RDBMS better than vector store.&#10;- Use of object store or nosql where applicable.&#10; &#10;## Strategies for Splitting Text into Meaningful Chunks&#10;&#10;### 1. Chunking with Overlap&#10;- Split large text into chunks with a **character or token overlap** between chunks.  &#10;- **Advantages:**  &#10;  - Maintains context between chunks.  &#10;  - Reduces risk of losing important information at chunk boundaries.  &#10;- **Disadvantages:**  &#10;  - Increases the total number of chunks → more embeddings and storage.  &#10;  - Can introduce redundancy in retrieval.  &#10;&#10;### 2. Language or Format-Aware Chunking&#10;- Split based on the type of content:  &#10;  - **Markdown or structured text:** split by headings, sections, or paragraphs.  &#10;  - **Code (Python, etc.):** split by functions, classes, or logical blocks.  &#10;- Ensures each chunk is **semantically coherent**.  &#10;&#10;### 3. Maintaining References&#10;- Keep a **reference to the original document** after splitting.  &#10;- Useful for workflows like **Reflexion**, where you might need to trace information back to the source.  &#10;- Helps the LLM provide **accurate citations or context**.  &#10;&#10;### Vector DB Issues  &#10;- **Document Change Tracking**: Use a SQL record manager library or similar strategies to track document updates.&#10;  - Versioning with Metadata: Each document update creates a new version; store version_id in metadata.&#10;  -   Hash-Based Updates: Compute hash for each chunk; update only changed chunks.&#10;  - Soft Deletes (Active Flag): Mark old chunks as is_active=False and insert new ones.&#10;  ```python&#10;  vectorstore.similarity_search(query, filter={&quot;is_active&quot;: True})&#10;  ```&#10;&#10;## Patterns&#10;&#10;**Problem**: Mixed-content documents (text + tables) can lose structure if split only by text.&#10;## MultiVector Retrieval  &#10;&lt;img width=&quot;1200&quot; height=&quot;611&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/830dff2f-637f-4987-9825-44a56cacb205&quot; /&gt;&#10;&#10;- **Design Pattern**: Follows *CQRS (Command Query Responsibility Segregation)* principle separate write (doc updates) and read (retrieval) models for consistency.&#10;- seperate out the **vector store** and **doc store**.&#10;- **Approach**: Maintain unique IDs across vector store and doc store for sync.  &#10;- **vector store** - Store summaries or embeddings  &#10;- **doc store** - Store original content (tables, full text) in RDBMS, NoSQL, object store, or in-memory docstore.  &#10;- Link via **ID references**.  &#10;    &#10;```python&#10;# Helper function to fetch docs from MySQL&#10;def fetch_docs_from_mysql(ids):&#10;    placeholders = ','.join(['%s'] * len(ids))&#10;    query = f&quot;SELECT id, content, metadata FROM documents WHERE id IN ({placeholders})&quot;&#10;    mysql_cursor.execute(query, tuple(ids))&#10;    rows = mysql_cursor.fetchall()&#10;    return [Document(page_content=row['content'], metadata=row['metadata']) for row in rows]&#10;&#10;# 3. Create MultiVectorRetriever&#10;retriever = MultiVectorRetriever(&#10;    vectorstore=pgvector_store,&#10;    docstore=fetch_docs_from_mysql,&#10;    id_key=&quot;id&quot;&#10;)&#10;```&#10;&#10;or &#10;```python&#10;docstore = InMemoryDocstore()&#10;&#10;# 2. Add documents to InMemoryDocstore&#10;doc_ids = [&quot;doc1&quot;, &quot;doc2&quot;]&#10;chunks = [&#10;    Document(page_content=&quot;First document about LangChain.&quot;, metadata={&quot;category&quot;: &quot;AI&quot;}),&#10;    Document(page_content=&quot;Second document about Vector Stores.&quot;, metadata={&quot;category&quot;: &quot;Database&quot;})&#10;]&#10;docstore.mset(list(zip(doc_ids, chunks)))&#10;&#10;# 3. Create MultiVectorRetriever&#10;retriever = MultiVectorRetriever(&#10;    vectorstore=pgvector_store,  # Same as before&#10;    docstore=docstore,           # InMemoryDocstore instance&#10;    id_key=&quot;id&quot;                  # ID used in vector store entries&#10;)&#10;```&#10;### Strategies for Mathematical operations&#10;- GPT models excel at predicting the next word and reasoning, but are not reliable for accurate mathematical calculations.&#10;- Strategies to handle mathematical operations:&#10;  - Use external tools or function calling (e.g., calculator APIs, Python code execution) for math tasks.&#10;  - Integrate tool-calling patterns so the LLM delegates math problems to a dedicated computation engine.&#10;  - Convert math queries into structured requests that trigger tool or function invocation.&#10;  - For classification-type math (e.g., &quot;is this a credit or debit&quot;), use the LLM to route the query to the appropriate tool.&#10;  - Always validate or post-process LLM-generated math answers with external logic or tools.&#10;&#10;### Strategies for Security Compliance&#10;- Hybrid model approach to classify data into secure data before using it on a public model.&#10;- rewrite-retrieve-read and multi query retrieval to compensate for queries with ill intentions&#10;&#10;### Recursive Abstractive Processing for Tree-Organized Retrieval (RAPTOR) —  &#10;&lt;img width=&quot;1810&quot; height=&quot;1356&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/781bebc6-19d2-4814-af83-0983f081b6ba&quot; /&gt;&#10;&#10;- **Problem:**  &#10;  - RAG systems must handle:&#10;    - **Lower-level questions:** referencing specific facts in a single document.  &#10;    - **Higher-level questions:** synthesizing ideas across multiple documents.  &#10;  - Standard k-nearest neighbors (k-NN) retrieval on document chunks struggles to cover both types effectively.  &#10;&#10;- **Solution: RAPTOR Approach**&#10;  - **Step 1:** Create **document summaries** capturing higher-level concepts.  &#10;  - **Step 2:** **Embed and cluster** the documents based on semantic similarity.  &#10;  - **Step 3:** **Summarize each cluster**, producing higher-level abstractions.  &#10;  - **Step 4:** Repeat the process **recursively**, forming a **tree of summaries** with increasing abstraction.  &#10;&#10;- **Indexing:**  &#10;  - Index **both the summaries and the original documents** together.  &#10;  - Ensures coverage for **questions ranging from low-level facts to high-level concepts**.&#10;&#10;- **Benefit:**  &#10;  - Efficient handling of **multi-granular retrieval**.  &#10;  - Supports queries spanning **single facts to overarching ideas**.&#10; &#10;```python&#10;# 2. Split documents into chunks&#10;docs = [Document(page_content=doc) for doc in all_documents]  # all_documents is your text list&#10;splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)&#10;chunks = []&#10;for doc in docs:&#10;    chunks.extend(splitter.split_text(doc.page_content))&#10;&#10;# 3. Generate summaries for each chunk&#10;chunk_summaries = []&#10;for chunk in chunks:&#10;    summary = llm(f&quot;Summarize the following text concisely:\n{chunk}&quot;)&#10;    chunk_summaries.append(Document(page_content=summary))&#10;&#10;# 4. Cluster summaries&#10;num_clusters = 5&#10;summary_embeddings = [embeddings.embed_query(s.page_content) for s in chunk_summaries]&#10;kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(summary_embeddings)&#10;&#10;# 5. Summarize each cluster (recursive abstraction)&#10;cluster_summaries = []&#10;for cluster_idx in range(num_clusters):&#10;    cluster_docs = [chunk_summaries[i].page_content for i in range(len(chunk_summaries)) &#10;                    if kmeans.labels_[i] == cluster_idx]&#10;    cluster_text = &quot;\n&quot;.join(cluster_docs)&#10;    cluster_summary = llm(f&quot;Summarize the following cluster text:\n{cluster_text}&quot;)&#10;    cluster_summaries.append(Document(page_content=cluster_summary))&#10;&#10;# 6. Index both original chunks and cluster summaries in vector store&#10;vectorstore.add_documents(chunks + cluster_summaries)&#10;&#10;# 7. Query example&#10;query = &quot;Explain high-level ideas from the documents.&quot;&#10;results = vectorstore.similarity_search(query, k=3)&#10;for r in results:&#10;    print(r.page_content)&#10;```&#10;&#10;### ColBERT: Optimizing Embeddings&#10;&lt;img width=&quot;1400&quot; height=&quot;578&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/cc1d9783-9da8-4220-a1c4-e7b48beb5e16&quot; /&gt;&#10;&#10;- **Problem with standard embeddings:**  &#10;  - Fixed-length embeddings compress entire text into a single vector.  &#10;  - Useful for retrieval, but embedding irrelevant/redundant content can cause **hallucination** in LLM outputs.  &#10;&#10;- **ColBERT Approach:**  &#10;  1. **Contextual token embeddings:** Generate embeddings for **each token** in the document and query instead of a single vector.  &#10;  2. **Token-level similarity scoring:** Calculate similarity between **each query token** and **all document tokens**.  &#10;  3. **Aggregate scores:** For each query token, take the **maximum similarity** to any document token; sum these to get a **document-level score**.  &#10;&#10;- **Benefits:**  &#10;  - Provides **granular, token-level retrieval**.  &#10;  - Reduces irrelevant content impact.  &#10;  - Improves accuracy of retrieved context for LLMs.  &#10;&#10;- **Key takeaway:**  &#10;  - ColBERT is an embedding model designed to implement this **token-level scoring mechanism**, optimizing document retrieval for downstream applications.&#10;&#10;&#10;```python&#10;from ragatouille import RAGPretrainedModel&#10;# Load the ColBERT-based RAG model&#10;RAG = RAGPretrainedModel.from_pretrained(&quot;colbert-ir/colbertv2.0&quot;)&#10;&#10;# Example usage: encode documents or queries&#10;docs = [&quot;LangChain helps build applications with LLMs efficiently.&quot;]&#10;query = &quot;How to use LangChain for LLM apps?&quot;&#10;&#10;# Encode the documents (token-level embeddings)&#10;doc_embeddings = RAG.encode_documents(docs)&#10;&#10;# Encode the query&#10;query_embedding = RAG.encode_query(query)&#10;&#10;# Retrieve top-k relevant documents&#10;results = RAG.retrieve(query_embedding, k=3)&#10;for r in results:&#10;    print(r)&#10;```&#10;&#10;## Effective Strategies Using RAG&#10;&#10;- Part of our data can be stored in a **vector store** for operations like semantic search.  &#10;- Depending on the data type, it may be more efficient to store data in **RDBMS** or **NoSQL** or **Object Store**.  &#10;&#10;### Query Transformation&#10;- Incoming inputs can be **varied and uncontrolled**, e.g.:  &#10;  1. Event-driven data  &#10;  2. Future integration with other systems  &#10;  3. User interface inputs  &#10;- **Solution:** Transform queries into a format the system can reliably answer.  &#10;- Benefits:  &#10;  - Ensures consistent processing across diverse inputs.  &#10;  - Helps address security concerns and prevents misuse.  &#10;&#10;### Step-Back Prompting&#10;- Ask the model to **analyze or reflect** before giving a final answer.  &#10;- Helps identify assumptions, gaps, or errors in reasoning.  &#10;- Reduces mistakes in **multi-step reasoning**.&#10;&#10;### Subquestion Prompting&#10;- Breaks a **complex question into smaller subquestions**.  &#10;- Answer each subquestion individually and aggregate results.  &#10;- Improves accuracy for **multi-part queries** and reduces hallucination.&#10;&#10;### Rewritten Prompting aka Rewrite-Retrieve-Read &#10;- Reformulates a query to **clarify intent or simplify language**.  &#10;- Ensures the LLM focuses on the intended question.  &#10;- Reduces misinterpretation and improves output quality.&#10;  &#10;### Multi-Query Retrieval&#10;&lt;img width=&quot;1676&quot; height=&quot;596&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/595030dc-1a2b-4fa4-8a82-4ec58822a72d&quot; /&gt;&#10;&#10;- **Problem:** A single user query may not capture the full scope of information needed for a comprehensive answer.  &#10;&#10;- **Solution:** Multi-query retrieval strategy:&#10;  1. **Query Expansion:** Use an LLM to generate multiple related queries from the user’s initial query.  &#10;  2. **Parallel Retrieval:** Execute each generated query against the data source (vector store, RDBMS, etc.).  &#10;  3. **Context Aggregation:** Combine retrieved results as prompt context for the LLM.  &#10;  4. **Final Output:** The LLM generates a more complete and accurate answer using the aggregated context.  &#10;&#10;- **Benefit:** Improves coverage and accuracy by capturing **all relevant information** across the dataset.&#10;- **Example**: What is the weather in Sydney?”, give me 5 possible queries:&#10;    1. What is the current temperature in Sydney?&#10;    2. Are there any weather warnings or alerts in Sydney right now?&#10;    3. What is the forecast for Sydney for the next few hours?&#10;    4. Is it sunny, rainy, or cloudy in Sydney currently?&#10;    5. What is the humidity and wind speed in Sydney today?&#10;&#10;### RAG-Fusion&#10;&lt;img width=&quot;1284&quot; height=&quot;481&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/9597f42d-7e23-40f5-985e-cabef52035b3&quot; /&gt;&#10;&#10;RAG-Fusion (Retrieval-Augmented Generation with Fusion) is an extension of the multi-query retrieval strategy. It enhances the retrieval process by introducing a **final reranking step** using the **Reciprocal Rank Fusion (RRF)** algorithm.&#10;&#10;### Key Steps&#10;&#10;1. **Query Expansion:** Generate multiple related queries from the user's initial query.&#10;2. **Parallel Retrieval:** Execute each query independently against the data source (vector store, RDBMS, or search engine).&#10;3. **Reciprocal Rank Fusion (RRF):**  &#10;   - Each retrieved document has a rank for each query.  &#10;   - RRF combines these ranks into a single, unified ranking.  &#10;   - Formula: ```math  RRF Score = \sum_{i} \frac{1}{k + \text{rank}_i}``` where \(k\) is a constant (commonly 60) to reduce the effect of lower-ranked documents. a big contant so that the rank doesn't domainate and have advantage.&#10;   - The most relevant documents across all queries rise to the top.&#10;4. **Context Aggregation:** Use the top-ranked documents as context for the LLM.&#10;5. **Final Output:** LLM generates a more accurate and comprehensive answer using aggregated, reranked context.&#10;&#10;#### Benefits&#10;&#10;- Improves retrieval quality by prioritizing documents relevant to multiple query perspectives.&#10;- Handles queries with different scoring scales or distributions effectively.&#10;- Reduces noise from less relevant results while promoting consensus across queries.&#10;&#10;#### Use Cases&#10;&#10;1. **Legal or Compliance like ACMA or TCP or Security documenations:**  &#10;   - Searching across multiple regulations, rules, and previous incidents.  &#10;   - RRF helps surface the most relevant documents that are supported across different search formulations.&#10;&#10;2. **Enterprise Search:**  &#10;   - Documents across Telstra for example Flexcab.&#10;&#10;RAG-Fusion is particularly powerful when the information space is large and diverse, and a single query is insufficient to capture all relevant context.&#10;## Hypothetical Document Embeddings (HyDE)&#10;&lt;img width=&quot;460&quot; height=&quot;300&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/564420f8-9bd2-4f97-9e68-0e0dbcfb02f4&quot; /&gt;&#10;&#10;**HyDE** is a retrieval strategy that leverages LLMs to improve vector-based search by creating a **hypothetical document** from the user’s query. The idea is that the LLM can expand, paraphrase, or contextualize the query into a richer representation that is more semantically similar to relevant documents in the dataset.&#10;HyDE effectively **bridges the gap between natural language queries and document embeddings**, improving the relevance of retrieved documents in retrieval-augmented generation (RAG) systems.&#10;### How HyDE Works&#10;  - The user provides a query, e.g., `&quot;What is the current Balance and give step by step how you arrive at that.&quot;`&#10;  - An LLM generates a **hypothetical document** or passage as if it were an answer to the query.  &#10;  - The hypothetical document is converted into a **vector embedding** using the same embedding model as used for the document database.&#10;  - The embedding is used to perform a **vector search** against the document corpus.  &#10;  - The assumption: the LLM-generated document is **closer in embedding space** to relevant documents than the raw query. &#10;  - Top-k similar documents are retrieved and can then be fed as context to the LLM for final answer generation. &#10;&#10;### Benefits&#10;&#10;- **Query Expansion without Manual Effort:** LLM generates richer semantic content automatically.&#10;- **Better Retrieval Accuracy:** Hypothetical documents are often closer to relevant documents in embedding space than terse user queries.&#10;- **Works with Sparse Queries:** Short or ambiguous queries benefit from the additional context generated by the LLM.&#10;&#10;## Query Routing&#10;Query routing is a strategy used in retrieval or search systems to **direct a user query to the most relevant subset of data or service**. The goal is to improve retrieval efficiency, relevance, and response time. There are two main strategies: **logical routing** and **semantic routing**.&#10;&#10;### 1. Logical Routing&#10;Logical routing directs queries based on **predefined rules, categories, or metadata** associated with the documents or data sources.&#10;- Documents are classified or tagged into **logical partitions** (e.g., departments, product lines, regions).  &#10;- Queries are routed to the partition(s) that match certain **keywords, tags, or rules**.  &#10;&#10;**Example:**  &#10;```python&#10;class RouteQuery(BaseModel):&#10;    &quot;&quot;&quot;Route a user query to the most relevant datasource.&quot;&quot;&quot;&#10;    datasource: Literal[&quot;python_docs&quot;, &quot;js_docs&quot;] = Field(&#10;        ...,&#10;        description=&quot;Choose the most relevant datasource based on keywords.&quot;&#10;    )&#10;```&#10;---&#10;&#10;### 2. Semantic Routing&#10;Semantic routing directs queries based on **meaning or intent**, often using embeddings, LLMs, or vector similarity, rather than strict keywords or rules.&#10;&#10;**How it works:**  &#10;- Queries are converted into **semantic representations** (embeddings).  &#10;- Documents or partitions are also represented in the same embedding space.  &#10;- The system routes the query to the **most semantically relevant subset**.  &#10;&#10;**Example:**  &#10;```python&#10;physics_template = &quot;&quot;&quot;You are a very smart physics professor. You are great at     answering questions about physics in a concise and easy-to-understand manner.     When you don't know the answer to a question, you admit that you don't know. Here is a question: {query}&quot;&quot;&quot;&#10;math_template = &quot;&quot;&quot;You are a very good mathematician. You are great at answering     math questions. You are so good because you are able to break down hard     problems into their component parts, answer the component parts, and then     put them together to answer the broader question. Here is a question: {query}&quot;&quot;&quot;&#10;&#10;# Embed prompts&#10;embeddings = OpenAIEmbeddings()&#10;prompt_templates = [physics_template, math_template]&#10;prompt_embeddings = embeddings.embed_documents(prompt_templates)&#10;&#10;# Route question to prompt&#10;@chain&#10;def prompt_router(query):&#10;    query_embedding = embeddings.embed_query(query)&#10;    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]&#10;    most_similar = prompt_templates[similarity.argmax()]&#10;    print(&quot;Using MATH&quot; if most_similar == math_template else &quot;Using PHYSICS&quot;)&#10;    return PromptTemplate.from_template(most_similar)&#10;```&#10;**Use Cases:**  &#10;- Logical routing: Enterprise databases, departmental knowledge bases, FAQs.  &#10;- Semantic routing: RAG systems, multi-domain knowledge search, chatbots, recommendation engines.  &#10;&#10;## Query Construction&#10;&#10;Query construction is the process of **transforming a user's natural language query into a structured query** that can be executed against various data sources. This is essential for retrieval systems, vector stores, and relational databases.&#10;&#10;---&#10;&#10;## 1. Text-to-Metadata Conversion&#10;&#10;Vector stores often support **metadata-based filtering**, allowing more precise searches. During the embedding process:&#10;&#10;- **Attach metadata**: Each vector can be associated with key-value pairs (e.g., document type, author, date)&#10;- **Filter on query**: When performing a search, you can specify metadata filters to narrow down results&#10;&#10;1. **Embed documents and attach metadata:**&#10;   ```python&#10;   {&#10;       &quot;text&quot;: &quot;Annual financial report 2024&quot;,&#10;       &quot;embedding&quot;: [...],&#10;       &quot;metadata&quot;: {&quot;year&quot;: 2024, &quot;type&quot;: &quot;report&quot;}&#10;   }&#10;   ```&#10;&#10;2. **Construct a query with metadata filters:**&#10;   ```python&#10;   query_embedding = embed(&quot;Financial summary 2024&quot;)&#10;   results = vector_store.search(&#10;       query_embedding,&#10;       filter={&quot;year&quot;: 2024, &quot;type&quot;: &quot;report&quot;}&#10;   )&#10;   ```&#10;&#10;### Self-Querying Approach&#10;An LLM can translate a natural language query into metadata filters automatically.&#10;&#10;**Example:** &quot;Show event by event name and event publisher&quot; → `{&quot;event_name&quot;: &quot;payment event&quot;, &quot;event_publisher&quot;: &quot;payment system&quot;}`&#10;&#10;---&#10;&#10;## 2. Text-to-SQL&#10;Relational databases require SQL queries, which are not naturally compatible with human language. Text-to-SQL allows LLMs to translate user queries into SQL queries.&#10;&#10;### Strategies for Effective Translation&#10;#### Database Description (Grounding SQL)&#10;Provide the LLM with table definitions using `CREATE TABLE` statements, including:&#10;- Column names and types&#10;- Sample rows (typically 2-3 examples)&#10;This helps the LLM generate syntactically and semantically correct SQL queries.&#10;&#10;#### Few-Shot Examples&#10;&#10;Include question-to-SQL examples in the prompt to guide query generation.&#10;&#10;**Example:**&#10;* Q: Total salary of HR employees&#10;* SQL: SELECT SUM(salary) FROM employees WHERE department='HR';&#10;&#10;#### Error Handling&#10;When SQL execution fails, the LLM can:&#10;- Regenerate the query based on error messages&#10;- Repair the existing query with corrections&#10;- Improve robustness when working with dynamic or unfamiliar schemas&#10;&#10;## Summary&#10;| Query Construction Type | Description | Key Techniques |&#10;|------------------------|-------------|----------------|&#10;| **Text-to-Metadata** | Attach metadata to vectors and filter during search | Metadata key-value pairs, LLM-assisted filter extraction |&#10;| **Text-to-SQL** | Translate natural language to SQL queries | Database description, few-shot examples, error recovery |&#10;&#10;These strategies enable LLMs and retrieval systems to effectively interact with both unstructured and structured data, ensuring more accurate and relevant results.&#10;" />
              <option name="updatedContent" value="+++&#10;date = '2025-05-20T12:44:47+10:00'&#10;draft = false&#10;title = 'LLM Strategies and Design Patterns'&#10;tags = ['LLM', 'Design Patterns']&#10;+++&#10;&#10;LLM design patterns are reusable strategies for building robust, efficient, and scalable AI applications. They help developers structure retrieval, reading, rewriting, memory, agent, and orchestration workflows for large language models. These patterns improve performance, maintainability accuracy, cost and security.&#10;&#10;## Table of Contents&#10;1. [Introduction](#introduction)&#10;2. [Concepts](#concepts)&#10;    1. [LLMs](#llms)&#10;    2. [Transformer Architecture](#transformer-architecture)&#10;    3. [Tokens](#tokens)&#10;    4. [LLM Model Variants and Techniques](#llm-model-variants-and-techniques)&#10;    5. [Chat Models](#chat-models)&#10;    6. [Model Parameters](#model-paramaters)&#10;    7. [Popular LLM Tools and Frameworks](#popular-llm-tools-and-frameworks)&#10;    8. [Prompt](#prompt)&#10;    9. [Prompt Engineering and Patterns](#prompt-engineering-and-patterns)&#10;    10. [Roles](#roles)&#10;    11. [Structured Output](#structured-output)&#10;    12. [Common Issues](#common-issues)&#10;    13. [Output &amp; Performance](#output--performance)&#10;    14. [Embeddings](#embeddings)&#10;    15. [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)&#10;    16. [Vector Store](#vector-store)&#10;    17. [Vector Stores and Embeddings: Problems Solved &amp; Techniques](#vector-stores-and-embeddings-problems-solved--techniques)&#10;3. [Strategies for Loading Documents into Chat Model Prompts](#strategies-for-loading-documents-into-chat-model-prompts)&#10;    1. [Current Knowledge Base](#1-current-knowledge-base)&#10;    2. [Representing Information](#2-representing-information)&#10;4. [Strategies for Splitting Text into Meaningful Chunks](#strategies-for-splitting-text-into-meaningful-chunks)&#10;    1. [Chunking with Overlap](#1-chunking-with-overlap)&#10;    2. [Language or Format-Aware Chunking](#2-language-or-format-aware-chunking)&#10;    3. [Maintaining References](#3-maintaining-references)&#10;    4. [Using PgVector with Embedding Models](#using-pgvector-with-embedding-models)&#10;    5. [Vector DB Issues](#vector-db-issues)&#10;5. [Vector Stores](#vector-stores)&#10;6. [Patterns](#patterns)&#10;    1. [MultiVector Retrieval](#multivector-retrieval)&#10;    2. [Strategies for Mathematical Operations](#strategies-for-mathematical-operations)&#10;    3. [Strategies for Security Compliance](#strategies-for-security-compliance)&#10;    4. [Recursive Abstractive Processing for Tree-Organized Retrieval (RAPTOR)](#recursive-abstractive-processing-for-tree-organized-retrieval-raptor)&#10;    5. [ColBERT: Optimizing Embeddings](#colbert-optimizing-embeddings)&#10;    6. [Effective Strategies Using RAG](#effective-strategies-using-rag)&#10;    7. [Query Transformation](#query-transformation)&#10;    8. [Step-Back Prompting](#step-back-prompting)&#10;    9. [Subquestion Prompting](#subquestion-prompting)&#10;    10. [Rewritten Prompting aka Rewrite-Retrieve-Read](#rewritten-prompting-aka-rewrite-retrieve-read)&#10;    11. [Multi-Query Retrieval](#multi-query-retrieval)&#10;    12. [RAG-Fusion](#rag-fusion)&#10;    13. [Hypothetical Document Embeddings (HyDE)](#hypothetical-document-embeddings-hyde)&#10;    14. [Query Routing](#query-routing)&#10;    15. [Query Construction](#query-construction)&#10;        - [Text-to-Metadata Conversion](#1-text-to-metadata-conversion)&#10;        - [Text-to-SQL](#2-text-to-sql)&#10;        - [Summary](#summary)&#10;        &#10;## Introduction&#10;Instruction were to keep it basic &amp;  AI WG experiences 3 usecases are in progress. we will be sharing some basics wrt to the learnings as strategies or design patterns here. if haven't already will go through.&#10;* security arch approvals&#10;* cost&#10;* availablility of an appropriate model &#10;* model support for context size&#10;* amount of data in the knowledge base.&#10;* format of the data (e.g table, plantuml xmls)&#10;* challenges with the quality of context data.&#10;* Speed of models&#10;&#10;Before we proceed setting the stage.&#10;## Concepts &#10;### LLMs&#10;- GPT-3: 175B Parameters, ~350B tokens. GPT-4:is 6x bigger. GPT-5 was supposed to be 20x bigger but is counter intuitive 300B Parameters.&#10;- Chinchilla's Law: model performance optimal when training tokens ≈ 20 × model parameters.&#10;    - Example: 70B parameters → ~1.4T tokens needed.&#10;    - hundreds of billions/trillions of parameters requires enormous data more than available high-quality text.&#10;  - GPT-5 leverages on many architectural innovations like:&#10;    - Unified Intelligence System: integrates multiple reasoning and perception modules for more general intelligence.&#10;    - Mixture-of-Experts: routes inputs to specialized sub-models for improved efficiency and accuracy.&#10;    - Multimodal Integration: processes and understands text, images, and other data types together.&#10;- Sometimes we have bigger models but often we have restrictions using best available model due reasons mentioned before. Hence we have to compensate with design patterns. we will be going to few models available to atleast the Fintech and how do we get past some of the common issues.&#10;&#10;### Transformer Architecture: &#10;  - LLMs use the transformer neural network architecture.&#10;  - Transformers understand relationships between words, regardless of their position in the text.&#10;  - The architecture enables prediction of the next word in a sequence. that is to say they should not expected to perform for accurate mathematical computation; lot of our AI usecases some have some math involved.&#10;&#10;### Tokens:&#10;  - A token is a chunk of text, it could be entire word or part.&#10;  - Roughly, one token is about 4 characters or for calculation sake 75% of  words in English language.&#10;  - why important? - for us to estimate monthly cost: (tokens used per month) × (API cost per token).&#10;  - If transaction rate is T tokens/sec, that's about (T × 0.75) words/sec. (include ss)&#10;&#10;### LLM Model Variants and Techniques:&#10;  - Two main techniques:&#10;    - Predict next word (causal language modeling): Used by models like GPT; generates text by predicting the next token in a sequence.&#10;    - Predict masked word: Used by models like BERT; predicts masked tokens within a sentence for better understanding of context.&#10;  - GPT models: Focus on text generation, conversation, and completion tasks.&#10;  - BERT models: Focus on understanding, classification, and extracting information from text.&#10;  - Other flavours:&#10;    - Encoder-only (BERT): Good for understanding and classification.&#10;    - Decoder-only (GPT): Good for generation and completion.&#10;    - Encoder-decoder (T5, BART): Good for translation, summarization, and more complex tasks.&#10;  - Many LLMs are specialized for tasks like code generation, multimodal input (text + images), or domain-specific knowledge.&#10;&#10;&#10;### Chat Models&#10;- Chat models are specialized LLMs designed for conversational interactions.&#10;&#10;| Model                                                                                                        | Provider       | Strengths                                   | Typical Use Cases                                 |&#10;|--------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------|---------------------------------------------------|&#10;| Copilot                                                                                        |  Copilot utilizes Mixture-of-Experts - uses variety of AI models to provide coding assistance, adapting to different tasks and user needs. OpenAI and Microsoft's property models and tools      | Text generation, conversation, code, summarization | Chatbots, assistants, writing, code, content      |&#10;| LLaMA                                                                                                        | Meta           | Open-source, efficient, adaptable           | Research, custom training, open-source apps       |&#10;| Claude                                                                                                       | Anthropic      | Safety-focused, reliable, conversational    | Support, safe chatbots, document analysis         |&#10;| Mistral                                                                                                      | Mistral AI     | Kind of something from all aspects - Efficient, open weights, strong benchmarks  | Research, production NLP, open-source projects    |&#10;| Gemini                                                                                                       | Google DeepMind | Multimodal (text, image, audio), reasoning  | Multimodal assistants, content analysis           |&#10;| Anthropic (Claude), Amazon (Titan), Meta (Llama),&lt;br/&gt;Mistral, AI21 Labs (Jurassic), Cohere (Command) | Amazon Bedrock | Wide model selection, enterprise integration, scalable APIs | Chatbots, search, summarization, enterprise AI    |&#10;&#10;&#10;### Model Paramaters&#10;| Parameter            | Description                                                                                     | Typical Range        | Impact                                                                 |&#10;|----------------------|-------------------------------------------------------------------------------------------------|----------------------|------------------------------------------------------------------------|&#10;| **temperature**      | Controls randomness. Lower = deterministic, higher = creative/unpredictable.                  | `0.0` – `2.0`       | `0.0` = deterministic, `1.0` = balanced, `&gt;1.2` = very random.        |&#10;| **top_p**            | Nucleus sampling. keep adding tokens from smallest set until the cumulative probability ≥ p.           | `0.1` – `1.0`       | Lower = more focused, 1.0 = no restriction.                           |&#10;| **top_k**            | Limits token selection to the k most probable tokens at each step.   | `1` – `100`         | Low values reduce creativity, high values allow more variation.       |&#10;| **max_tokens**       | Maximum tokens to generate in the response.                                                   | Depends on model    | Controls output length.                                               |&#10;| **min_tokens**       | Minimum tokens to generate before stopping.                                                   | Depends on model    | Ensures longer answers if needed.                                     |&#10;| **presence_penalty** | Penalizes tokens already present in the text (encourages new topics).                          | `-2.0` – `2.0`      | Higher = more diversity.                                              |&#10;| **frequency_penalty**| Penalizes frequent tokens to reduce repetition.                                               | `-2.0` – `2.0`      | Higher = less repetition.                                             |&#10;| **stop**             | List of stop sequences where generation should halt.                                          | Strings / tokens    | Useful for structured responses.                                      |&#10;| **seed**             | Fixes randomness for reproducibility.                                                         | Integer             | Same seed = same output (with sampling).                              |&#10;| **length_penalty**   | Adjusts likelihood for longer sequences in beam search.                                       | `&gt;0`                | Higher = longer outputs favored.                                      |&#10;| **early_stopping**   | Stops beam search when best candidates are found.                                             | `true` / `false`    | Speeds up generation, may miss better completions.                    |&#10;| **logit_bias**       | dictionary to control probability of certain token ids   | Useful for forcing or avoiding words.                                 |&#10;| **echo**             | Return the prompt along with the completion.                                                  | `true` / `false`    | For debugging or prompt reconstruction.                                |&#10;&#10;#### Top p&#10;&#10;| Token | Probability |&#10;|-------|------------|&#10;| the   | 0.4        |&#10;| a     | 0.3        |&#10;| an    | 0.1        |&#10;| dog   | 0.05       |&#10;| ...   | ...        |&#10;&#10;If `p = 0.8`:&#10;- Cumulative probabilities:&#10;  - `&quot;the&quot; (0.4) + &quot;a&quot; (0.3) + &quot;an&quot; (0.1) = 0.8`&#10;- Nucleus = `{&quot;the&quot;, &quot;a&quot;, &quot;an&quot;}`&#10;- Next token is sampled **only** from this set.&#10;&#10;#### Top k &#10;&#10;Suppose the token probabilities are:&#10;&#10;| Token | Probability |&#10;|-------|------------|&#10;| the   | 0.4        |&#10;| a     | 0.3        |&#10;| an    | 0.1        |&#10;| dog   | 0.05       |&#10;| ...   | ...        |&#10;&#10;If `k = 3`:&#10;&#10;- Top 3 tokens = `{&quot;the&quot;, &quot;a&quot;, &quot;an&quot;}`&#10;- Next token is sampled **only** from this set.&#10;&#10;### Popular LLM Tools and Frameworks&#10;&#10;| Tool/Framework         | Language   | Focus/Strengths                  | Integration      | Use Case Examples              |&#10;|-----------------------|------------|----------------------------------|------------------|-------------------------------|&#10;| LangChain             | Python     | Chaining, agents, retrieval      | Many LLM APIs    | Chatbots, RAG, workflows      |&#10;| LangGraph             | Python     | Multi-agent, graph orchestration | LangChain        | Reasoning, modular agents     |&#10;| Spring AI             | Java       | Enterprise, abstraction          | Spring ecosystem | Business apps, backend        |&#10;| HF Transformers       | Python     | Model training, deployment       | Model hub, APIs  | NLP tasks, research, prod     |&#10;| Agentic AI (AutoGPT, CrewAI, N8N) | Python/JS | Autonomous agents, automation | Various          | Task automation, multi-agent  |&#10;&#10;### Prompt&#10;- A prompt is the input text or set of instruction given to an LLM to guide its response. &#10;- Considerations for prompts Common Issues:&#10;  - **Clarity**: Avoiding ambiguity to get accurate results. Cannot use our own DSLs. &#10;  - **Context**: Provide relevant background or examples.&#10;  - **Length**: Too short may lack detail; too long may confuse or dilute intent.&#10;  - **Structure**: Use formatting, lists, or step-by-step instructions for better outputs.&#10;  - **Constraints**: Specify requirements, style, or limitations as needed.&#10;  - **Iteration**: Refine prompts based on model responses to improve outcomes.&#10;  - **Inconsistent results**: Due to different models and also different model parameters.&#10;  - Output format varies by model:&#10;    - Some prepend `assistant:` or add Markdown fences (```json```).&#10;    - Some omit required fields or slightly alter the structure.&#10;    - Nested structures may be inconsistent or flattened unexpectedly. This was particularly the case with Mistral.&#10;  - Models may generate **extra explanations** or comments alongside the data.&#10;  - Inconsistent **data types** (numbers as strings, null vs missing fields).&#10;  - Missing **mandatory keys** in JSON or headers in CSV.&#10;  - Some models **truncate long outputs**, breaking the structure.&#10;  - Free-text injections may appear despite formatting instructions.&#10;&#10;#### Solutions&#10;- Use **prompt engeinnering techniques** with examples to guide the structure.&#10;- **Use of Strong models**: Models such as OpenAI or use of CoPilot Strong support via JSON mode or function calling.&#10;&#10;### Prompt Engineering and Patterns&#10;- Prompt engineering is the practice of designing and refining prompts to optimize LLM outputs.&#10;- Prompt engineers experiment with wording, structure, and context to achieve desired results.&#10;- Common patterns:&#10;&#10;| **Type**                       | **Definition**                                                       | **Example**                                                                 |&#10;|--------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------|&#10;| **Single-String Prompting**    | One large text prompt, meant to be completed.             | `Translate to French: &quot;How are you?&quot;`                                      |&#10;| **Chat-Based Prompting**       | Structured messages with roles (`system`, `user`, `assistant`).     | System: &quot;You are helpful.&quot; User: &quot;Translate 'How are you?' to French.&quot;     |&#10;| **Zero-Shot**                  | No examples provided; just the instruction.                         | `Summarize this text in one sentence: ...`                                  |&#10;| **Few-Shot**                   | Include a few examples to guide the output.                         | `Q: Capital of France? A: Paris. Q: Capital of Germany? A:`                |&#10;| **Chain-of-Thought (CoT)**     | Ask model to reason step-by-step before answering.                  | `Explain step by step: If you have 3 apples and eat 1, how many are left?` |&#10;| **Self-Consistency**           | Model generates multiple solutions, picks most consistent.          |  reasoning-heavy tasks. Why is the balance showing at $200 instead of $100? Generate multiple reasoning paths                                              |&#10;| **Role-Based / Persona**       | Assign a role or persona for better context.                        | `Act as an expert Python tutor. Explain decorators.`                        |&#10;| **Instruction-Based**          | Direct task-oriented commands without dialogue.                     | `Extract all email addresses from the text.`                                |&#10;| **Structured Output**          | Request output in JSON, table, or specific format.                  | `Return the answer in JSON with keys: name, age.`                           |&#10;| **ReAct (Reason + Act)**       | Model reasons, then calls tools or APIs.                            | `Thought: I need weather info. Action: call weather API.`                   |&#10;| **Retrieval-Augmented (RAG)**  | Combine prompt with retrieved external knowledge.                   | `[Context] Answer based on above.`                                          |&#10;| **Multimodal Prompting**       | Combines text + images (or other modalities).                       | `Describe the scene in this image.`                                         |&#10;| **Prompt Assiting**       | Similar to meta prompting but using copilot to generate prompts for mistral                       | `Describe the scene in this image.`                                         |&#10;| **Prompt Chaining**            | Splits a task into multiple linked prompts.                         | Step 1: Summarize → Step 2: Extract entities → Step 3: Generate questions. |&#10;| **Reflection / Deliberation**  | Ask model to review or critique its own response.                   | `Check your previous answer and improve it.`                                |&#10;| **Dynamic Prompt Composition** | Build prompts programmatically for personalization.                 | Insert user name, context dynamically from a database.                      |&#10;| **Tool Calling**               | Enable model to trigger external tools for answers.                 | `If math needed, call calculator function.`                                  |&#10;| **Function Calling**           | Model outputs structured JSON to call specific function.            | `{ &quot;function&quot;: &quot;get_weather&quot;, &quot;location&quot;: &quot;Paris&quot; }`                        |&#10;&#10;### Roles&#10;| Role        | Purpose                                                       | Example                                                                 |&#10;|-------------|--------------------------------------------------------------|------------------------------------------------------------------------|&#10;| **system**  | Sets high-level instructions, behavior, or persona for model | &quot;You are an expert software engineer. Respond concisely with examples.&quot; |&#10;| **user**    | Represents the end-user’s input or question                  | &quot;Write a Python function to reverse a string.&quot;                         |&#10;| **assistant**| Represents the model’s own response                         | &quot;Here is the Python code:\n```python\ndef reverse_string(s): return s[::-1]\n```&quot; |&#10;| **tool**    | Output from an external tool or function call                | `{&quot;result&quot;: &quot;42&quot;}` (after a calculator function call)                  |&#10;| **function**| (Older term for tool) Shows result from a function           | Same as tool                                                           |&#10;| **critic**  | Used for self-evaluation or reinforcement learning loops     | &quot;Check if the previous answer followed all constraints.&quot;               |&#10;| **developer** (optional) | Provides additional instructions (multi-role systems) | &quot;Add unit tests for the function above.&quot;                               |&#10;&#10;apart from the system, user and assistant roles which are the default roles in some frameworks most frameworks support custom roles.&#10;```python&#10;from langchain.schema import ChatMessage&#10;&#10;messages = [&#10;    ChatMessage(role=&quot;system&quot;, content=&quot;You are an expert AI.&quot;),&#10;    ChatMessage(role=&quot;developer&quot;, content=&quot;Always include type hints in Python code.&quot;),&#10;    ChatMessage(role=&quot;user&quot;, content=&quot;Write a function to add two numbers.&quot;)&#10;]&#10;```&#10;&#10;### Structured Output&#10;Structured output model returns data in a **specific or predefined, machine-readable format** like JSON or CSV, instead of free-form text.&#10;&#10;### Output &amp; performance&#10;&#10;| Strategy                   | Description / Tips                                                                                       |&#10;|-----------------------------|---------------------------------------------------------------------------------------------------------|&#10;| **Streaming vs Batch**      | Stream for user-facing outputs (low latency). Batch multiple prompts with templates for high throughput. |&#10;| **Token Management**        | Limit `max_tokens`. Use `stop` sequences to avoid unnecessary generation.                               |&#10;| **Parallelization**         | Run independent requests concurrently. Use async frameworks or thread pools.                             |&#10;| **Prompt Caching**                 | Cache repeated prompts/responses to reduce API calls and latency.                                       |&#10;| **Model Selection**         | Use smaller/faster models for simple tasks; larger models for complex reasoning or high-quality output. |&#10;| **Tooling Integration**     | Use RAG or vector databases to reduce redundant computation.                                           |&#10;| **Function/Structured Output** | Use function calls or structured outputs to reduce parsing and token overhead.                        |&#10;&#10;* Writing templates for prompts help reusable prompts.&#10;* Some frameworks support, **Imperative or Declarative Composition** which helps in controlling the sequence and flow of operations with LLMs, tools, and data. precise control over prompts, model calls, or conditional logic.&#10;&#10;&#10;### Embeddings&#10;- Embeddings are dense vector representations of text (words, sentences, or documents) generated by LLMs.&#10;- advantage is they capture semantic meaning, allowing for usecases - **semantic search, recommendation systems, document classification, clustering, and information retrieval**.&#10;- Common models for embeddings: **OpenAI Embeddings, Cohere, Sentence Transformers, BERT.**&#10;&#10;  - An embedding model is an algorithm that converts a text into a numerical representation of its meaning.  &#10;  - The output is a long array of floating-point numbers called dimensions.  &#10;  - These are called **dense embeddings**, where most dimensions have non-zero values.  &#10;  - Each number is a floating-point value representing a **semantic dimension** of the text.&#10;  - **traveling the meaning** in space by using the elementary math operations of addition and subtraction: for instance, the operation king – man + woman = queen. take the meaning (or semantic embedding) of king, subtract the meaning of man, I guess you reach something like a ruler, add woman, would've arrived close to word queen. &#10;  &#10;- Embedding models produce vectors for **semantic understanding**, while chat models produce **human-readable text**.&#10;&#10;### RAG (Retrieval-Augmented Generation)&#10;- Models have limited knowledge in the context of a specific business use case or problem, augmenting it with Business related knowledge base is essential. RAG Combines retrieval of relevant documents with LLM generation. Helps LLMs answer questions using external knowledge beyond their training data.&#10;  &#10;- **3 stages**&#10;  - **Indexing:** create and storing embeddings of data in a vector store.&#10;  - **Retrieval:** retrieving the relevant embeddings and data from vector store&#10;    - **Techniques / Similarity Measures:**  &#10;      - **Cosine Similarity:** Measures angle between vectors; commonly used for semantic similarity.  &#10;      - **Euclidean Distance:** Measures straight-line distance in embedding space; good for spatial closeness.  &#10;      - **Dot Product / Inner Product:** Measures magnitude of similarity; &#10;      - **Approximate Nearest Neighbors (KNN):** Efficient search in large vector stores for top-k similar vectors.  &#10;  - **Generation:** Augmenting the original prompt with the relevant retrieved documents. &#10;&#10;- **Some issues you may face with RAG:**  &#10;  - Large context data cause incorrect results.  &#10;  - data will be split across multiple documents will cause incorrect results.  &#10;  - Too much context requires the model to filter irrelevant info → risk of hallucination.&#10;&#10;### Vector Store&#10;- A vector store is a database for storing and searching embeddings.&#10;- good for similarity search and retrieval of relevant documents or data points based on vector operations.&#10;- Popular vector stores: Pinecone, FAISS, PgVector.&#10;- Use cases: retrieval-augmented generation (RAG), semantic search and knowledge management.&#10;&#10;### Vector Stores and Embeddings: Problems Solved &amp; Techniques&#10;&#10;- **Problems They Solve:**  &#10;  - **Semantic Search:** Find documents or text that are meaningfully similar to a query.&#10;  - **Classification:** Assigning a new document to a previously identified group or label (for instance, a topic)&#10;  - **Question Answering:** Retrieve relevant context for LLMs to answer questions accurately.  &#10;  - **Recommendation Systems:** Suggest items based on similarity of embeddings.  &#10;  - **Clustering &amp; Topic Analysis:** Group similar content or detect topics.  &#10;  - **Anomaly Detection:** Identify outliers by distance from typical embeddings.&#10;  &#10;```python&#10;# 1. Semantic Search&#10;query = &quot;Find documents about long documents.&quot;&#10;results = pgvector_store.similarity_search(query, k=3)&#10;&#10;# 2. Question Answering&#10;answer = qa.run(&quot;What does the document talk about?&quot;)&#10;&#10;# 3. Classification&#10;nearest_docs = pgvector_store.similarity_search(&quot;New text to classify&quot;, k=1)&#10;predicted_label = nearest_docs[0].metadata.get(&quot;label&quot;)&#10;&#10;# 4. Recommendation&#10;recommendations = pgvector_store.similarity_search(&quot;Guide on API design&quot;, k=5)&#10;&#10;# 5. Clustering&#10;all_embeddings = pgvector_store.get_embeddings()  # then apply KMeans externally&#10;&#10;# 6. Anomaly Detection&#10;new_embedding = embeddings.embed_query(&quot;Some unusual text.&quot;)&#10;nearest = pgvector_store.similarity_search_by_vector(new_embedding, k=1)&#10;```&#10;&#10;## Strategis for Loading Documents into Chat Model Prompts &#10;&#10;### 1. Current Knowledge Base&#10;- Use of structured templates and formats for knowledge base - serve as **context needed for the LLM**.&#10;- follow best practices of prompt engineering check results in Copilot after generation.&#10;- Knowledge can be provided as plain text, PDF, or **Markdown** format.  &#10;- Markdown format easily converted to and from Confluence tools like `pandoc`.  &#10;&#10;### 2. Representing Information based on Retrieval strategy&#10;- Representing information in **tables** is effective.  &#10;- Each cell corresponds to a **row and column**, making structured retrieval easier for the model.  &#10;- tables might sometimes be stored in RDBMS better than vector store.&#10;- Use of object store or nosql where applicable.&#10; &#10;## Strategies for Splitting Text into Meaningful Chunks&#10;&#10;### 1. Chunking with Overlap&#10;- Split large text into chunks with a **character or token overlap** between chunks.  &#10;- **Advantages:**  &#10;  - Maintains context between chunks.  &#10;  - Reduces risk of losing important information at chunk boundaries.  &#10;- **Disadvantages:**  &#10;  - Increases the total number of chunks → more embeddings and storage.  &#10;  - Can introduce redundancy in retrieval.  &#10;&#10;### 2. Language or Format-Aware Chunking&#10;- Split based on the type of content:  &#10;  - **Markdown or structured text:** split by headings, sections, or paragraphs.  &#10;  - **Code (Python, etc.):** split by functions, classes, or logical blocks.  &#10;- Ensures each chunk is **semantically coherent**.  &#10;&#10;### 3. Maintaining References&#10;- Keep a **reference to the original document** after splitting.  &#10;- Useful for workflows like **Reflexion**, where you might need to trace information back to the source.  &#10;- Helps the LLM provide **accurate citations or context**.  &#10;&#10;### Vector DB Issues  &#10;- **Document Change Tracking**: Use a SQL record manager library or similar strategies to track document updates.&#10;  - Versioning with Metadata: Each document update creates a new version; store version_id in metadata.&#10;  -   Hash-Based Updates: Compute hash for each chunk; update only changed chunks.&#10;  - Soft Deletes (Active Flag): Mark old chunks as is_active=False and insert new ones.&#10;  ```python&#10;  vectorstore.similarity_search(query, filter={&quot;is_active&quot;: True})&#10;  ```&#10;&#10;## Patterns&#10;&#10;**Problem**: Mixed-content documents (text + tables) can lose structure if split only by text.&#10;## MultiVector Retrieval  &#10;&lt;img width=&quot;1200&quot; height=&quot;611&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/830dff2f-637f-4987-9825-44a56cacb205&quot; /&gt;&#10;&#10;- **Design Pattern**: Follows *CQRS (Command Query Responsibility Segregation)* principle separate write (doc updates) and read (retrieval) models for consistency.&#10;- seperate out the **vector store** and **doc store**.&#10;- **Approach**: Maintain unique IDs across vector store and doc store for sync.  &#10;- **vector store** - Store summaries or embeddings  &#10;- **doc store** - Store original content (tables, full text) in RDBMS, NoSQL, object store, or in-memory docstore.  &#10;- Link via **ID references**.  &#10;    &#10;```python&#10;# Helper function to fetch docs from MySQL&#10;def fetch_docs_from_mysql(ids):&#10;    placeholders = ','.join(['%s'] * len(ids))&#10;    query = f&quot;SELECT id, content, metadata FROM documents WHERE id IN ({placeholders})&quot;&#10;    mysql_cursor.execute(query, tuple(ids))&#10;    rows = mysql_cursor.fetchall()&#10;    return [Document(page_content=row['content'], metadata=row['metadata']) for row in rows]&#10;&#10;# 3. Create MultiVectorRetriever&#10;retriever = MultiVectorRetriever(&#10;    vectorstore=pgvector_store,&#10;    docstore=fetch_docs_from_mysql,&#10;    id_key=&quot;id&quot;&#10;)&#10;```&#10;&#10;or &#10;```python&#10;docstore = InMemoryDocstore()&#10;&#10;# 2. Add documents to InMemoryDocstore&#10;doc_ids = [&quot;doc1&quot;, &quot;doc2&quot;]&#10;chunks = [&#10;    Document(page_content=&quot;First document about LangChain.&quot;, metadata={&quot;category&quot;: &quot;AI&quot;}),&#10;    Document(page_content=&quot;Second document about Vector Stores.&quot;, metadata={&quot;category&quot;: &quot;Database&quot;})&#10;]&#10;docstore.mset(list(zip(doc_ids, chunks)))&#10;&#10;# 3. Create MultiVectorRetriever&#10;retriever = MultiVectorRetriever(&#10;    vectorstore=pgvector_store,  # Same as before&#10;    docstore=docstore,           # InMemoryDocstore instance&#10;    id_key=&quot;id&quot;                  # ID used in vector store entries&#10;)&#10;```&#10;### Strategies for Mathematical operations&#10;- GPT models excel at predicting the next word and reasoning, but are not reliable for accurate mathematical calculations.&#10;- Strategies to handle mathematical operations:&#10;  - Use external tools or function calling (e.g., calculator APIs, Python code execution) for math tasks.&#10;  - Integrate tool-calling patterns so the LLM delegates math problems to a dedicated computation engine.&#10;  - Convert math queries into structured requests that trigger tool or function invocation.&#10;  - For classification-type math (e.g., &quot;is this a credit or debit&quot;), use the LLM to route the query to the appropriate tool.&#10;  - Always validate or post-process LLM-generated math answers with external logic or tools.&#10;&#10;### Strategies for Security Compliance&#10;- Use a hybrid model approach: classify and filter sensitive or secure data before sending to public or third-party models.&#10;- Apply rewrite-retrieve-read and multi-query retrieval patterns to detect and mitigate queries with ill intent or attempts to extract confidential information.&#10;- Implement prompt sanitization and validation to prevent prompt injection attacks.&#10;- Use access controls and audit logging for all LLM interactions involving sensitive data.&#10;- Prefer on-prem or private models for highly confidential workloads.&#10;- Regularly review and update security policies as new LLM capabilities and risks emerge.&#10;&#10;### Recursive Abstractive Processing for Tree-Organized Retrieval (RAPTOR) —  &#10;&lt;img width=&quot;1810&quot; height=&quot;1356&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/781bebc6-19d2-4814-af83-0983f081b6ba&quot; /&gt;&#10;&#10;- **Problem:**  &#10;  - RAG systems must handle:&#10;    - **Lower-level questions:** referencing specific facts in a single document.  &#10;    - **Higher-level questions:** synthesizing ideas across multiple documents.  &#10;  - Standard k-nearest neighbors (k-NN) retrieval on document chunks struggles to cover both types effectively.  &#10;&#10;- **Solution: RAPTOR Approach**&#10;  - **Step 1:** Create **document summaries** capturing higher-level concepts.  &#10;  - **Step 2:** **Embed and cluster** the documents based on semantic similarity.  &#10;  - **Step 3:** **Summarize each cluster**, producing higher-level abstractions.  &#10;  - **Step 4:** Repeat the process **recursively**, forming a **tree of summaries** with increasing abstraction.  &#10;&#10;- **Indexing:**  &#10;  - Index **both the summaries and the original documents** together.  &#10;  - Ensures coverage for **questions ranging from low-level facts to high-level concepts**.&#10;&#10;- **Benefit:**  &#10;  - Efficient handling of **multi-granular retrieval**.  &#10;  - Supports queries spanning **single facts to overarching ideas**.&#10; &#10;```python&#10;# 2. Split documents into chunks&#10;docs = [Document(page_content=doc) for doc in all_documents]  # all_documents is your text list&#10;splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)&#10;chunks = []&#10;for doc in docs:&#10;    chunks.extend(splitter.split_text(doc.page_content))&#10;&#10;# 3. Generate summaries for each chunk&#10;chunk_summaries = []&#10;for chunk in chunks:&#10;    summary = llm(f&quot;Summarize the following text concisely:\n{chunk}&quot;)&#10;    chunk_summaries.append(Document(page_content=summary))&#10;&#10;# 4. Cluster summaries&#10;num_clusters = 5&#10;summary_embeddings = [embeddings.embed_query(s.page_content) for s in chunk_summaries]&#10;kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(summary_embeddings)&#10;&#10;# 5. Summarize each cluster (recursive abstraction)&#10;cluster_summaries = []&#10;for cluster_idx in range(num_clusters):&#10;    cluster_docs = [chunk_summaries[i].page_content for i in range(len(chunk_summaries)) &#10;                    if kmeans.labels_[i] == cluster_idx]&#10;    cluster_text = &quot;\n&quot;.join(cluster_docs)&#10;    cluster_summary = llm(f&quot;Summarize the following cluster text:\n{cluster_text}&quot;)&#10;    cluster_summaries.append(Document(page_content=cluster_summary))&#10;&#10;# 6. Index both original chunks and cluster summaries in vector store&#10;vectorstore.add_documents(chunks + cluster_summaries)&#10;&#10;# 7. Query example&#10;query = &quot;Explain high-level ideas from the documents.&quot;&#10;results = vectorstore.similarity_search(query, k=3)&#10;for r in results:&#10;    print(r.page_content)&#10;```&#10;&#10;### ColBERT: Optimizing Embeddings&#10;&lt;img width=&quot;1400&quot; height=&quot;578&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/cc1d9783-9da8-4220-a1c4-e7b48beb5e16&quot; /&gt;&#10;&#10;- **Problem with standard embeddings:**  &#10;  - Fixed-length embeddings compress entire text into a single vector.  &#10;  - Useful for retrieval, but embedding irrelevant/redundant content can cause **hallucination** in LLM outputs.  &#10;&#10;- **ColBERT Approach:**  &#10;  1. **Contextual token embeddings:** Generate embeddings for **each token** in the document and query instead of a single vector.  &#10;  2. **Token-level similarity scoring:** Calculate similarity between **each query token** and **all document tokens**.  &#10;  3. **Aggregate scores:** For each query token, take the **maximum similarity** to any document token; sum these to get a **document-level score**.  &#10;&#10;- **Benefits:**  &#10;  - Provides **granular, token-level retrieval**.  &#10;  - Reduces irrelevant content impact.  &#10;  - Improves accuracy of retrieved context for LLMs.  &#10;&#10;- **Key takeaway:**  &#10;  - ColBERT is an embedding model designed to implement this **token-level scoring mechanism**, optimizing document retrieval for downstream applications.&#10;&#10;&#10;```python&#10;from ragatouille import RAGPretrainedModel&#10;# Load the ColBERT-based RAG model&#10;RAG = RAGPretrainedModel.from_pretrained(&quot;colbert-ir/colbertv2.0&quot;)&#10;&#10;# Example usage: encode documents or queries&#10;docs = [&quot;LangChain helps build applications with LLMs efficiently.&quot;]&#10;query = &quot;How to use LangChain for LLM apps?&quot;&#10;&#10;# Encode the documents (token-level embeddings)&#10;doc_embeddings = RAG.encode_documents(docs)&#10;&#10;# Encode the query&#10;query_embedding = RAG.encode_query(query)&#10;&#10;# Retrieve top-k relevant documents&#10;results = RAG.retrieve(query_embedding, k=3)&#10;for r in results:&#10;    print(r)&#10;```&#10;&#10;## Effective Strategies Using RAG&#10;&#10;- Part of our data can be stored in a **vector store** for operations like semantic search.  &#10;- Depending on the data type, it may be more efficient to store data in **RDBMS** or **NoSQL** or **Object Store**.  &#10;&#10;### Query Transformation&#10;- Incoming inputs can be **varied and uncontrolled**, e.g.:  &#10;  1. Event-driven data  &#10;  2. Future integration with other systems  &#10;  3. User interface inputs  &#10;- **Solution:** Transform queries into a format the system can reliably answer.  &#10;- Benefits:  &#10;  - Ensures consistent processing across diverse inputs.  &#10;  - Helps address security concerns and prevents misuse.  &#10;&#10;### Step-Back Prompting&#10;- Ask the model to **analyze or reflect** before giving a final answer.  &#10;- Helps identify assumptions, gaps, or errors in reasoning.  &#10;- Reduces mistakes in **multi-step reasoning**.&#10;&#10;### Subquestion Prompting&#10;- Breaks a **complex question into smaller subquestions**.  &#10;- Answer each subquestion individually and aggregate results.  &#10;- Improves accuracy for **multi-part queries** and reduces hallucination.&#10;&#10;### Rewritten Prompting aka Rewrite-Retrieve-Read &#10;- Reformulates a query to **clarify intent or simplify language**.  &#10;- Ensures the LLM focuses on the intended question.  &#10;- Reduces misinterpretation and improves output quality.&#10;  &#10;### Multi-Query Retrieval&#10;&lt;img width=&quot;1676&quot; height=&quot;596&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/595030dc-1a2b-4fa4-8a82-4ec58822a72d&quot; /&gt;&#10;&#10;- **Problem:** A single user query may not capture the full scope of information needed for a comprehensive answer.  &#10;&#10;- **Solution:** Multi-query retrieval strategy:&#10;  1. **Query Expansion:** Use an LLM to generate multiple related queries from the user’s initial query.  &#10;  2. **Parallel Retrieval:** Execute each generated query against the data source (vector store, RDBMS, etc.).  &#10;  3. **Context Aggregation:** Combine retrieved results as prompt context for the LLM.  &#10;  4. **Final Output:** The LLM generates a more complete and accurate answer using the aggregated context.  &#10;&#10;- **Benefit:** Improves coverage and accuracy by capturing **all relevant information** across the dataset.&#10;- **Example**: What is the weather in Sydney?”, give me 5 possible queries:&#10;    1. What is the current temperature in Sydney?&#10;    2. Are there any weather warnings or alerts in Sydney right now?&#10;    3. What is the forecast for Sydney for the next few hours?&#10;    4. Is it sunny, rainy, or cloudy in Sydney currently?&#10;    5. What is the humidity and wind speed in Sydney today?&#10;&#10;### RAG-Fusion&#10;&lt;img width=&quot;1284&quot; height=&quot;481&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/9597f42d-7e23-40f5-985e-cabef52035b3&quot; /&gt;&#10;&#10;RAG-Fusion (Retrieval-Augmented Generation with Fusion) is an extension of the multi-query retrieval strategy. It enhances the retrieval process by introducing a **final reranking step** using the **Reciprocal Rank Fusion (RRF)** algorithm.&#10;&#10;### Key Steps&#10;&#10;1. **Query Expansion:** Generate multiple related queries from the user's initial query.&#10;2. **Parallel Retrieval:** Execute each query independently against the data source (vector store, RDBMS, or search engine).&#10;3. **Reciprocal Rank Fusion (RRF):**  &#10;   - Each retrieved document has a rank for each query.  &#10;   - RRF combines these ranks into a single, unified ranking.  &#10;   - Formula: ```math  RRF Score = \sum_{i} \frac{1}{k + \text{rank}_i}``` where \(k\) is a constant (commonly 60) to reduce the effect of lower-ranked documents. a big contant so that the rank doesn't domainate and have advantage.&#10;   - The most relevant documents across all queries rise to the top.&#10;4. **Context Aggregation:** Use the top-ranked documents as context for the LLM.&#10;5. **Final Output:** LLM generates a more accurate and comprehensive answer using aggregated, reranked context.&#10;&#10;#### Benefits&#10;&#10;- Improves retrieval quality by prioritizing documents relevant to multiple query perspectives.&#10;- Handles queries with different scoring scales or distributions effectively.&#10;- Reduces noise from less relevant results while promoting consensus across queries.&#10;&#10;#### Use Cases&#10;&#10;1. **Legal or Compliance like ACMA or TCP or Security documenations:**  &#10;   - Searching across multiple regulations, rules, and previous incidents.  &#10;   - RRF helps surface the most relevant documents that are supported across different search formulations.&#10;&#10;2. **Enterprise Search:**  &#10;   - Documents across Telstra for example Flexcab.&#10;&#10;RAG-Fusion is particularly powerful when the information space is large and diverse, and a single query is insufficient to capture all relevant context.&#10;## Hypothetical Document Embeddings (HyDE)&#10;&lt;img width=&quot;460&quot; height=&quot;300&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/564420f8-9bd2-4f97-9e68-0e0dbcfb02f4&quot; /&gt;&#10;&#10;**HyDE** is a retrieval strategy that leverages LLMs to improve vector-based search by creating a **hypothetical document** from the user’s query. The idea is that the LLM can expand, paraphrase, or contextualize the query into a richer representation that is more semantically similar to relevant documents in the dataset.&#10;HyDE effectively **bridges the gap between natural language queries and document embeddings**, improving the relevance of retrieved documents in retrieval-augmented generation (RAG) systems.&#10;### How HyDE Works&#10;  - The user provides a query, e.g., `&quot;What is the current Balance and give step by step how you arrive at that.&quot;`&#10;  - An LLM generates a **hypothetical document** or passage as if it were an answer to the query.  &#10;  - The hypothetical document is converted into a **vector embedding** using the same embedding model as used for the document database.&#10;  - The embedding is used to perform a **vector search** against the document corpus.  &#10;  - The assumption: the LLM-generated document is **closer in embedding space** to relevant documents than the raw query. &#10;  - Top-k similar documents are retrieved and can then be fed as context to the LLM for final answer generation. &#10;&#10;### Benefits&#10;&#10;- **Query Expansion without Manual Effort:** LLM generates richer semantic content automatically.&#10;- **Better Retrieval Accuracy:** Hypothetical documents are often closer to relevant documents in embedding space than terse user queries.&#10;- **Works with Sparse Queries:** Short or ambiguous queries benefit from the additional context generated by the LLM.&#10;&#10;## Query Routing&#10;Query routing is a strategy used in retrieval or search systems to **direct a user query to the most relevant subset of data or service**. The goal is to improve retrieval efficiency, relevance, and response time. There are two main strategies: **logical routing** and **semantic routing**.&#10;&#10;### 1. Logical Routing&#10;Logical routing directs queries based on **predefined rules, categories, or metadata** associated with the documents or data sources.&#10;- Documents are classified or tagged into **logical partitions** (e.g., departments, product lines, regions).  &#10;- Queries are routed to the partition(s) that match certain **keywords, tags, or rules**.  &#10;&#10;**Example:**  &#10;```python&#10;class RouteQuery(BaseModel):&#10;    &quot;&quot;&quot;Route a user query to the most relevant datasource.&quot;&quot;&quot;&#10;    datasource: Literal[&quot;python_docs&quot;, &quot;js_docs&quot;] = Field(&#10;        ...,&#10;        description=&quot;Choose the most relevant datasource based on keywords.&quot;&#10;    )&#10;```&#10;---&#10;&#10;### 2. Semantic Routing&#10;Semantic routing directs queries based on **meaning or intent**, often using embeddings, LLMs, or vector similarity, rather than strict keywords or rules.&#10;&#10;**How it works:**  &#10;- Queries are converted into **semantic representations** (embeddings).  &#10;- Documents or partitions are also represented in the same embedding space.  &#10;- The system routes the query to the **most semantically relevant subset**.  &#10;&#10;**Example:**  &#10;```python&#10;physics_template = &quot;&quot;&quot;You are a very smart physics professor. You are great at     answering questions about physics in a concise and easy-to-understand manner.     When you don't know the answer to a question, you admit that you don't know. Here is a question: {query}&quot;&quot;&quot;&#10;math_template = &quot;&quot;&quot;You are a very good mathematician. You are great at answering     math questions. You are so good because you are able to break down hard     problems into their component parts, answer the component parts, and then     put them together to answer the broader question. Here is a question: {query}&quot;&quot;&quot;&#10;&#10;# Embed prompts&#10;embeddings = OpenAIEmbeddings()&#10;prompt_templates = [physics_template, math_template]&#10;prompt_embeddings = embeddings.embed_documents(prompt_templates)&#10;&#10;# Route question to prompt&#10;@chain&#10;def prompt_router(query):&#10;    query_embedding = embeddings.embed_query(query)&#10;    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]&#10;    most_similar = prompt_templates[similarity.argmax()]&#10;    print(&quot;Using MATH&quot; if most_similar == math_template else &quot;Using PHYSICS&quot;)&#10;    return PromptTemplate.from_template(most_similar)&#10;```&#10;**Use Cases:**  &#10;- Logical routing: Enterprise databases, departmental knowledge bases, FAQs.  &#10;- Semantic routing: RAG systems, multi-domain knowledge search, chatbots, recommendation engines.  &#10;&#10;## Query Construction&#10;&#10;Query construction is the process of **transforming a user's natural language query into a structured query** that can be executed against various data sources. This is essential for retrieval systems, vector stores, and relational databases.&#10;&#10;---&#10;&#10;## 1. Text-to-Metadata Conversion&#10;&#10;Vector stores often support **metadata-based filtering**, allowing more precise searches. During the embedding process:&#10;&#10;- **Attach metadata**: Each vector can be associated with key-value pairs (e.g., document type, author, date)&#10;- **Filter on query**: When performing a search, you can specify metadata filters to narrow down results&#10;&#10;1. **Embed documents and attach metadata:**&#10;   ```python&#10;   {&#10;       &quot;text&quot;: &quot;Annual financial report 2024&quot;,&#10;       &quot;embedding&quot;: [...],&#10;       &quot;metadata&quot;: {&quot;year&quot;: 2024, &quot;type&quot;: &quot;report&quot;}&#10;   }&#10;   ```&#10;&#10;2. **Construct a query with metadata filters:**&#10;   ```python&#10;   query_embedding = embed(&quot;Financial summary 2024&quot;)&#10;   results = vector_store.search(&#10;       query_embedding,&#10;       filter={&quot;year&quot;: 2024, &quot;type&quot;: &quot;report&quot;}&#10;   )&#10;   ```&#10;&#10;### Self-Querying Approach&#10;An LLM can translate a natural language query into metadata filters automatically.&#10;&#10;**Example:** &quot;Show event by event name and event publisher&quot; → `{&quot;event_name&quot;: &quot;payment event&quot;, &quot;event_publisher&quot;: &quot;payment system&quot;}`&#10;&#10;---&#10;&#10;## 2. Text-to-SQL&#10;Relational databases require SQL queries, which are not naturally compatible with human language. Text-to-SQL allows LLMs to translate user queries into SQL queries.&#10;&#10;### Strategies for Effective Translation&#10;#### Database Description (Grounding SQL)&#10;Provide the LLM with table definitions using `CREATE TABLE` statements, including:&#10;- Column names and types&#10;- Sample rows (typically 2-3 examples)&#10;This helps the LLM generate syntactically and semantically correct SQL queries.&#10;&#10;#### Few-Shot Examples&#10;&#10;Include question-to-SQL examples in the prompt to guide query generation.&#10;&#10;**Example:**&#10;* Q: Total salary of HR employees&#10;* SQL: SELECT SUM(salary) FROM employees WHERE department='HR';&#10;&#10;#### Error Handling&#10;When SQL execution fails, the LLM can:&#10;- Regenerate the query based on error messages&#10;- Repair the existing query with corrections&#10;- Improve robustness when working with dynamic or unfamiliar schemas&#10;&#10;## Summary&#10;| Query Construction Type | Description | Key Techniques |&#10;|------------------------|-------------|----------------|&#10;| **Text-to-Metadata** | Attach metadata to vectors and filter during search | Metadata key-value pairs, LLM-assisted filter extraction |&#10;| **Text-to-SQL** | Translate natural language to SQL queries | Database description, few-shot examples, error recovery |&#10;&#10;These strategies enable LLMs and retrieval systems to effectively interact with both unstructured and structured data, ensuring more accurate and relevant results." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/content/posts/system_design/core_design_patterns.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/system_design/core_design_patterns.md" />
              <option name="originalContent" value="+++&#10;date = '2024-05-03T12:44:47+10:00'&#10;draft = false&#10;title = '23 Core Design Patterns'&#10;tags = ['Core Design Patterns', 'Interview']&#10;+++&#10;&#10;About Core Design Patterns.&#10;&#10;## Table of Contents&#10;&#10;- [Creational Patterns](#creational-patterns)&#10;  - [Singleton](#singleton)&#10;  - [Factory Method](#factory-method)&#10;  - [Abstract Factory](#abstract-factory)&#10;  - [Builder](#builder)&#10;  - [Prototype](#prototype)&#10;- [Structural Patterns](#structural-patterns)&#10;  - [Adapter](#adapter)&#10;  - [Bridge](#bridge)&#10;  - [Composite](#composite)&#10;  - [Decorator](#decorator)&#10;  - [Facade](#facade)&#10;  - [Flyweight](#flyweight)&#10;  - [Proxy](#proxy)&#10;- [Behavioral Patterns](#behavioral-patterns)&#10;  - [Chain of Responsibility](#chain-of-responsibility)&#10;  - [Command](#command)&#10;  - [Interpreter](#interpreter)&#10;  - [Iterator](#iterator)&#10;  - [Mediator](#mediator)&#10;  - [Memento](#memento)&#10;  - [Observer](#observer)&#10;  - [State](#state)&#10;  - [Strategy](#strategy)&#10;  - [Template Method](#template-method)&#10;  - [Visitor](#visitor)&#10;- [Summary](#summary)&#10;&#10;---&#10;&#10;## Creational Patterns&#10;&#10;### Singleton&#10;&#10;Ensures a class has only one instance and provides a global point of access to it.&#10;&#10;**Example (Java):**&#10;```java&#10;public class Singleton {&#10;    private static Singleton uniqueInstance;&#10;    private Singleton() {}&#10;    public static synchronized Singleton getInstance() {&#10;        if (uniqueInstance == null) {&#10;            uniqueInstance = new Singleton();&#10;        }&#10;        return uniqueInstance;&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need a single shared resource (e.g., config, logger, cache).&#10;&#10;**When not to use:**  &#10;&#10;- When you need multiple instances (e.g., for testing, parallelism).&#10;- Can introduce hidden dependencies and global state.&#10;&#10;---&#10;&#10;### Factory Method&#10;&#10;Defines an interface for creating an object, but lets subclasses decide which class to instantiate.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class Animal {&#10;    public abstract String speak();&#10;}&#10;&#10;public class Dog extends Animal {&#10;    public String speak() { return &quot;Woof!&quot;; }&#10;}&#10;&#10;public class Cat extends Animal {&#10;    public String speak() { return &quot;Meow!&quot;; }&#10;}&#10;&#10;public abstract class AnimalFactory {&#10;    public abstract Animal createAnimal();&#10;}&#10;&#10;public class DogFactory extends AnimalFactory {&#10;    public Animal createAnimal() { return new Dog(); }&#10;}&#10;&#10;public class CatFactory extends AnimalFactory {&#10;    public Animal createAnimal() { return new Cat(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When a class can't anticipate the type of objects it needs to create.&#10;&#10;**When not to use:**  &#10;&#10;- When object creation is simple and doesn't need abstraction.&#10;&#10;---&#10;&#10;### Abstract Factory&#10;&#10;Provides an interface for creating families of related or dependent objects without specifying their concrete classes.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Button {&#10;    void paint();&#10;}&#10;&#10;public class WinButton implements Button {&#10;    public void paint() { System.out.println(&quot;Windows Button&quot;); }&#10;}&#10;&#10;public class MacButton implements Button {&#10;    public void paint() { System.out.println(&quot;Mac Button&quot;); }&#10;}&#10;&#10;public interface GUIFactory {&#10;    Button createButton();&#10;}&#10;&#10;public class WinFactory implements GUIFactory {&#10;    public Button createButton() { return new WinButton(); }&#10;}&#10;&#10;public class MacFactory implements GUIFactory {&#10;    public Button createButton() { return new MacButton(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to create families of related objects.&#10;&#10;**When not to use:**  &#10;&#10;- When products don't need to be related.&#10;&#10;---&#10;&#10;### Builder&#10;&#10;Separates the construction of a complex object from its representation.&#10;&#10;**Example (Java):**&#10;```java&#10;public class Burger {&#10;    private boolean cheese;&#10;    private boolean lettuce;&#10;&#10;    public static class Builder {&#10;        private boolean cheese;&#10;        private boolean lettuce;&#10;&#10;        public Builder addCheese() {&#10;            cheese = true;&#10;            return this;&#10;        }&#10;        public Builder addLettuce() {&#10;            lettuce = true;&#10;            return this;&#10;        }&#10;        public Burger build() {&#10;            Burger burger = new Burger();&#10;            burger.cheese = this.cheese;&#10;            burger.lettuce = this.lettuce;&#10;            return burger;&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When constructing complex objects step by step.&#10;&#10;**When not to use:**  &#10;&#10;- For simple objects with few parameters.&#10;&#10;---&#10;&#10;### Prototype&#10;&#10;Creates new objects by copying an existing object.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class Prototype implements Cloneable {&#10;    public Prototype clone() {&#10;        try {&#10;            return (Prototype) super.clone();&#10;        } catch (CloneNotSupportedException e) {&#10;            throw new AssertionError();&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When object creation is costly and similar objects are needed.&#10;&#10;**When not to use:**  &#10;&#10;- When objects are simple or copying is not needed.&#10;&#10;---&#10;&#10;## Structural Patterns&#10;&#10;### Adapter&#10;&#10;Allows incompatible interfaces to work together.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface USASocket {&#10;    int voltage();&#10;}&#10;&#10;public class EuropeanSocket {&#10;    public int voltage() { return 230; }&#10;}&#10;&#10;public class SocketAdapter implements USASocket {&#10;    private EuropeanSocket europeanSocket;&#10;    public SocketAdapter(EuropeanSocket socket) {&#10;        this.europeanSocket = socket;&#10;    }&#10;    public int voltage() {&#10;        return europeanSocket.voltage();&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When integrating with legacy or third-party code.&#10;&#10;**When not to use:**  &#10;&#10;- When you can refactor code directly.&#10;&#10;---&#10;&#10;### Bridge&#10;&#10;Separates abstraction from implementation.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface DrawingAPI {&#10;    void drawCircle(int x, int y, int r);&#10;}&#10;&#10;public class DrawingAPI1 implements DrawingAPI {&#10;    public void drawCircle(int x, int y, int r) {&#10;        System.out.println(&quot;API1: Circle at &quot; + x + &quot;,&quot; + y + &quot; radius &quot; + r);&#10;    }&#10;}&#10;&#10;public abstract class Shape {&#10;    protected DrawingAPI api;&#10;    public Shape(DrawingAPI api) { this.api = api; }&#10;    public abstract void draw();&#10;}&#10;&#10;public class Circle extends Shape {&#10;    private int x, y, r;&#10;    public Circle(int x, int y, int r, DrawingAPI api) {&#10;        super(api);&#10;        this.x = x; this.y = y; this.r = r;&#10;    }&#10;    public void draw() { api.drawCircle(x, y, r); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When abstraction and implementation should vary independently.&#10;&#10;**When not to use:**  &#10;&#10;- When only one implementation is needed.&#10;&#10;---&#10;&#10;### Composite&#10;&#10;Composes objects into tree structures.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Component {&#10;    void operation();&#10;}&#10;&#10;public class Leaf implements Component {&#10;    public void operation() { System.out.println(&quot;Leaf&quot;); }&#10;}&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;&#10;public class Composite implements Component {&#10;    private List&lt;Component&gt; children = new ArrayList&lt;&gt;();&#10;    public void add(Component component) { children.add(component); }&#10;    public void operation() {&#10;        for (Component child : children) {&#10;            child.operation();&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to treat individual and composite objects uniformly.&#10;&#10;**When not to use:**  &#10;&#10;- When hierarchy is not needed.&#10;&#10;---&#10;&#10;### Decorator&#10;&#10;Adds new functionality to an object dynamically.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Coffee {&#10;    int cost();&#10;}&#10;&#10;public class SimpleCoffee implements Coffee {&#10;    public int cost() { return 5; }&#10;}&#10;&#10;public class MilkDecorator implements Coffee {&#10;    private Coffee coffee;&#10;    public MilkDecorator(Coffee coffee) { this.coffee = coffee; }&#10;    public int cost() { return coffee.cost() + 2; }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to add responsibilities to objects dynamically.&#10;&#10;**When not to use:**  &#10;&#10;- When subclassing is simpler.&#10;&#10;---&#10;&#10;### Facade&#10;&#10;Provides a simplified interface to a complex subsystem.&#10;&#10;**Example (Java):**&#10;```java&#10;public class CPU { public void freeze() {} }&#10;public class Memory { public void load(int pos, String data) {} }&#10;&#10;public class ComputerFacade {&#10;    private CPU cpu;&#10;    private Memory memory;&#10;    public ComputerFacade() {&#10;        cpu = new CPU();&#10;        memory = new Memory();&#10;    }&#10;    public void start() {&#10;        cpu.freeze();&#10;        memory.load(0, &quot;data&quot;);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you want to provide a simple interface to a complex system.&#10;&#10;**When not to use:**  &#10;&#10;- When subsystem is already simple.&#10;&#10;---&#10;&#10;### Flyweight&#10;&#10;Reduces memory usage by sharing data.&#10;&#10;**Example (Java):**&#10;```java&#10;import java.util.HashMap;&#10;import java.util.Map;&#10;&#10;public class Flyweight {&#10;    private String shared;&#10;    public Flyweight(String shared) { this.shared = shared; }&#10;}&#10;&#10;public class FlyweightFactory {&#10;    private Map&lt;String, Flyweight&gt; flyweights = new HashMap&lt;&gt;();&#10;    public Flyweight getFlyweight(String shared) {&#10;        if (!flyweights.containsKey(shared)) {&#10;            flyweights.put(shared, new Flyweight(shared));&#10;        }&#10;        return flyweights.get(shared);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When many objects share common data.&#10;&#10;**When not to use:**  &#10;&#10;- When objects are unique.&#10;&#10;---&#10;&#10;### Proxy&#10;&#10;Provides a surrogate for another object.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Subject {&#10;    void request();&#10;}&#10;&#10;public class RealSubject implements Subject {&#10;    public void request() { System.out.println(&quot;RealSubject&quot;); }&#10;}&#10;&#10;public class Proxy implements Subject {&#10;    private RealSubject real;&#10;    public Proxy(RealSubject real) { this.real = real; }&#10;    public void request() {&#10;        System.out.println(&quot;Proxy: Checking access&quot;);&#10;        real.request();&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For access control, lazy loading, logging.&#10;&#10;**When not to use:**  &#10;&#10;- When direct access is acceptable.&#10;&#10;---&#10;&#10;## Behavioral Patterns&#10;&#10;### Chain of Responsibility&#10;&#10;Passes a request along a chain of handlers.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class Handler {&#10;    protected Handler next;&#10;    public void setNext(Handler next) { this.next = next; }&#10;    public void handle(String request) {&#10;        if (next != null) next.handle(request);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When multiple objects can handle a request.&#10;&#10;**When not to use:**  &#10;&#10;- When only one handler is needed.&#10;&#10;---&#10;&#10;### Command&#10;&#10;Encapsulates a request as an object.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Command {&#10;    void execute();&#10;}&#10;&#10;public class LightOnCommand implements Command {&#10;    public void execute() { System.out.println(&quot;Light On&quot;); }&#10;}&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;&#10;public class Remote {&#10;    private List&lt;Command&gt; commands = new ArrayList&lt;&gt;();&#10;    public void addCommand(Command cmd) { commands.add(cmd); }&#10;    public void run() {&#10;        for (Command cmd : commands) { cmd.execute(); }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For undo/redo, queuing, logging.&#10;&#10;**When not to use:**  &#10;&#10;- When simple method calls suffice.&#10;&#10;---&#10;&#10;### Interpreter&#10;&#10;Defines a grammar and provides an interpreter.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Expression {&#10;    int interpret();&#10;}&#10;&#10;public class Number implements Expression {&#10;    private int value;&#10;    public Number(int value) { this.value = value; }&#10;    public int interpret() { return value; }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For languages, expressions, parsing.&#10;&#10;**When not to use:**  &#10;&#10;- For simple or infrequent grammar.&#10;&#10;---&#10;&#10;### Iterator&#10;&#10;Provides a way to access elements sequentially.&#10;&#10;**Example (Java):**&#10;```java&#10;import java.util.Iterator;&#10;import java.util.List;&#10;&#10;public class MyIterator implements Iterator&lt;String&gt; {&#10;    private List&lt;String&gt; collection;&#10;    private int index = 0;&#10;    public MyIterator(List&lt;String&gt; collection) { this.collection = collection; }&#10;    public boolean hasNext() { return index &lt; collection.size(); }&#10;    public String next() { return collection.get(index++); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to traverse a collection.&#10;&#10;**When not to use:**  &#10;&#10;- When direct access is sufficient.&#10;&#10;---&#10;&#10;### Mediator&#10;&#10;Encapsulates how objects interact.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Mediator {&#10;    void notify(Component sender, String event);&#10;}&#10;&#10;public class Component {&#10;    private Mediator mediator;&#10;    public Component(Mediator mediator) { this.mediator = mediator; }&#10;    public void doSomething() { mediator.notify(this, &quot;event&quot;); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When objects communicate in complex ways.&#10;&#10;**When not to use:**  &#10;&#10;- When communication is simple.&#10;&#10;---&#10;&#10;### Memento&#10;&#10;Captures and restores an object's state.&#10;&#10;**Example (Java):**&#10;```java&#10;public class Memento {&#10;    private String state;&#10;    public Memento(String state) { this.state = state; }&#10;    public String getState() { return state; }&#10;}&#10;&#10;public class Originator {&#10;    private String state;&#10;    public void setState(String state) { this.state = state; }&#10;    public Memento save() { return new Memento(state); }&#10;    public void restore(Memento memento) { state = memento.getState(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For undo/redo functionality.&#10;&#10;**When not to use:**  &#10;&#10;- When state is simple or not needed.&#10;&#10;---&#10;&#10;### Observer&#10;&#10;One-to-many dependency so dependents are notified of changes.&#10;&#10;**Example (Java):**&#10;```java&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;&#10;public interface Observer {&#10;    void update();&#10;}&#10;&#10;public class Subject {&#10;    private List&lt;Observer&gt; observers = new ArrayList&lt;&gt;();&#10;    public void attach(Observer obs) { observers.add(obs); }&#10;    public void notifyObservers() {&#10;        for (Observer obs : observers) { obs.update(); }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For event handling, UI updates.&#10;&#10;**When not to use:**  &#10;&#10;- When only one object needs notification.&#10;&#10;---&#10;&#10;### State&#10;&#10;Alters behavior when internal state changes.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface State {&#10;    void handle();&#10;}&#10;&#10;public class Context {&#10;    private State state;&#10;    public Context(State state) { this.state = state; }&#10;    public void request() { state.handle(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When object behavior depends on state.&#10;&#10;**When not to use:**  &#10;&#10;- When state changes are rare.&#10;&#10;---&#10;&#10;### Strategy&#10;&#10;Encapsulates interchangeable algorithms.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Strategy {&#10;    int execute(int[] data);&#10;}&#10;&#10;public class SortStrategy implements Strategy {&#10;    public int execute(int[] data) {&#10;        java.util.Arrays.sort(data);&#10;        return data[0]; // Just for demonstration&#10;    }&#10;}&#10;&#10;public class Context {&#10;    private Strategy strategy;&#10;    public Context(Strategy strategy) { this.strategy = strategy; }&#10;    public int doTask(int[] data) { return strategy.execute(data); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When multiple algorithms are needed.&#10;&#10;**When not to use:**  &#10;&#10;- When only one algorithm is used.&#10;&#10;---&#10;&#10;### Template Method&#10;&#10;Defines the skeleton of an algorithm, deferring steps to subclasses.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class AbstractClass {&#10;    public final void templateMethod() {&#10;        step1();&#10;        step2();&#10;    }&#10;    protected abstract void step1();&#10;    protected abstract void step2();&#10;}&#10;&#10;public class ConcreteClass extends AbstractClass {&#10;    protected void step1() { System.out.println(&quot;Step 1&quot;); }&#10;    protected void step2() { System.out.println(&quot;Step 2&quot;); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When algorithms have invariant steps.&#10;&#10;**When not to use:**  &#10;&#10;- When steps never change.&#10;&#10;---&#10;&#10;### Visitor&#10;&#10;Represents an operation to be performed on elements of an object structure.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Visitor {&#10;    void visit(Element element);&#10;}&#10;&#10;public interface Element {&#10;    void accept(Visitor visitor);&#10;}&#10;&#10;public class ConcreteElement implements Element {&#10;    public void accept(Visitor visitor) { visitor.visit(this); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to perform operations on object structures.&#10;&#10;**When not to use:**  &#10;&#10;- When object structure rarely changes.&#10;&#10;---&#10;&#10;## Summary&#10;&#10;The 23 core design patterns (Gang of Four) are essential tools for software engineers.  &#10;They provide proven solutions to common problems in software design, improve code maintainability, and are frequently discussed in interviews.  &#10;Understanding these patterns helps you write flexible, scalable, and robust code.&#10;" />
              <option name="updatedContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = '23 Core Design Patterns'&#10;tags = ['Core Design Patterns', 'Interview']&#10;+++&#10;&#10;Core design patterns are proven solutions to common software engineering problems. They help structure code for flexibility, scalability, and maintainability. Mastering these patterns is essential for interviews and for building robust, reusable, and understandable software systems.&#10;&#10;## Table of Contents&#10;&#10;- [Creational Patterns](#creational-patterns)&#10;  - [Singleton](#singleton)&#10;  - [Factory Method](#factory-method)&#10;  - [Abstract Factory](#abstract-factory)&#10;  - [Builder](#builder)&#10;  - [Prototype](#prototype)&#10;- [Structural Patterns](#structural-patterns)&#10;  - [Adapter](#adapter)&#10;  - [Bridge](#bridge)&#10;  - [Composite](#composite)&#10;  - [Decorator](#decorator)&#10;  - [Facade](#facade)&#10;  - [Flyweight](#flyweight)&#10;  - [Proxy](#proxy)&#10;- [Behavioral Patterns](#behavioral-patterns)&#10;  - [Chain of Responsibility](#chain-of-responsibility)&#10;  - [Command](#command)&#10;  - [Interpreter](#interpreter)&#10;  - [Iterator](#iterator)&#10;  - [Mediator](#mediator)&#10;  - [Memento](#memento)&#10;  - [Observer](#observer)&#10;  - [State](#state)&#10;  - [Strategy](#strategy)&#10;  - [Template Method](#template-method)&#10;  - [Visitor](#visitor)&#10;- [Summary](#summary)&#10;&#10;---&#10;&#10;## Creational Patterns&#10;&#10;### Singleton&#10;&#10;Ensures a class has only one instance and provides a global point of access to it.&#10;&#10;**Example (Java):**&#10;```java&#10;public class Singleton {&#10;    private static Singleton uniqueInstance;&#10;    private Singleton() {}&#10;    public static synchronized Singleton getInstance() {&#10;        if (uniqueInstance == null) {&#10;            uniqueInstance = new Singleton();&#10;        }&#10;        return uniqueInstance;&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need a single shared resource (e.g., config, logger, cache).&#10;&#10;**When not to use:**  &#10;&#10;- When you need multiple instances (e.g., for testing, parallelism).&#10;- Can introduce hidden dependencies and global state.&#10;&#10;---&#10;&#10;### Factory Method&#10;&#10;Defines an interface for creating an object, but lets subclasses decide which class to instantiate.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class Animal {&#10;    public abstract String speak();&#10;}&#10;&#10;public class Dog extends Animal {&#10;    public String speak() { return &quot;Woof!&quot;; }&#10;}&#10;&#10;public class Cat extends Animal {&#10;    public String speak() { return &quot;Meow!&quot;; }&#10;}&#10;&#10;public abstract class AnimalFactory {&#10;    public abstract Animal createAnimal();&#10;}&#10;&#10;public class DogFactory extends AnimalFactory {&#10;    public Animal createAnimal() { return new Dog(); }&#10;}&#10;&#10;public class CatFactory extends AnimalFactory {&#10;    public Animal createAnimal() { return new Cat(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When a class can't anticipate the type of objects it needs to create.&#10;&#10;**When not to use:**  &#10;&#10;- When object creation is simple and doesn't need abstraction.&#10;&#10;---&#10;&#10;### Abstract Factory&#10;&#10;Provides an interface for creating families of related or dependent objects without specifying their concrete classes.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Button {&#10;    void paint();&#10;}&#10;&#10;public class WinButton implements Button {&#10;    public void paint() { System.out.println(&quot;Windows Button&quot;); }&#10;}&#10;&#10;public class MacButton implements Button {&#10;    public void paint() { System.out.println(&quot;Mac Button&quot;); }&#10;}&#10;&#10;public interface GUIFactory {&#10;    Button createButton();&#10;}&#10;&#10;public class WinFactory implements GUIFactory {&#10;    public Button createButton() { return new WinButton(); }&#10;}&#10;&#10;public class MacFactory implements GUIFactory {&#10;    public Button createButton() { return new MacButton(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to create families of related objects.&#10;&#10;**When not to use:**  &#10;&#10;- When products don't need to be related.&#10;&#10;---&#10;&#10;### Builder&#10;&#10;Separates the construction of a complex object from its representation.&#10;&#10;**Example (Java):**&#10;```java&#10;public class Burger {&#10;    private boolean cheese;&#10;    private boolean lettuce;&#10;&#10;    public static class Builder {&#10;        private boolean cheese;&#10;        private boolean lettuce;&#10;&#10;        public Builder addCheese() {&#10;            cheese = true;&#10;            return this;&#10;        }&#10;        public Builder addLettuce() {&#10;            lettuce = true;&#10;            return this;&#10;        }&#10;        public Burger build() {&#10;            Burger burger = new Burger();&#10;            burger.cheese = this.cheese;&#10;            burger.lettuce = this.lettuce;&#10;            return burger;&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When constructing complex objects step by step.&#10;&#10;**When not to use:**  &#10;&#10;- For simple objects with few parameters.&#10;&#10;---&#10;&#10;### Prototype&#10;&#10;Creates new objects by copying an existing object.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class Prototype implements Cloneable {&#10;    public Prototype clone() {&#10;        try {&#10;            return (Prototype) super.clone();&#10;        } catch (CloneNotSupportedException e) {&#10;            throw new AssertionError();&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When object creation is costly and similar objects are needed.&#10;&#10;**When not to use:**  &#10;&#10;- When objects are simple or copying is not needed.&#10;&#10;---&#10;&#10;## Structural Patterns&#10;&#10;### Adapter&#10;&#10;Allows incompatible interfaces to work together.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface USASocket {&#10;    int voltage();&#10;}&#10;&#10;public class EuropeanSocket {&#10;    public int voltage() { return 230; }&#10;}&#10;&#10;public class SocketAdapter implements USASocket {&#10;    private EuropeanSocket europeanSocket;&#10;    public SocketAdapter(EuropeanSocket socket) {&#10;        this.europeanSocket = socket;&#10;    }&#10;    public int voltage() {&#10;        return europeanSocket.voltage();&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When integrating with legacy or third-party code.&#10;&#10;**When not to use:**  &#10;&#10;- When you can refactor code directly.&#10;&#10;---&#10;&#10;### Bridge&#10;&#10;Separates abstraction from implementation.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface DrawingAPI {&#10;    void drawCircle(int x, int y, int r);&#10;}&#10;&#10;public class DrawingAPI1 implements DrawingAPI {&#10;    public void drawCircle(int x, int y, int r) {&#10;        System.out.println(&quot;API1: Circle at &quot; + x + &quot;,&quot; + y + &quot; radius &quot; + r);&#10;    }&#10;}&#10;&#10;public abstract class Shape {&#10;    protected DrawingAPI api;&#10;    public Shape(DrawingAPI api) { this.api = api; }&#10;    public abstract void draw();&#10;}&#10;&#10;public class Circle extends Shape {&#10;    private int x, y, r;&#10;    public Circle(int x, int y, int r, DrawingAPI api) {&#10;        super(api);&#10;        this.x = x; this.y = y; this.r = r;&#10;    }&#10;    public void draw() { api.drawCircle(x, y, r); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When abstraction and implementation should vary independently.&#10;&#10;**When not to use:**  &#10;&#10;- When only one implementation is needed.&#10;&#10;---&#10;&#10;### Composite&#10;&#10;Composes objects into tree structures.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Component {&#10;    void operation();&#10;}&#10;&#10;public class Leaf implements Component {&#10;    public void operation() { System.out.println(&quot;Leaf&quot;); }&#10;}&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;&#10;public class Composite implements Component {&#10;    private List&lt;Component&gt; children = new ArrayList&lt;&gt;();&#10;    public void add(Component component) { children.add(component); }&#10;    public void operation() {&#10;        for (Component child : children) {&#10;            child.operation();&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to treat individual and composite objects uniformly.&#10;&#10;**When not to use:**  &#10;&#10;- When hierarchy is not needed.&#10;&#10;---&#10;&#10;### Decorator&#10;&#10;Adds new functionality to an object dynamically.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Coffee {&#10;    int cost();&#10;}&#10;&#10;public class SimpleCoffee implements Coffee {&#10;    public int cost() { return 5; }&#10;}&#10;&#10;public class MilkDecorator implements Coffee {&#10;    private Coffee coffee;&#10;    public MilkDecorator(Coffee coffee) { this.coffee = coffee; }&#10;    public int cost() { return coffee.cost() + 2; }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to add responsibilities to objects dynamically.&#10;&#10;**When not to use:**  &#10;&#10;- When subclassing is simpler.&#10;&#10;---&#10;&#10;### Facade&#10;&#10;Provides a simplified interface to a complex subsystem.&#10;&#10;**Example (Java):**&#10;```java&#10;public class CPU { public void freeze() {} }&#10;public class Memory { public void load(int pos, String data) {} }&#10;&#10;public class ComputerFacade {&#10;    private CPU cpu;&#10;    private Memory memory;&#10;    public ComputerFacade() {&#10;        cpu = new CPU();&#10;        memory = new Memory();&#10;    }&#10;    public void start() {&#10;        cpu.freeze();&#10;        memory.load(0, &quot;data&quot;);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you want to provide a simple interface to a complex system.&#10;&#10;**When not to use:**  &#10;&#10;- When subsystem is already simple.&#10;&#10;---&#10;&#10;### Flyweight&#10;&#10;Reduces memory usage by sharing data.&#10;&#10;**Example (Java):**&#10;```java&#10;import java.util.HashMap;&#10;import java.util.Map;&#10;&#10;public class Flyweight {&#10;    private String shared;&#10;    public Flyweight(String shared) { this.shared = shared; }&#10;}&#10;&#10;public class FlyweightFactory {&#10;    private Map&lt;String, Flyweight&gt; flyweights = new HashMap&lt;&gt;();&#10;    public Flyweight getFlyweight(String shared) {&#10;        if (!flyweights.containsKey(shared)) {&#10;            flyweights.put(shared, new Flyweight(shared));&#10;        }&#10;        return flyweights.get(shared);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When many objects share common data.&#10;&#10;**When not to use:**  &#10;&#10;- When objects are unique.&#10;&#10;---&#10;&#10;### Proxy&#10;&#10;Provides a surrogate for another object.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Subject {&#10;    void request();&#10;}&#10;&#10;public class RealSubject implements Subject {&#10;    public void request() { System.out.println(&quot;RealSubject&quot;); }&#10;}&#10;&#10;public class Proxy implements Subject {&#10;    private RealSubject real;&#10;    public Proxy(RealSubject real) { this.real = real; }&#10;    public void request() {&#10;        System.out.println(&quot;Proxy: Checking access&quot;);&#10;        real.request();&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For access control, lazy loading, logging.&#10;&#10;**When not to use:**  &#10;&#10;- When direct access is acceptable.&#10;&#10;---&#10;&#10;## Behavioral Patterns&#10;&#10;### Chain of Responsibility&#10;&#10;Passes a request along a chain of handlers.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class Handler {&#10;    protected Handler next;&#10;    public void setNext(Handler next) { this.next = next; }&#10;    public void handle(String request) {&#10;        if (next != null) next.handle(request);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When multiple objects can handle a request.&#10;&#10;**When not to use:**  &#10;&#10;- When only one handler is needed.&#10;&#10;---&#10;&#10;### Command&#10;&#10;Encapsulates a request as an object.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Command {&#10;    void execute();&#10;}&#10;&#10;public class LightOnCommand implements Command {&#10;    public void execute() { System.out.println(&quot;Light On&quot;); }&#10;}&#10;&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;&#10;public class Remote {&#10;    private List&lt;Command&gt; commands = new ArrayList&lt;&gt;();&#10;    public void addCommand(Command cmd) { commands.add(cmd); }&#10;    public void run() {&#10;        for (Command cmd : commands) { cmd.execute(); }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For undo/redo, queuing, logging.&#10;&#10;**When not to use:**  &#10;&#10;- When simple method calls suffice.&#10;&#10;---&#10;&#10;### Interpreter&#10;&#10;Defines a grammar and provides an interpreter.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Expression {&#10;    int interpret();&#10;}&#10;&#10;public class Number implements Expression {&#10;    private int value;&#10;    public Number(int value) { this.value = value; }&#10;    public int interpret() { return value; }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For languages, expressions, parsing.&#10;&#10;**When not to use:**  &#10;&#10;- For simple or infrequent grammar.&#10;&#10;---&#10;&#10;### Iterator&#10;&#10;Provides a way to access elements sequentially.&#10;&#10;**Example (Java):**&#10;```java&#10;import java.util.Iterator;&#10;import java.util.List;&#10;&#10;public class MyIterator implements Iterator&lt;String&gt; {&#10;    private List&lt;String&gt; collection;&#10;    private int index = 0;&#10;    public MyIterator(List&lt;String&gt; collection) { this.collection = collection; }&#10;    public boolean hasNext() { return index &lt; collection.size(); }&#10;    public String next() { return collection.get(index++); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to traverse a collection.&#10;&#10;**When not to use:**  &#10;&#10;- When direct access is sufficient.&#10;&#10;---&#10;&#10;### Mediator&#10;&#10;Encapsulates how objects interact.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Mediator {&#10;    void notify(Component sender, String event);&#10;}&#10;&#10;public class Component {&#10;    private Mediator mediator;&#10;    public Component(Mediator mediator) { this.mediator = mediator; }&#10;    public void doSomething() { mediator.notify(this, &quot;event&quot;); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When objects communicate in complex ways.&#10;&#10;**When not to use:**  &#10;&#10;- When communication is simple.&#10;&#10;---&#10;&#10;### Memento&#10;&#10;Captures and restores an object's state.&#10;&#10;**Example (Java):**&#10;```java&#10;public class Memento {&#10;    private String state;&#10;    public Memento(String state) { this.state = state; }&#10;    public String getState() { return state; }&#10;}&#10;&#10;public class Originator {&#10;    private String state;&#10;    public void setState(String state) { this.state = state; }&#10;    public Memento save() { return new Memento(state); }&#10;    public void restore(Memento memento) { state = memento.getState(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For undo/redo functionality.&#10;&#10;**When not to use:**  &#10;&#10;- When state is simple or not needed.&#10;&#10;---&#10;&#10;### Observer&#10;&#10;One-to-many dependency so dependents are notified of changes.&#10;&#10;**Example (Java):**&#10;```java&#10;import java.util.ArrayList;&#10;import java.util.List;&#10;&#10;public interface Observer {&#10;    void update();&#10;}&#10;&#10;public class Subject {&#10;    private List&lt;Observer&gt; observers = new ArrayList&lt;&gt;();&#10;    public void attach(Observer obs) { observers.add(obs); }&#10;    public void notifyObservers() {&#10;        for (Observer obs : observers) { obs.update(); }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- For event handling, UI updates.&#10;&#10;**When not to use:**  &#10;&#10;- When only one object needs notification.&#10;&#10;---&#10;&#10;### State&#10;&#10;Alters behavior when internal state changes.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface State {&#10;    void handle();&#10;}&#10;&#10;public class Context {&#10;    private State state;&#10;    public Context(State state) { this.state = state; }&#10;    public void request() { state.handle(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When object behavior depends on state.&#10;&#10;**When not to use:**  &#10;&#10;- When state changes are rare.&#10;&#10;---&#10;&#10;### Strategy&#10;&#10;Encapsulates interchangeable algorithms.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Strategy {&#10;    int execute(int[] data);&#10;}&#10;&#10;public class SortStrategy implements Strategy {&#10;    public int execute(int[] data) {&#10;        java.util.Arrays.sort(data);&#10;        return data[0]; // Just for demonstration&#10;    }&#10;}&#10;&#10;public class Context {&#10;    private Strategy strategy;&#10;    public Context(Strategy strategy) { this.strategy = strategy; }&#10;    public int doTask(int[] data) { return strategy.execute(data); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When multiple algorithms are needed.&#10;&#10;**When not to use:**  &#10;&#10;- When only one algorithm is used.&#10;&#10;---&#10;&#10;### Template Method&#10;&#10;Defines the skeleton of an algorithm, deferring steps to subclasses.&#10;&#10;**Example (Java):**&#10;```java&#10;public abstract class AbstractClass {&#10;    public final void templateMethod() {&#10;        step1();&#10;        step2();&#10;    }&#10;    protected abstract void step1();&#10;    protected abstract void step2();&#10;}&#10;&#10;public class ConcreteClass extends AbstractClass {&#10;    protected void step1() { System.out.println(&quot;Step 1&quot;); }&#10;    protected void step2() { System.out.println(&quot;Step 2&quot;); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When algorithms have invariant steps.&#10;&#10;**When not to use:**  &#10;&#10;- When steps never change.&#10;&#10;---&#10;&#10;### Visitor&#10;&#10;Represents an operation to be performed on elements of an object structure.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Visitor {&#10;    void visit(Element element);&#10;}&#10;&#10;public interface Element {&#10;    void accept(Visitor visitor);&#10;}&#10;&#10;public class ConcreteElement implements Element {&#10;    public void accept(Visitor visitor) { visitor.visit(this); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;&#10;- When you need to perform operations on object structures.&#10;&#10;**When not to use:**  &#10;&#10;- When object structure rarely changes.&#10;&#10;---&#10;&#10;## Summary&#10;&#10;The 23 core design patterns (Gang of Four) are essential tools for software engineers.  &#10;They provide proven solutions to common problems in software design, improve code maintainability, and are frequently discussed in interviews.  &#10;Understanding these patterns helps you write flexible, scalable, and robust code." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/content/posts/system_design/jee_design_patterns.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/system_design/jee_design_patterns.md" />
              <option name="originalContent" value="+++&#10;date = '2024-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'JEE Design Patterns'&#10;tags = ['JEE Design Patterns', 'Interview']&#10;+++&#10;&#10;About JEE Design Patterns.&#10;&#10;## Table of Contents&#10;" />
              <option name="updatedContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'JEE Design Patterns'&#10;tags = ['JEE Design Patterns', 'Interview']&#10;+++&#10;&#10;JEE design patterns are specialized solutions for enterprise Java applications. They address scalability, maintainability, and performance challenges in distributed systems, helping developers build robust, modular, and efficient enterprise-grade software.&#10;&#10;## Table of Contents&#10;&#10;- [Presentation Tier Patterns](#presentation-tier-patterns)&#10;  - [Intercepting Filter](#intercepting-filter)&#10;  - [Front Controller](#front-controller)&#10;  - [View Helper](#view-helper)&#10;  - [Composite View](#composite-view)&#10;- [Business Tier Patterns](#business-tier-patterns)&#10;  - [Business Delegate](#business-delegate)&#10;  - [Session Facade](#session-facade)&#10;  - [Application Service](#application-service)&#10;  - [Service Locator](#service-locator)&#10;  - [Transfer Object](#transfer-object)&#10;- [Integration Tier Patterns](#integration-tier-patterns)&#10;  - [Data Access Object (DAO)](#data-access-object-dao)&#10;  - [Service Activator](#service-activator)&#10;  - [Web Service Broker](#web-service-broker)&#10;- [Summary](#summary)&#10;&#10;---&#10;&#10;## Presentation Tier Patterns&#10;&#10;### Intercepting Filter&#10;&#10;Provides centralized request pre-processing and post-processing.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface Filter {&#10;    void execute(String request);&#10;}&#10;&#10;public class AuthenticationFilter implements Filter {&#10;    public void execute(String request) {&#10;        System.out.println(&quot;Authenticating request: &quot; + request);&#10;    }&#10;}&#10;&#10;public class FilterChain {&#10;    private List&lt;Filter&gt; filters = new ArrayList&lt;&gt;();&#10;    public void addFilter(Filter filter) { filters.add(filter); }&#10;    public void execute(String request) {&#10;        for (Filter filter : filters) { filter.execute(request); }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When you need reusable request processing logic (logging, authentication).&#10;&#10;**When not to use:**  &#10;- When processing logic is simple or not reusable.&#10;&#10;---&#10;&#10;### Front Controller&#10;&#10;Centralizes request handling to improve control and flexibility.&#10;&#10;**Example (Java):**&#10;```java&#10;public class FrontController {&#10;    public void dispatchRequest(String request) {&#10;        if (&quot;HOME&quot;.equals(request)) {&#10;            System.out.println(&quot;Displaying Home Page&quot;);&#10;        } else {&#10;            System.out.println(&quot;404 Not Found&quot;);&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When you want a single entry point for requests.&#10;&#10;**When not to use:**  &#10;- For very simple applications.&#10;&#10;---&#10;&#10;### View Helper&#10;&#10;Separates business logic from view rendering.&#10;&#10;**Example (Java):**&#10;```java&#10;public class ViewHelper {&#10;    public String formatDate(Date date) {&#10;        return new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).format(date);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When you want to keep views clean and reusable.&#10;&#10;**When not to use:**  &#10;- When formatting logic is trivial.&#10;&#10;---&#10;&#10;### Composite View&#10;&#10;Creates views from modular, reusable subviews.&#10;&#10;**Example (Java):**&#10;```java&#10;public interface View {&#10;    void render();&#10;}&#10;&#10;public class HeaderView implements View {&#10;    public void render() { System.out.println(&quot;Header&quot;); }&#10;}&#10;&#10;public class CompositeView implements View {&#10;    private List&lt;View&gt; views = new ArrayList&lt;&gt;();&#10;    public void addView(View view) { views.add(view); }&#10;    public void render() {&#10;        for (View view : views) { view.render(); }&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- For complex UIs with reusable components.&#10;&#10;**When not to use:**  &#10;- For simple, static views.&#10;&#10;---&#10;&#10;## Business Tier Patterns&#10;&#10;### Business Delegate&#10;&#10;Decouples presentation and business logic.&#10;&#10;**Example (Java):**&#10;```java&#10;public class BusinessService {&#10;    public void doTask() { System.out.println(&quot;Business logic executed&quot;); }&#10;}&#10;&#10;public class BusinessDelegate {&#10;    private BusinessService service = new BusinessService();&#10;    public void executeTask() { service.doTask(); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When you want to hide business logic complexity from the presentation tier.&#10;&#10;**When not to use:**  &#10;- When business logic is simple.&#10;&#10;---&#10;&#10;### Session Facade&#10;&#10;Provides a unified interface to a set of business services.&#10;&#10;**Example (Java):**&#10;```java&#10;public class OrderService { public void placeOrder() {} }&#10;public class PaymentService { public void processPayment() {} }&#10;&#10;public class SessionFacade {&#10;    private OrderService orderService = new OrderService();&#10;    private PaymentService paymentService = new PaymentService();&#10;    public void completeOrder() {&#10;        orderService.placeOrder();&#10;        paymentService.processPayment();&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When you want to reduce network calls and simplify client interaction.&#10;&#10;**When not to use:**  &#10;- When only one service is involved.&#10;&#10;---&#10;&#10;### Application Service&#10;&#10;Coordinates business logic across multiple operations.&#10;&#10;**Example (Java):**&#10;```java&#10;public class ApplicationService {&#10;    public void performBusinessOperation() {&#10;        // Business logic spanning multiple domain objects&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- For complex business workflows.&#10;&#10;**When not to use:**  &#10;- For simple, single-step operations.&#10;&#10;---&#10;&#10;### Service Locator&#10;&#10;Centralizes service lookup and management.&#10;&#10;**Example (Java):**&#10;```java&#10;public class ServiceLocator {&#10;    private static Map&lt;String, Object&gt; services = new HashMap&lt;&gt;();&#10;    public static Object getService(String name) { return services.get(name); }&#10;    public static void registerService(String name, Object service) { services.put(name, service); }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When you need to decouple service consumers from service creation.&#10;&#10;**When not to use:**  &#10;- When dependency injection is preferred.&#10;&#10;---&#10;&#10;### Transfer Object&#10;&#10;Encapsulates data for transfer between layers.&#10;&#10;**Example (Java):**&#10;```java&#10;public class CustomerDTO {&#10;    private String name;&#10;    private String email;&#10;    // getters and setters&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When transferring multiple data fields between layers.&#10;&#10;**When not to use:**  &#10;- For simple, single-field transfers.&#10;&#10;---&#10;&#10;## Integration Tier Patterns&#10;&#10;### Data Access Object (DAO)&#10;&#10;Abstracts and encapsulates all access to the data source.&#10;&#10;**Example (Java):**&#10;```java&#10;public class CustomerDAO {&#10;    public CustomerDTO findCustomerById(int id) {&#10;        // DB lookup logic&#10;        return new CustomerDTO();&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- When you want to separate persistence logic from business logic.&#10;&#10;**When not to use:**  &#10;- For trivial data access.&#10;&#10;---&#10;&#10;### Service Activator&#10;&#10;Enables asynchronous invocation of business services.&#10;&#10;**Example (Java):**&#10;```java&#10;public class ServiceActivator {&#10;    public void activateService(Runnable serviceTask) {&#10;        new Thread(serviceTask).start();&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- For asynchronous processing.&#10;&#10;**When not to use:**  &#10;- For synchronous, simple calls.&#10;&#10;---&#10;&#10;### Web Service Broker&#10;&#10;Centralizes and manages web service interactions.&#10;&#10;**Example (Java):**&#10;```java&#10;public class WebServiceBroker {&#10;    public void callService(String endpoint) {&#10;        System.out.println(&quot;Calling web service at &quot; + endpoint);&#10;    }&#10;}&#10;```&#10;&#10;**When to use:**  &#10;- For integrating multiple web services.&#10;&#10;**When not to use:**  &#10;- For direct, simple web service calls.&#10;&#10;---&#10;&#10;## Summary&#10;&#10;JEE design patterns help solve common problems in enterprise Java applications.  &#10;They improve scalability, maintainability, and modularity, and are essential knowledge for interviews and building robust distributed systems." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/themes/gravionis/layouts/_partials/header.html">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/themes/gravionis/layouts/_partials/header.html" />
              <option name="originalContent" value="&lt;div style=&quot;position: fixed; top: 0; left: 0; width: 100vw; height: 64px; background: #fff; box-shadow: 0 2px 4px rgba(0,0,0,0.03); z-index: 201; display: flex; flex-direction: row; align-items: center; padding: 0 2rem;&quot;&gt;&#10;  &lt;h1 style=&quot;margin: 0 2rem 0 0; font-size: 1.5rem;&quot;&gt;{{ site.Title }}&lt;/h1&gt;&#10;  &lt;nav style=&quot;height: 100%; display: flex; align-items: center; margin-left: auto; margin-right: 8vw;&quot;&gt;&#10;    {{ partial &quot;menu.html&quot; (dict &quot;menuID&quot; &quot;main&quot; &quot;page&quot; .) }}&#10;  &lt;/nav&gt;&#10;&lt;/div&gt;&#10;&lt;!-- Add extra top margin to main content to shift blogs below the header --&gt;&#10;&lt;style&gt;&#10;  body {&#10;    margin-top: 64px; /* Header height */&#10;    margin-left: 5vw; /* Move content further to the right */&#10;    margin-right: 0;&#10;    width: 100%;&#10;    font-size: 0.95rem;&#10;    background: #f0f0f0;&#10;  }&#10;  @media (max-width: 600px) {&#10;    body {&#10;      margin-top: 56px;&#10;      margin-left: 1rem; /* Slightly increased margin on mobile */&#10;      margin-right: 0;&#10;      width: 100%;&#10;      font-size: 1rem;&#10;      background: #f0f0f0;&#10;    }&#10;    .top-menu {&#10;      display: none;&#10;    }&#10;  }&#10;  /* Glow effect for menu items on hover */&#10;  .menu-glow:hover {&#10;    box-shadow: 0 0 8px 2px #0078ff, 0 0 2px 1px #fff;&#10;    background: #f0f8ff;&#10;    color: #0078ff !important;&#10;    transition: box-shadow 0.2s, background 0.2s, color 0.2s;&#10;  }&#10;&lt;/style&gt;&#10;" />
              <option name="updatedContent" value="&lt;div id=&quot;main-header&quot; style=&quot;position: fixed; top: 0; left: 0; width: 100vw; height: 64px; background: #fff; box-shadow: 0 2px 4px rgba(0,0,0,0.03); z-index: 201; display: flex; flex-direction: row; align-items: center; padding: 0 2rem;&quot;&gt;&#10;  &lt;h1 style=&quot;margin: 0 2rem 0 0; font-size: 1.5rem;&quot;&gt;{{ site.Title }}&lt;/h1&gt;&#10;  &lt;nav style=&quot;height: 100%; display: flex; align-items: center; margin-left: auto; margin-right: 8vw;&quot;&gt;&#10;    {{ partial &quot;menu.html&quot; (dict &quot;menuID&quot; &quot;main&quot; &quot;page&quot; .) }}&#10;  &lt;/nav&gt;&#10;&lt;/div&gt;&#10;&lt;!-- Add extra top margin to main content to shift blogs below the header --&gt;&#10;&lt;style&gt;&#10;  body {&#10;    margin-top: 64px; /* Header height */&#10;    margin-left: 8vw; /* Move content further to the right */&#10;    margin-right: 0;&#10;    width: 100%;&#10;    font-size: 0.95rem;&#10;    background: #f0f0f0;&#10;  }&#10;  #main-header {&#10;    box-shadow: 0 2px 8px 2px #0078ff, 0 0 8px 2px #fff;&#10;    transition: box-shadow 0.3s;&#10;  }&#10;  #main-header:hover {&#10;    box-shadow: 0 0 24px 6px #0078ff, 0 0 12px 4px #fff;&#10;    transition: box-shadow 0.3s;&#10;  }&#10;  @media (max-width: 600px) {&#10;    body {&#10;      margin-top: 56px;&#10;      margin-left: 1rem; /* Slightly increased margin on mobile */&#10;      margin-right: 0;&#10;      width: 100%;&#10;      font-size: 1rem;&#10;      background: #f0f0f0;&#10;    }&#10;    .top-menu {&#10;      display: none;&#10;    }&#10;  }&#10;  /* Glow effect for menu items on hover */&#10;  .menu-glow:hover {&#10;    box-shadow: 0 0 8px 2px #0078ff, 0 0 2px 1px #fff;&#10;    background: #f0f8ff;&#10;    color: #0078ff !important;&#10;    transition: box-shadow 0.2s, background 0.2s, color 0.2s;&#10;  }&#10;&lt;/style&gt;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>