<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/content/posts/ai/1_nlp_basics.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/ai/1_nlp_basics.md" />
              <option name="originalContent" value="---&#10;title: NLP, LLM &amp; Generative AI  &#10;date: 2024-08-11&#10;tags: [&quot;Chatbots&quot;, &quot;RASA&quot;, &quot;ChatGPT&quot;, &quot;BERT&quot;, &quot;Transformers&quot;, &quot;Prompt Engineering&quot;]&#10;image : &quot;/img/posts/generative-ai-intro.jpeg&quot;&#10;Description  : &quot;Generative AI with NLP LLM: &#10;&quot;&#10;---&#10;&#10;# 1. Introduction&#10;---&#10;## NLP Tasks&#10;- **Language Modelling** : Predict the next word based on the sequence of words that already occurs in a given language. Application: speech recognition, OCR Translation etc.&#10;- **Text classification** : Assigning a text into one of the known categories based on content. Application: Email spam, sentiment analysis etc&#10;- **Information Extraction** : Extracting important information from a text. Application: extracting user's intent from input text, calendar etc.&#10;- **Information Retrieval** : Finding data based on user query. Application: Used in search engine.&#10;- **Conversational Agent** : A Dialogue system that can converse in a human language. Application: Siri, Alexa etc.&#10;- **Text Summarization** : Short summary of longer documents by retaining the important information. Application: Summary report generation from social media information.&#10;- **Question Answering** : Automatically answer questions posted in Natural Language. Application: Answering a user query based on data from a database.&#10;- **Machine Translation** : Converting a piece of text from one to another language. Application: Google transalator&#10;- **Topic Modelling** : Uncover the topical structure of large collection of text. Application: Text Mining&#10;&#10;## Understanding Human language and its building blocks&#10;- **Language**: words used in a Structured and conventional way and used to convey an idea by speech, writing or gesture.&#10;- **Linguistics**: Scientific study of a language and its structure. Study of language grammer, syntax and phonitics.&#10;  - Building Blocks:&#10;    - Phonemes: smallest unit of speech &amp; sound. English language has 44 of them. Applications: Speech to text transcriptions and text to speech conversations.&#10;    - morphemes and lexemes: Applications: Tokenization, Stemming, lemmatization, word embedding, parts of speech tagging.&#10;       - morphemes: smallest unit of a word. not all morphemes are words but the prefixes and suffixes are. e.g. 'multi' in multistory.&#10;       - lexemes: basic building block of a language. dictionary entries are lexems. lexemes are built on basic form e.g. walk, walking, walked.&#10; - **Syntax**: arragnement of words in a sentence. Representation of sentence is done using parse tree. Entity Extraction and relation extraction.&#10;    - syntax - phrases and sentences&#10;    - context - meaning&#10;    - Syntax Parse Tree:&#10;      ![](/blogs/img/posts/syntax-parse-tree.png)&#10;      - NP - noun phrase&#10;      - VP - verb phrase&#10;      - PP - prepositional phrase&#10;      - S - sentence at the highest level.&#10;- **Context**: words and sentences that surround any part of discourse and that helps determine the meaning. Application: Sarcasm detection, summarization, topic modelling. Made up of:&#10;  - semantics: direct meaning &#10;  - pragmatics: adds world knowledge and external knowledge.&#10;&#10;## Challenges of NLP&#10;- Ambiguity: two or more meanings of a single passage. e.g. we saw her duck. Common knowledge assumptions. e.g he says Sun rises in the west (assumption that a preson knows sun rises in the east)&#10;- Creativity&#10;&#10;---&#10;# 2. Pipeline of NLP&#10;---&#10;## NLP Pipeline&#10;Step by step processing of text is known as NLP Pipeline:&#10;- Data collection (scrapy)&#10;- Text Cleaning &#10;- Pre-processing (stemming and lemmetization)&#10;- Feature engineering (one hot encoding, bag of words technique)&#10;- Modeling&#10;- Evaluation&#10;- Deployment&#10;- Monitoring&#10;---&#10;&#10;---&#10;## NLTK library&#10;NLTK library is most commonly used NLP library. Common text pre-processing steps in NLP: &#10;  - Tokenization: breaking up text into smaller pieces called tokens. &#10;  - Stemming&#10;  - Lemmatization&#10;  - Word Embedding&#10;  - Parts of speech tagging&#10;  - Stop Word removal&#10;  - Word Sence disambiguation&#10;  - Named Entity Recognition (NER)&#10;    &#10;### Tokenization: breaking up text into smaller pieces called tokens. &#10;- 3 types of tokenizers in NLTK&#10;  - word_tokenize()&#10;  - wordpunct_tokenize()&#10;  - sent_tokenize()&#10;- when a tokenization is performed, we get individual tokens. sometimes it is necessary to group multiple tokens into 1.&#10;  - Unigrams: &quot;Steve&quot; &quot;went&quot; &quot;to&quot; &quot;school&quot;&#10;  - Bigrams: tokens of two consequtive words in a sentence; &quot;Steve went&quot; &quot;went to&quot; &quot;to school&quot;&#10;  - Trigrams: tokens of 3; &quot;Steve went to&quot; &quot;went to school&quot;&#10;  - Ngrams: tokens of n&#10;&#10;Setting the stage for tokenization:&#10;```python&#10;import nltk&#10;nltk.download('punkt')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens = nltk.word_tokenize(text)&#10;list(nltk.bigrams(ml_tokens)) # or trigrams&#10;```&#10;### Parts of speech tagging &amp; Stop words removal&#10;- Parts of speech tagging: process of marking words as corresponding to parts of speech, based on both definition and context.&#10;  - e.g. I like(Verb) to read(Verb) books&#10;  - this is helpful in understanding the context in which a word is used.&#10;- stopwords&#10;  - e.g. a, the, is, are&#10;  - not adding any important information, which can be elimiated.&#10;&#10;```python&#10;ml_tokens=nltk.word_tokenize(&quot;Jerry eats a banana&quot;)&#10;nltk.download(&quot;averaged_perceptron_tagger&quot;) # needs to be downloaded for tagging.&#10;for token in ml_tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;This outputs&#10;```python&#10;[('Jerry', 'NN')]&#10;[('eats', 'NNS')]&#10;[('a', 'DT')]&#10;[('banana', 'NN')]&#10;```&#10;pos_tag is a very basic version of the library, see how Jerry and eats is NNS - its tagged as a single term and categorized it as a Noun. In real life we use pos_tag from spacial library or transformers.&#10;&#10;### Regular expression tokenizer&#10;```python&#10;from nltk.tokenize import RegexpTokenizer&#10;sent = &quot;Jerry eats a banana&quot;&#10;reg_tokenizer = RegexpTokenizer('(?u)\W+|\$[\d\.]+|\S+')&#10;tokens = reg_tokenizer.tokenize(sent)&#10;for token in tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;&#10;```python&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens=nltk.word_tokenize(text)&#10;filtered_data = [w for w in ml_tokens if not w in stop_words]&#10;filtered_data&#10;```&#10;&#10;### Stemming, Lemmatization&#10;#### Stemming&#10;Reducing a word or part of a word to its stem or root form. It lowers the inflection (process we do inorder to modify the word in order to communicate mini-gramatical categories like tensors, voices, aspect, gender, mood etc. added to communicate to other person) of words into their root form. This is a pre-processing activity.&#10;Using the same word in different inflected forms in a text can lead to redundancy in natural language processing tasks. By reducing inflection, we decrease the number of unique words that machine learning models need to process.&#10;&#10;**Example 1**&#10;* Without Inflection: Original sentence: &quot;She runs every day, and they are running in the park while he ran yesterday.&quot;&#10;Inflected forms: runs, running, ran&#10;* With Reduced Inflection:Simplified sentence: &quot;She run every day, and they run in the park while he run yesterday.&quot; In this simplified version, we use &quot;run&quot; for all forms.&#10;* Impact: Original sentence has three different inflected forms, which can create redundancy for a natural language processing model.&#10;Simplified sentence reduces the variety of words, making it easier for the model to analyze the core action (running) without getting bogged down by different forms.&#10;&#10;**Example 2**&#10;&#10;* after stemming Generate → Generat also Generation → Generat&#10;* Stemming can create non-dictionary forms (like &quot;generat&quot;). It's important to note that in stemming, the goal is to reduce words to their root form, which might not always be a valid dictionary word. The main purpose of stemming is to reduce data redundancy by grouping related words together. The primary aim is to reduce the variety of word forms to improve processing efficiency and analysis.&#10;&#10;**Uses:**&#10;- SEO&#10;- Text mining&#10;- Web-search&#10;- Indexing&#10;- Tagging.&#10;&#10;**4 Types of Stemming Algorithms:**&#10;* Porter Stemmer: Martin Porter invented it and Original Stemmer algorithm. Ease of use and rapid. &#10;* Snowball Stemmer: Also invented by same guy. more presise than porter stemmer.&#10;* Lancaster Stemmer: Sometimes does over stemming, sometimes non linguistic or meaningless. &#10;* Regex Stemmer: morphological affixes.&#10;&#10;```python&#10;from nltk.stem import PorterStemmer&#10;porter = PorterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {porter.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gener&#10;# generation -&gt; gener&#10;# genorously -&gt; genor&#10;# generate -&gt; gener&#10;from nltk.stem import SnowballStemmer&#10;snowball = SnowballStemmer(language='english')&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {snowball.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generat&#10;# genorously -&gt; genor&#10;# generate -&gt; generat&#10;from nltk.stem import LancasterStemmer&#10;lancaster = LancasterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lancaster.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gen&#10;# generation -&gt; gen&#10;# genorously -&gt; gen&#10;# generate -&gt; gen&#10;from nltk.stem import RegexpStemmer&#10;regex = RegexpStemmer('ing|s$|able$',min=4)&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {regex.stem(word)}&quot;)&#10;#Output&#10;# generous -&gt; generou&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;#### Lemmatization&#10;Converting the words into root word using Parts of Speech (POS) tag as well as context as a base. Similar to stemming but brings context to the words and the result is a word in the dictionary. &#10;* Applications e.g. search engine and compacting&#10;**Example:**&#10;* eats → eat&#10;* ate → eat&#10;* ate → eat&#10;* eating → eat&#10;&#10;```python&#10;import nltk&#10;from nltk.stem import WordNetLemmatizer&#10;nltk.download('wordnet')&#10;lemma = WordNetLemmatizer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lemma.lemmatize(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;&#10;# Named Entity Recognition&#10;* First step in the information extraction&#10;* NER seeks to locate and classify named entities into pre-defined categories such as names of person, Organization, location etc. e.g. Modi, America, Apple Inc, Tesla&#10;&#10;## Challenges:&#10;- Word sense disambiguiation: method by which meaning of the word is determined from the context it is used.&#10;- Example: bark, cinnamon bark or sound made by dog is bark.&#10;- when the two sentences passed to the algorithm word sense disambiguiation comes into picture, it removes the ambiguity. &#10;&#10;## Application:&#10;* Text mining&#10;* Information extraction&#10;* used alongside with Lexicography&#10;* Information retrieval process&#10;&#10;## Word Sence disambiguation:&#10;**Lesk Algorithm:** based on the idea that words in each region will have a similar meaning.&#10;```python&#10;from nltk.wsd import lesk&#10;from nltk.tokenize import word_tokenize&#10;nltk.download('punkt')&#10;a1=lesk(word_tokenize('The building has a device to jam the signal'), 'jam')&#10;print(a1, a1.definition())&#10;a2=lesk(word_tokenize('I am stuck in a traffic jam'), 'jam')&#10;print(a2, a2.definition())&#10;a3=lesk(word_tokenize('I like to eat jam with bread'), 'jam')&#10;print(a3, a3.definition())&#10;#Output&#10;# Synset('jamming.n.01') deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems&#10;# Synset('jam.v.05') get stuck and immobilized&#10;# Synset('jam.v.06') crowd or pack to capacity --&gt; Somehow this isn't coming correct&#10;```&#10;&#10;## Named Entity Recognition&#10;```python&#10;nltk.download('averaged_perceptron_tagger')&#10;nltk.download('maxent_ne_chunker')&#10;nltk.download('words')&#10;text=&quot;Apple is an American company based out of California&quot;&#10;for w in nltk.word_tokenize(text):&#10;  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))):&#10;    if hasattr(chunk, 'label'):&#10;      print(chunk.label(), ' '.join(c[0] for c in chunk))&#10;# Output - GPE stands for Geo political entity&#10;# GPE Apple&#10;# GPE American&#10;# GPE California&#10;``` &#10;# spaCy Library&#10;spaCy is a free open source library for advaned Natural Language Processing in python for production use. NLTK was for research purpose. spaCy is for production use. Can handle and process large volume of text.&#10;## Features&#10;- Tokenization &#10;- Parts of Speech Tagging - word types of tokens, like verb or noun.&#10;- Dependency Parsing&#10;- Lemmatization&#10;- Sentence Boundary Detection (SBD) - finding and segmenting individual sentences.&#10;- Named Entity Recognition&#10;- Entity Linking (EL)- Disambiguating texual entities to unique identifiers ina knowledge base.&#10;- Similarity - comparing words, text apans and documents and how similar they are to each other.&#10;- Text Classification - assigning caterfores or labels to a whole document or parts of it.&#10;- Rule based Matching - finding sequence of token based on their texts and linguistic annotations, similar to regular expressions.&#10;- Training - updating and improving a statstical models predictions&#10;- Serialization - Saving objects to files or byte string.&#10;```python&#10;import spacy&#10;nlp = spacy.load(&quot;en_core_web_sm&quot;) # verson of spacy library - english small model&#10;doc = nlp(&quot;Apple is looking at buying U.K startup for $1 billion&quot;) # by default the spacy applies tagger, parser, ner&#10;&#10;for token in doc:&#10;  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)&#10;&#10;# Output&#10;# Apple Apple PROPN NNP nsubj Xxxxx True False&#10;# is be AUX VBZ aux xx True True&#10;# looking look VERB VBG ROOT xxxx True False&#10;# at at ADP IN prep xx True True&#10;# buying buy VERB VBG pcomp xxxx True False&#10;# U.K U.K PROPN NNP dobj X.X False False&#10;# startup startup VERB VB dep xxxx True False&#10;# for for ADP IN prep xxx True True&#10;# $ $ SYM $ quantmod $ False False&#10;# 1 1 NUM CD compound d False False&#10;# billion billion NUM CD pobj xxxx True False&#10;```&#10;* in the above by default the spacy applies tagger, parser, ner. The steps however can be added or replaced.&#10;![](https://spacy.io/images/pipeline.svg)&#10;&#10;* first step is tokenization&#10;![](https://spacy.io/images/tokenization.svg)&#10;&#10;```python&#10;text=&quot;Mission impossible is one of the best movies I have watched. I love it.&quot;&#10;print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(&quot;text&quot;, &quot;lemmatization&quot;, &quot;partofspeech&quot;, &quot;TAG&quot;, &quot;DEP&quot;, &quot;SHAPE&quot;, &quot;ALPHA&quot;, &quot;STOP&quot;))&#10;doc = nlp(text)&#10;for token in doc:&#10;  print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop))&#10;&#10;# text      |lemmatization  |partofspeech   |TAG       |DEP       |SHAPE     |ALPHA     |STOP      &#10;# Mission   |mission        |NOUN           |NN        |nsubj     |Xxxxx     |         1|         0&#10;# impossible|impossible     |ADJ            |JJ        |amod      |xxxx      |         1|         0&#10;# is        |be             |AUX            |VBZ       |ROOT      |xx        |         1|         1&#10;# one       |one            |NUM            |CD        |attr      |xxx       |         1|         1&#10;# of        |of             |ADP            |IN        |prep      |xx        |         1|         1&#10;# the       |the            |DET            |DT        |det       |xxx       |         1|         1&#10;# best      |good           |ADJ            |JJS       |amod      |xxxx      |         1|         0&#10;# movies    |movie          |NOUN           |NNS       |pobj      |xxxx      |         1|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# have      |have           |AUX            |VBP       |aux       |xxxx      |         1|         1&#10;# watched   |watch          |VERB           |VBN       |relcl     |xxxx      |         1|         0&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# love      |love           |VERB           |VBP       |ROOT      |xxxx      |         1|         0&#10;# it        |it             |PRON           |PRP       |dobj      |xx        |         1|         1&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I you do not understand sonething&#10;print(spacy.explain('nsubj')) #nominal subject&#10;print(spacy.explain('pobj')) #object of preposition&#10;# print entities&#10;```&#10;* Extracting the Named Entities&#10;```python&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia&quot;&#10;doc = nlp(text)&#10;for token in doc.ents:&#10;  print(token)&#10;# Output&#10;# Narendra Modi&#10;# India&#10;# Asia&#10;```&#10;&#10;* If you want to see a colourful version of the named entities then,&#10;```python&#10;from spacy import displacy&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia which embraces Machine Learning&quot;&#10;doc=nlp(text)&#10;displacy.render(docs=doc, style=&quot;ent&quot;,jupyter=True)&#10;spacy.explain('GPE') #Geo Political Entity&#10;```&#10;&#10;# NLP Text Vectorization&#10;Convertion of raw text into numerical form is called Text Vectorization. Machine learning expects text in numerical form. This is also called Feature Extraction.&#10;Many ways of achieving feature extraction:&#10;1. One Hot Encoding&#10;2. Count Vectorizer&#10;3. TF-IDF&#10;4. Word Embeddings&#10;&#10;## One Hot Encoding&#10;Every word including symbols are written in the vector form. This vector will only have 0 &amp; 1s. each word is written or encoded as a one hot vector, each word will have different vector representation. example:&#10;&#10;| Color  | Red | Blue | Green |&#10;|--------|-----|------|-------|&#10;| Red    |  1  |  0   |   0   |&#10;| Blue   |  0  |  1   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;| Red    |  1  |  0   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;&#10;```python&#10;corpus = ['dog eats meat','man eats meat']&#10;from sklearn.preprocessing import OneHotEncoder&#10;one_hot = OneHotEncoder()&#10;all_in_one = [indi.split() for indi in corpus]&#10;one_hot.fit_transform(all_in_one).toarray()&#10;#Output&#10;# [['dog', 'eats', 'meat'], ['man', 'eats', 'meat']]&#10;# array([[1., 0., 1., 1.],&#10;#        [0., 1., 1., 1.]])&#10;```&#10;&#10;we generally dont use the scikitlearn onehotencoding directly as it's mainly for structured data not for unstructured data.&#10;&#10;### Disadvantages&#10;* Size of the one hot encoding is propotional to the size of the vocabulary.&#10;* Sparse representation of data&#10;* Insufficent in storing, computing and learning from data.&#10;* No sequence of words is considered and is ignored.&#10;* If words outside the vocabulary exists there is no way to deal with it.&#10;* Word context is not considered in the representation.&#10;&#10;## Bag of Words technique (BoW)&#10;NLP pipeline has multiple steps as mentioned above. This step comes in the feature engineering step. Classical text represenation technique. Representation of the text under the consideration of bag of words. Text is characterised by a unique set of words. e.g. movie was bad; movie was excellent. This is characterised by the unique set of words not based on where it occurs in the sentence. so if the word bad it will be in one bag and excellent it will be in a different bag.&#10;&#10;**Application:** Sentiment analysis (positive and negative sentiments). Harry potter was good, a movie was good - they are classified into the same bag.&#10;&#10;### Write your own Bow Representation&#10;```python&#10;# if you are adventrous and dont want to use the Count Vectorizer.&#10;import pandas as pd&#10;import re&#10;t1 = &quot;dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower()).split()]&#10;&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;&#10;def bow(all, sentences):&#10;  results = []&#10;  for sentence in sentences:&#10;    result = {word: 0 for word in all}&#10;    for word in sentence:&#10;      result[word] = 1&#10;    results.append(result)&#10;  print(pd.DataFrame(results))&#10;&#10;bow(all_words, sentences)&#10;# Output&#10;# dog  eats  meat  everyday  man  once  in  a  while  rarely&#10;# 0    1     1     1         1    0     0   0  0      0       0&#10;# 1    0     1     1         0    1     1   1  1      1       0&#10;# 2    1     1     0         0    1     0   0  0      0       1&#10;```&#10;### Disadvantages:&#10;* Size of the vector increases with the size of the vocabulary&#10;* Sparsity (property of being scattered) is still an issue.&#10;* Does not capture the similarity between words (not context aware). 'I eat', 'I ate', 'I ran' Bag of Words Vectors for all the three documents will be equally apart - in layman terms - 'eat and ran' and 'eat and ate' will be same distance apart.&#10;&#10;&#10;```python&#10;# use the countvectorize or just write your own python code after finding the unique words&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;man eats meat once in a while.&quot;&#10;t3 = &quot;man eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;# vectorizer = CountVectorizer(binary=True) --&gt; use this for sentiment analysis  &#10;vectorizer = CountVectorizer()  &#10;X = vectorizer.fit_transform([t1, t2, t3])&#10;print(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;pd.DataFrame(bag_of_words, columns=feature_names)&#10;# Output&#10;# dog&#9;eats&#9;everyday&#9;in&#9;man&#9;meat&#9;once&#9;rarely&#9;while&#10;# 0&#9;5&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;1&#9;0&#10;```&#10;```Note: ``` vectorizer = CountVectorizer(**binary=True**)``` if you dont want actual counts but just 1s and 0s. This is a technique used specific to sentiment classification&#10;&#10;&#10;Now even if you want it as a unigram, bigram and trigram thats also possible.&#10;```python&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] &#10;unique_words = set(all_words)&#10;vectorizer = CountVectorizer(ngram_range=(1,3)) # See here &lt;--&#10;X = vectorizer.fit_transform(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;print(&quot;Feature Names (Vocabulary):&quot;, feature_names)&#10;print(&quot;Bag of Words Representation:&quot;)&#10;pd.DataFrame(bag_of_words)&#10;#Output&#10;# Feature Names (Vocabulary): ['dog' 'dog dog' 'dog dog dog' 'dog dog eats' 'dog eats' 'dog eats meat'&#10;#  'dog rarely' 'eats' 'eats dog' 'eats dog rarely' 'eats meat'&#10;#  'eats meat everyday' 'eats meat once' 'everyday' 'in' 'in while' 'man'&#10;#  'man eats' 'man eats dog' 'man eats meat' 'meat' 'meat everyday'&#10;#  'meat once' 'meat once in' 'once' 'once in' 'once in while' 'rarely'&#10;#  'while']&#10;# Bag of Words Representation:&#10;# 0&#9;1&#9;2&#9;3&#9;4&#9;5&#9;6&#9;7&#9;8&#9;9&#9;...&#9;19&#9;20&#9;21&#9;22&#9;23&#9;24&#9;25&#9;26&#9;27&#9;28&#10;# 0&#9;5&#9;4&#9;3&#9;1&#9;1&#9;1&#9;0&#9;1&#9;0&#9;0&#9;...&#9;0&#9;1&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#9;0&#9;...&#9;1&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;1&#9;1&#9;1&#9;...&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#10;# 3 rows × 29 columns&#10;```&#10;### Pros and Cons&#10;- has the ability to capture the context and word order information in the form of n-grams&#10;- Documents  having the same ngrams will have vectors closer to each other in euclidean space as compared to documents with different ngrams.&#10;- As n increased the dimensionlity (sparsity) increases&#10;- issue related to out of vocabulary problem exists&#10;&#10;## TF-IDF &#10;- a word most repeated in one document but not in any other documents are considered more  important. Stop words however dont fall into this category. &#10;- Term Frequency (TF) * Inverse Document Frequency (IDF)&#10;- quantify a word in a set of documents.&#10;- importance of words in the given context is represented here.&#10;&#10;**Terminology**&#10;t - term&#10;d - document (set of words)&#10;N - count of corpus&#10;corpus - the total document set.&#10;e.g. 'This Dress is so beautiful' - how is the computer to know that the important words here are dress and beautiful? thats where TF*IDF shines.&#10;&#10;* TF - number of times a particular word appears in a sentence.&#10;e.g. Sun rises in East; frequency of Sun - 1/4&#10;* IDF - Dress is beautiful; is isn't adding any importance. stop words needs to be weightage reduced particularly when these words are used more freqently it's importance will increase. IDF measures the informativeness of term t. it will be low for stop words. inverse document frequency ```formula: idf(t) = log(N/(df+1))```&#10;&#10;```&#10;IDF(word)=log10(total number of documents/ (1+number of documents containing the word))&#10;```&#10;&#10;hence, ```TF-IDF formula: tf-idf(t,d) = tf(t,d) * log(N/(df+1)) ```&#10;where, **N** - total number of documents in the corpus &amp; **df** - number of document with term t.&#10; e.g. lets say sentences: &#10;&#10; ```python&#10; import math&#10;s1='man eats pizza'&#10;s2='dog eats food'&#10;s3='ant eats pizza'&#10;# for man in s1 → tf = 1/3 &#10;# idf = log₂(3/1) &#10;tf = 1/3 &#10;idf = math.log(3/2)&#10;tf_idf = tf *  idf&#10;print(tf_idf) # 0.13515503603605478&#10;# for eats in s1 → tf = 1/3 &#10;tf = 1/3&#10;idf = math.log(3/4)&#10;tf_idf = tf*idf&#10;print(tf_idf) #-0.09589402415059363&#10;# hence eats is not a very important word.&#10;```&#10;## tf-idf hands on&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;freq1 = {}&#10;freq2 = {}&#10;for word in vocabulary:&#10;  freq1[word] = word_dict1[word]/len(first_sent)&#10;  freq2[word] = word_dict2[word]/len(second_sent)&#10;&#10;pd.DataFrame([freq1, freq2])&#10;```&#10;## implement the tf-idf using scikit&#10;it is supposed to be something like this. Below isn't fully working need to check why.&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data machine science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;def calculateTF(doc):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateIDF(docs):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateTFIDF(tfBagOfWords, idfs):&#10;  print(idfs)&#10;  tfIdf = {}&#10;  for word, value in tfBagOfWords.items():&#10;    tfIdf[word] = value*idfs[word]&#10;  return tfIdf&#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;pd.DataFrame([&#10;    calculateTFIDF(calculateTF(word_dict1), calculateIDF([word_dict1, word_dict2])),&#10;    calculateTFIDF(calculateTF(word_dict2), calculateIDF([word_dict1, word_dict2]))&#10;    ])&#10;```&#10;&#10;## implement using sklearn&#10;```python&#10;import sklearn&#10;import pandas as pd&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;vec = TfidfVectorizer()&#10;result = vec.fit_transform([first_sent, second_sent])&#10;pd.DataFrame(result.toarray(), columns=vec.get_feature_names_out())&#10;# Output&#10;# amazing&#9;an&#9;career&#9;current&#9;data&#9;deep&#9;in&#9;is&#9;learning&#9;machine&#9;of&#9;science&#9;subset&#9;the&#9;world&#10;# 0&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.230768&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.324336&#10;# 1&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.243956&#9;0.685743&#9;0.342871&#9;0.342871&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.000000&#10;```&#10;&#10;## Pros and cons of the tf-idf technique&#10;### Advantages&#10;- use this to calculate the similarity between two texts using similarity measures like Cosing similarity/Euclidean distance&#10;- has application in text classification, information retrieval etc.&#10;- better than earlier methods.&#10;### Disadvantages&#10;- high dimentionality&#10;- They are still discrete representation of units of text, hence unable to capture relation between words&#10;- sparse and high dimension&#10;- cannot handle OOV (Out of Vocabulary) words.&#10;---&#10;# TBC&#10;---&#10;https://www.youtube.com/watch?v=tFHeUSJAYbE&amp;list=PLz-ep5RbHosU2hnz5ejezwaYpdMutMVB0&#10;# Large Language Models (LLMs)&#10;is a type of Language Model. Quantatively it is the number of model parameters vary from 10 to 100 billion parameters per model. Qualitatively also called emergent properties starts emerging - properties in large language model that do not appear in small language models, e.g. zero shot learning - capability of a model to complete a task it is not explicitely trained to do.&#10;&#10;In the earlier days the model was trained using supervised learning we use thousands if not millions of examples - but with LLMs we use self-supervised learning. Train a very large model in a very large corpus of data. In self-supervised learning doesn't require manual labelling of each example. The labels or the targets of the model defined from the inherent structure of the data itself.&#10;&#10;One of the popular way of doing this is &quot;next word prediction paradigm&quot;. There is not just one word but many that can go after ***listen to your....***. What the llm would do is to use probablistic distribution of next word given the previous word. in the above example the words could be heart or gut or body or parents etc.. each with different probability distribution. Its essentially trained on a large set of data with so many examples of corpus of data - so it can statistically predict the next set of data. Important thing is the context matters - if for example we add the word ***don't*** in front of ***listen to your...***, the probably distribution will entirely change.&#10;&#10;Autoregression Task formula: ***P(tn | tn-1,..., tn-m)*** P(tn) given n tokens.&#10;&#10;This is how LLMs like chatgpt works.&#10;# 3 levels of Using LLMs&#10;- Level 1: Prompt Engineering&#10;  - using LLM out of the box - not changing any model parameters. Two ways to do this &#10;    - using an agent like chatgpt&#10;    - using open AI API or hugging face tranformers library: help to interact with LLMs programmatically using python for example. Pay per api call in case of open API. Hugging face transformer library is an open source option, you can run the models locally in this case so no need to send your proprietary data into 3rd party or open ai.&#10;- Level 2: Model Fine Tuning&#10;  - adjusting model parameters for a particular tasks.&#10;  - steps&#10;    - Step 1: pre-trained models are obtained. (usually trained by self supervised learning). in this step the base model is learning useful representations for a wide variety of tasks.&#10;    - Step 2: update model parameters given task-specific examples (trained by supervised learning or even reinforcement learning).e.g. chatgpt, the model we use here is a fine tuned model learnt by reinforcement learning. Some techniques is lora or low range adaptation. another technique is reinforcement learning based on human feedback (RLHF).&#10;    - Step 3: Deploy the fine tuned large language model.&#10;- Level 3: Build your own.&#10;  - This is only for 1% of all usecases. &#10;  - One example usecase: in a large company we dont want to use open source models where security is a concern, dont want to send data to 3rd party via an API. &#10;  - Another usecase is you want to create your own model and commercialize it.&#10;  - At a high level steps are:&#10;    - get the data or corpus.&#10;    - pre process and refine it &#10;    - model training&#10;    - pre trained llm.&#10;    - then go to step 2.&#10;&#10;## Connecting to AI using API, Programmatically&#10;### OpenAIs Python API&#10;It's similar to chatGPT but with Python. In both we pass a request and use the language modelling to predict the next word. Apart from the difference in the web interface in chatgpt and here programmatically some differences are as follows. most of the below aren't possible with chatgpt but programmatically possible with openai python.&#10;1) Customizable System message: Message or prompt or a set of instructions that help define the tone, personality and functionality of the model during a conversation. This helps model how to respond to user input and what constraints to follow. I customized the message in chatgpt first to give back sarcastic answers.&#10;![](/blogs/img/posts/chatgpt-customized-system-message.png)&#10;![](/blogs/img/posts/chatgpt-customized-system-message-output.png)&#10;Then i changed the message to give negative and dark response. This time the results were entirely opposite.&#10;![](/blogs/img/posts/chatgpt-customized-system-message-dark.png)&#10;2) Adjust input parameter &#10;  - max response length: response length sent back by model&#10;  - number of responses: (number of outputs you may want to programmatically select one of the response e.g.)&#10;  - temperature: randomness of response generated by the model.&#10;3) Process image and other types&#10;4) extact helful word embeddings for downstream tasks&#10;5) input audio for transcription and tranlations&#10;6) model fine tuning functionality.&#10;7) with chatgpt can only use GPT 3.5 or 4, with openai several other models are available read: https://platform.openai.com/docs/models&#10;### Costing:&#10;Tokens &amp; Pricing:&#10;same as tokenization above a given text is converted and represented as numbers. Pricing is based on the tokens, bigger prompts will incur larger costs. To use we have to get the Secret key to make API calls.&#10;&#10;```python&#10;import openai&#10;from openai import OpenAI&#10;from sk import openai_key # my own file with a variable openai_key='sk-proj-4D1ID8ZeQ...'&#10;&#10;client = OpenAI(api_key=openai_key)&#10;response = client.chat.completions.create(&#10;    model=&quot;gpt-3.5-turbo&quot;,&#10;    max_tokens=2,&#10;    temperature=2, # degree of randomness, 0 is predictable.&#10;    n=3,  &#10;    messages=[&#10;        {&#10;            &quot;role&quot;: &quot;user&quot;, &#10;            &quot;content&quot;: &quot;where there is a will there is a &quot;&#10;        }&#10;    ]&#10;)&#10;&#10;for idx, choice in enumerate(response.choices):&#10;    print(f&quot;Response {idx+1}: {choice['message']['content']}&quot;)&#10;&#10;# Output &#10;# Response 1: way.&#10;# Response 2: plan.&#10;# Response 3: chance.&#10;# note that 2 tokens - 'way' and '.'&#10;```&#10;## Hugging face Transformer library&#10;Major hub for open source Machine learning (ML) like Dockerhub for docker. It has models, dataset (its own data used to train models) and spaces (for building and deploying machine learning applications).&#10;&#10;### Transformers library&#10;Downloading and training machine learning models in python. Like NLP, computer vision, audio processing etc. E.g. for sentiment analysis - find the model that does sentiment analysis classification task then you have to take raw text convert into numerical value that is then passed to the model; finally decode the numerical output of the output to get the label of the text. This can be done easily in the transformers library using a pipeline function. &#10;other things that can be done &#10;- sentiment analysis&#10;- summarization&#10;- translation&#10;- question-answering&#10;- feature extraction &#10;- text generation etc.&#10;&#10;```python&#10;! pip install transformers&#10;from transformers import pipeline&#10;sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;)&#10;# sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;, model=&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;)&#10;texts = [&#10;    &quot;&quot;&quot;One is that the mining giant's shares are pushing higher this morning.&#10;        In early trade, the Big Australian's shares are 1.5% higher to $45.74.&#10;        This means that the BHP share price is now up 13% over the past two weeks.&quot;&quot;&quot;&#10;]&#10;results = sentiment_pipeline(texts)&#10;for text, result in zip(texts, results):&#10;    print(f&quot;Text: {text}\nSentiment: {result['label']}, Score: {result['score']}\n&quot;)&#10;```&#10;**Question**: How does it decide if a text is positive or negative without perception?&#10;### signup and logininto huggingface&#10;- lookup for transformer tag and select a model. Then you will check also for pytorch tag. This is because hugging face also supports models which aren't just compatible with pytorch and transformers but also others.&#10;- The train button on the right will have options like Amazon Sagemaker, NVIDIA NDX Cloud, AutoTrain which will help jump start the model finetuning part.&#10;-  &#10;### Getting started&#10;to get started copy the [hf-env.yml](https://github.com/ShawhinT/YouTube-Blog/blob/26dff2786a7d64620e5e7dd71fcd51a416aad1db/LLMs/hugging-face/hf-env.yml) file into your code repository.&#10;&#10;```bash&#10;conda env create --file hf-env.yml&#10;```&#10;another example for text-classification&#10;```python&#10;from transformers import pipeline&#10;classifier = pipeline(task=&quot;text-classification&quot;, model=&quot;SamLowe/roberta-base-go_emotions&quot;, top_k=None)&#10;sentences = [&quot;I am not having a great day&quot;]&#10;model_outputs = classifier(sentences)&#10;print(model_outputs[0])&#10;# Output&#10;# [{'label': 'disappointment', 'score': 0.4666951894760132}, {'label': 'sadness', 'score': 0.39849498867988586}, {'label': 'annoyance', 'score': 0.06806593388319016}, {'label': 'neutral', 'score': 0.05703023821115494}, {'label': 'disapproval', 'score': 0.044239308685064316}, {'label': 'nervousness', 'score': 0.014850745908915997}, {'label': 'realization', 'score': 0.014059904962778091}, {'label': 'approval', 'score': 0.0112674655392766}, {'label': 'joy', 'score': 0.006303396541625261}, {'label': 'remorse', 'score': 0.006221492309123278}, {'label': 'caring', 'score': 0.006029403302818537}, {'label': 'embarrassment', 'score': 0.0052654859609901905}, {'label': 'anger', 'score': 0.004981426056474447}, {'label': 'disgust', 'score': 0.004259029403328896}, {'label': 'grief', 'score': 0.0040021371096372604}, {'label': 'confusion', 'score': 0.003382918192073703}, {'label': 'relief', 'score': 0.0031405005138367414}, {'label': 'desire', 'score': 0.00282747158780694}, {'label': 'admiration', 'score': 0.002815794898197055}, {'label': 'fear', 'score': 0.002707520266994834}, {'label': 'optimism', 'score': 0.0026164911687374115}, {'label': 'love', 'score': 0.0024883910082280636}, {'label': 'excitement', 'score': 0.0024494787212461233}, {'label': 'curiosity', 'score': 0.0023743617348372936}, {'label': 'amusement', 'score': 0.001746696187183261}, {'label': 'surprise', 'score': 0.0014529851032420993}, {'label': 'gratitude', 'score': 0.0006464761681854725}, {'label': 'pride', 'score': 0.00055424973834306}]&#10;```&#10;yet another example for summarization&#10;```python&#10;from transformers import pipeline&#10;summarizer = pipeline(&quot;summarization&quot;, model=&quot;Falconsai/text_summarization&quot;)&#10;ARTICLE = &quot;&quot;&quot; &#10;Hugging Face: Revolutionizing Natural Language Processing&#10;Introduction&#10;In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.&#10;The Birth of Hugging Face&#10;Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name &quot;Hugging Face&quot; was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.&#10;Transformative Innovations&#10;Hugging Face is best known for its open-source contributions, particularly the &quot;Transformers&quot; library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.&#10;Key Contributions:&#10;1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.&#10;2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.&#10;3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.&#10;Democratizing AI&#10;Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.&#10;By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.&#10;Industry Adoption&#10;The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.&#10;Future Directions&#10;Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.&#10;Conclusion&#10;Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.&#10;&quot;&quot;&quot;&#10;print(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))&#10;&gt;&gt;&gt; [{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name &quot;Hugging Face&quot; was chosen to reflect the company\'s mission of making AI models more accessible and friendly to humans .'}]&#10;```&#10;Other transformers like ```Falconsai/text_summarization``` to use is the ```facebook/bart-large-cnn``` for text summarization.&#10;&#10;finally, you can chain together multiple objects for example first do a text summarization and then do a sentiment analysis. &#10;Another interesting task is conversational text. For this we can use the ```facebook/blenderbot-400M-distill```. There is supposed to be a class called ```Conversation``` (also imported from transformers) which is supposed to be a container for conversation. &#10;```python&#10;from transformers import pipeline&#10;&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;conversation_history = &quot;Hello, how are you?&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;&#10;# Continue the conversation&#10;conversation_history += f&quot; {response[0]['generated_text']}&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;```&#10;There is a library called **Gradio** to make it conversational. Gradio is very similar to streamlit. &#10;```python&#10;import gradio as gr&#10;from transformers import pipeline&#10;&#10;# Load the chatbot model&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;&#10;# Function to handle chatbot conversation&#10;def respond(user_input, history=[]):&#10;    # Add the user input to the conversation history&#10;    history = history or []&#10;    history.append(f&quot;User: {user_input}&quot;)&#10;    print(history)&#10;    # Generate a response&#10;    response = chatbot(user_input)&#10;    bot_reply = response[0]['generated_text']&#10;    print(bot_reply)&#10;&#10;    # Add the bot reply to the history&#10;    history.append(f&quot;Bot: {bot_reply}&quot;)&#10;    &#10;    # Return the entire conversation history as a string&#10;    return &quot;\n&quot;.join(history), history&#10;&#10;# Create the Gradio interface&#10;demo = gr.Interface(&#10;    fn=respond,  # The function that processes input&#10;    inputs=[gr.Textbox(label=&quot;Your Message here:&quot;), gr.State([])],  # Input is a message and conversation history&#10;    outputs=[gr.Textbox(label=&quot;Response here:&quot;), gr.State([])],  # Output is updated conversation and history&#10;    title=&quot;AI Chatbot&quot;&#10;)&#10;&#10;# Launch the interface&#10;demo.launch()&#10;```&#10;![](/blogs/img/posts/gradio-initial.png)&#10;you can post this in hugging face spaces or [hf.co/spaces](hf.co/spaces). They allow to create ML applications and host it here.&#10;example of this [Llama chatbot](https://huggingface.co/spaces/huggingface-projects/llama-3.2-vision-11B).&#10;* Go to hf.co/spaces and click on create new space.&#10;* follow the instructions to clone the repo and push your code.&#10;&#10;# Prompt Engineering&#10;## What is Prompt Engineering&#10;Prompt engineering refers to the process of designing and refining the input (or &quot;prompt&quot;) given to an AI language model, like GPT, to produce desired outputs. It's kind of the future of computer programming in Natural Language. Language models are not designed to peform a task, all that it does is to predict the next token, thus you can trick the model into solving your problem.&#10;Example of a prompt:&#10;```&#10;---&#10;&#10;**Prompt:**&#10;&#10;You are an intelligent system that processes natural language queries and selects the most relevant SQL query from a given list. Based on the user's question, match the correct SQL query that will retrieve the desired information from a database.&#10;&#10;**Input:**&#10;&#10;- **User Query (NLP):** The user asks a question in natural language, describing the data they want from the database.&#10;- **SQL Queries List:** A list of SQL queries is provided as possible answers.&#10;&#10;**Task:**&#10;&#10;- Analyze the user's natural language question.&#10;- Select the most appropriate SQL query from the list that best answers the user's question.&#10;&#10;**Example:**&#10;&#10;- **User Query:** &quot;What are the names and email addresses of all customers who made a purchase in the last 30 days?&quot;&#10;- **SQL Queries List:**&#10;    1. `SELECT * FROM customers WHERE purchase_date &gt; '2023-09-01';`&#10;    2. `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;    3. `SELECT id, name FROM orders WHERE status = 'complete';`&#10;    4. `SELECT email FROM customers WHERE created_at &gt; NOW() - INTERVAL 1 YEAR;`&#10;&#10;**Expected Output:**&#10;&#10;- The system should select query 2: `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;```&#10;## Two ways of implementing Prompt Enginner&#10;* Easy way - using an Agent like ChatGPT. You can't really use it to integrate it into another app.&#10;* Programmatically integrate using python or similar.&#10;&#10;## 7 Tricks for prompt engineering&#10;1. Be Descriptive - give a context around the problem&#10;2. Give Examples&#10;3. Use Structured Text&#10;    ```&#10;    give me the recipe for making chocolate cookies, give it in the format&#10;    **Title**: Chocolate Cookie Recipe&#10;    **Description**: .......&#10;    ```&#10;5. Chain of Thoughts&#10;    ```&#10;    Make me a resume for a job application at Google.&#10;    Step 1: Write an objective&#10;    Step 2: Write an introduction about my overall work experience. they are...&#10;    Step 3: Write in detail each experience.&#10;    Step 4: Summary and conclusion.&#10;    ```&#10;6. Chatbot personas: &#10;    ```&#10;    Act as an travel guide who knows everything about Sydney. Make me a travel itenaryfor weekend in Sydney in your Aussie Accent.&#10;    ```&#10;7. Flipped Approach:&#10;    The generic response might not be of interest to you hence we have depend on a conversational model. This is useful when you dont know what exactly you want. e.g.&#10;    ```&#10;    I want you to ask me questions to help me come up with an LLM based application idea. Ask me one question at a time to keep things conversational..&#10;    ```&#10;8. Reflective, Review and Refine&#10;&#10;## ChatGPT v/s GPT3.0&#10;ChatGPT is a finetuned model - easy to get useful responses, however with GPT 3.0 that isn't the case and more work is to be done on prompt engineering side - it just does work prediction. &#10;&#10;## LangChain&#10;LangChain is a framework designed to help developers build applications that leverage language models (like GPT) more effectively by integrating them with other tools, data sources, and workflows. It simplifies the process of creating applications that combine various natural language processing tasks with external data, APIs, and user interactions.&#10;```shell&#10;pip install langchain&#10;pip install langchain-community langchain-core&#10;pip install huggingface_hub&#10;```&#10;&#10;```python&#10;from langchain import HuggingFaceHub # or use openai&#10;from langchain.prompts import PromptTemplate&#10;from langchain.chains import LLMChain&#10;import os&#10;&#10;os.environ['HUGGINGFACEHUB_API_TOKEN'] = '&lt;your hf token&gt;'&#10;hugging_face_llm = HuggingFaceHub(repo_id=&quot;google/flan-t5-base&quot;, model_kwargs={&quot;temperature&quot;: 0.5})&#10;&#10;prompt_template = PromptTemplate(&#10;    input_variables=[&quot;question&quot;],&#10;    template=&quot;&quot;&quot;You are the teacher, and you are running a surprise test to see who are the attentive kids. &#10;    The questions will be in the form &#10;    :\nQuestion: {question}&quot;&quot;&quot;&#10;)&#10;&#10;qa_chain = LLMChain(llm=hugging_face_llm, prompt=prompt_template)&#10;question = &quot;What is the capital of France?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;question = &quot;What is the name of indias PM?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;# Output&#10;# Paris&#10;# Narendra Modi&#10;```&#10;## Model fine Tuning&#10;A smaller fine tuned model can outperform a larger base model. This involves taking an existing or pre-trained model like GPT 3 for a specific usecase like ChatGPT (GPT-3.5-turbo).&#10;3 ways to fine tune:&#10;1. Self Supervised learning &#10;    - you get the Training Corpus of data can cater to your usecase.&#10;    - you then use this corpus of text and you train the model in a self supervise way.&#10;2. Supervised Learning &#10;    - here you have a set of inputs and outputs e.g. feed in who is the 35th president of the US? and output JFK.&#10;    - So having these question answer pairs we can train the model how to answer questions.&#10;    - One way of doing this is via prompt templates.&#10;      ```text&#10;        Please answer the following questions&#10;        Q: {Question}&#10;        A: {Answer}&#10;      ```&#10;    -  Through this process we could translate the training data set to a series of prompts and generate a training corpus and go back to the self supervised process.&#10;3. Reinforcement Learning -&#10;    - Supervised Fine tunning, two steps:&#10;        - curating your training dataset&#10;        - Fine tuning the model.&#10;        - done in 5 steps:&#10;          1. Choose fine tuning task. it could be anything e.g.&#10;              - could be text summarization&#10;              - could be text generation&#10;              - text/binary classification what ever you want to do..&#10;          2. Prepare training dataset.&#10;              - e.g. if text summarization then the input/output pairs of text in desired summarization generate a training corpus using for e.g. a prompt template &#10;          3. Choose a base model.&#10;              - lots of foundantal llms for e.g. or fine tuned llms.&#10;              - use this as the starting point.&#10;          4. Fine-tune model via supervised learning&#10;              - There are 3 different options:&#10;                  1. retrain all the parameters: here we tweak all the parameters, the computation cost is very very very high.&#10;                  2. transfer learning: here we freeze most of the parameters only fine tune the head. cheaper than full retaining all the parameters.&#10;                  3. Parameter Efficient Fine Tuning (PEFT): here we freeze all the weights or parameters instead of most. Instead we augment the model with additional parameters that are trainable. Advantage is we can fine tune the model with a relatively small set of model parameters as against the above approaches.&#10;                  - One of the ways to do this is LoRA (Low Rank Adaptation). In short fine tune model by adding new trainable parameters.&#10;                  ![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*GmCISYhd-JLqHNEvAQU1tQ.png)&#10;                  the first component here h(x) = Wox is what looks like a model without LoRA. Wo has weights that are all trainable. her Wo is a d by k (dxk) matrix with d*k trainable parameters. e.g. d=1000, k=1000 Wo is a 1,000,000 trainable parameters. But with LoRA the ΔWx, another weight matrix with the same shape as Wo. &#10;&#10;                  To simplify things lets just represent ΔW as a product of two terms B and A (BA) hence we can represent ΔW in terms of a 2 one dimentional arrays or vectors A and B, which essentially generates the new h(x).&#10;&#10;                  In this case Wox itself if frozen but B and A are trainable. hence in the above context d = 1000 and k = 1000 hence (d*r)+(r*k) for intrensic rank or r = 2 which translates into 4000 trainable parameters as against the million parameters.&#10;          5. Evaluate the model performance.&#10;    - Train Reward model&#10;        - generating a score for language models completetion. highscore for correct answer and low score for an incorrect answer.&#10;        - start with a prompt pass it into supervised fine tuned model. this you do multiple times and then assign human labels and then use the ranking to train the rewards model. &#10;    - Reinforcement learning with Favourite algorithm&#10;        - example in the case of ChatGPT it uses PPO or Proximal Policy Optimization&#10;        - you give the prompt and pass it into supervised fine tuned model and pass it back to reward model. The reward model then will give feedback to the finetuned model, this is how you update the model parameters. &#10;## Practical Supervised Fine tunning&#10;**First thing first**&#10;These are some of the classes and it's uses.&#10;| Class                                      | Description                                                             |&#10;|--------------------------------------------|-------------------------------------------------------------------------|&#10;| `AutoModel`                                | Automatically loads a pre-trained model for various tasks.             |&#10;| `AutoModelForSequenceClassification`      | Loads a model specifically for sequence classification tasks.           |&#10;| `AutoModelForTokenClassification`         | Loads a model for token classification tasks (e.g., NER).              |&#10;| `AutoModelForQuestionAnswering`           | Loads a model for question answering tasks.                             |&#10;| `AutoModelForCausalLM`                    | Loads a model for causal language modeling tasks (e.g., text generation).|&#10;| `AutoModelForMaskedLM`                    | Loads a model for masked language modeling tasks.                      |&#10;| `AutoModelForImageClassification`         | Loads a model for image classification tasks.                          |&#10;| `AutoTokenizer`                            | Automatically loads a tokenizer corresponding to a pre-trained model.  |&#10;| `AutoFeatureExtractor`                     | Loads a feature extractor for models that require image preprocessing.  |&#10;| `AutoConfig`                               | Loads configuration settings for a model.                              |&#10;| `AutoPipeline`                             | Automatically creates a pipeline for various tasks using a model.     |&#10;| `AutoModelForSpeechSeq2Seq`               | Loads a model for speech-to-text sequence generation tasks.            |&#10;| `AutoModelForAudioClassification`         | Loads a model for audio classification tasks.                          |&#10;| `AutoModelForSeq2SeqLM`                   | Loads a model for sequence-to-sequence tasks (e.g., translation).     |&#10;| `AutoModelForImageSegmentation`           | Loads a model for image segmentation tasks.                            |&#10;| `AutoModelForImageToText`                 | Loads a model for image-to-text generation tasks.                      |&#10;| `AutoModelForTextToImage`                 | Loads a model for text-to-image generation tasks.                      |&#10;| `AutoModelForTextClassification`          | A more general class for text classification tasks.                   |&#10;| `AutoModelForConversational`               | Loads a model designed for conversational tasks.                       |&#10;&#10;&#10;**ChatGPT generated code for Sentiment analysis and then Finetuning using LoRA.**&#10;you can upload your dataset into huggingface like in here.&#10;![](/blogs/img/posts/huggingface-dataset-shawhin.png)&#10;you can access it using &#10;&#10;```python&#10;import torch&#10;from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments&#10;from peft import get_peft_model, LoraConfig, TaskType&#10;&#10;# Load the tokenizer and base model&#10;model_name = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;&#10;tokenizer = AutoTokenizer.from_pretrained(model_name)&#10;model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)&#10;&#10;# Define LoRA configuration&#10;lora_config = LoraConfig(&#10;    r=4,                   # Intrinsic rank&#10;    lora_alpha=32,        # Scaling factor&#10;    lora_dropout=0.01,    # Dropout rate&#10;    target_modules=[&quot;q_lin&quot;],  # Target modules for LoRA&#10;    task_type=TaskType.SEQ_CLS&#10;)&#10;&#10;# Wrap the model with LoRA&#10;lora_model = get_peft_model(model, lora_config)&#10;&#10;# Example training data&#10;train_texts = [&#10;    &quot;It was good.&quot;,&#10;    &quot;Not a fan, don't recommend.&quot;,&#10;    &quot;Better than the first one.&quot;,&#10;    &quot;This is not worth watching even once.&quot;,&#10;    &quot;This one is a pass.&quot;&#10;]&#10;train_labels = [1, 0, 1, 0, 0]  # Example labels corresponding to the texts&#10;&#10;# Example evaluation data&#10;eval_texts = [&#10;    &quot;A fantastic experience!&quot;,&#10;    &quot;Horrible movie, would not watch again.&quot;,&#10;]&#10;eval_labels = [1, 0]  # Example labels for evaluation&#10;&#10;# Tokenize the training and evaluation data&#10;train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;&#10;# Create PyTorch datasets&#10;class SentimentDataset(torch.utils.data.Dataset):&#10;    def __init__(self, encodings, labels):&#10;        self.encodings = encodings&#10;        self.labels = labels&#10;&#10;    def __getitem__(self, idx):&#10;        item = {key: val[idx] for key, val in self.encodings.items()}&#10;        item['labels'] = torch.tensor(self.labels[idx])&#10;        return item&#10;&#10;    def __len__(self):&#10;        return len(self.labels)&#10;&#10;# Create datasets&#10;train_dataset = SentimentDataset(train_encodings, train_labels)&#10;eval_dataset = SentimentDataset(eval_encodings, eval_labels)&#10;&#10;# Define training arguments&#10;training_args = TrainingArguments(&#10;    output_dir=&quot;./lora_finetuned_model&quot;,&#10;    evaluation_strategy=&quot;epoch&quot;,&#10;    learning_rate=5e-5,&#10;    per_device_train_batch_size=2,&#10;    per_device_eval_batch_size=2,  # Add eval batch size&#10;    num_train_epochs=3,&#10;    weight_decay=0.01,&#10;)&#10;&#10;# Define Trainer&#10;trainer = Trainer(&#10;    model=lora_model,&#10;    args=training_args,&#10;    train_dataset=train_dataset,&#10;    eval_dataset=eval_dataset,  # Provide eval dataset&#10;)&#10;&#10;# Fine-tune the model&#10;trainer.train()&#10;&#10;# Save the fine-tuned model&#10;trainer.save_model(&quot;./lora_finetuned_model&quot;)&#10;&#10;&#10;eval_texts = [&#10;    &quot;An absolutely stunning film! The visuals were breathtaking, and the storyline kept me engaged the entire time.&quot;,&#10;    &quot;I was really disappointed with this film. The plot was weak and the characters were poorly developed.&quot;,&#10;    &quot;A heartwarming story that left me in tears. The performances were phenomenal and truly captured the essence of the characters.&quot;,&#10;    &quot;A boring and predictable movie that dragged on for too long. I wouldn't recommend it to anyone.&quot;,&#10;    &quot;This movie exceeded my expectations! The plot twists were fantastic, and the acting was top-notch. Highly recommend!&quot;,&#10;    &quot;The special effects couldn't save this film. It was a chore to sit through, and I found myself looking at my watch constantly.&quot;,&#10;    &quot;An inspiring tale that resonates on many levels. The direction was exceptional, and the soundtrack was unforgettable.&quot;,&#10;    &quot;An absolute disaster! The acting was cringe-worthy and the story made no sense whatsoever.&quot;,&#10;    &quot;A brilliant blend of action and comedy. I couldn't stop laughing, and the action scenes were exhilarating!&quot;,&#10;    &quot;I expected much more from this film. It felt like a cash grab with no real substance or originality.&quot;&#10;]&#10;&#10;# Get predictions&#10;predicted_sentiments = predict_sentiment(eval_texts)&#10;&#10;# Print the results&#10;for text, sentiment in zip(eval_texts, predicted_sentiments):&#10;    print(f&quot;Text: \&quot;{text}\&quot; - Sentiment: {sentiment}&quot;)&#10;```" />
              <option name="updatedContent" value="---&#10;title: NLP, LLM &amp; Generative AI  &#10;date: 2024-08-11&#10;tags: [&quot;Chatbots&quot;, &quot;RASA&quot;, &quot;ChatGPT&quot;, &quot;BERT&quot;, &quot;Transformers&quot;, &quot;Prompt Engineering&quot;]&#10;image : &quot;/img/posts/generative-ai-intro.jpeg&quot;&#10;Description  : &quot;Generative AI with NLP LLM: &#10;&quot;&#10;---&#10;&#10;Natural Language Processing (NLP) is a foundational field in artificial intelligence that enables computers to understand, interpret, and generate human language. With the rapid evolution of large language models (LLMs) and generative AI, NLP has become central to applications such as chatbots, virtual assistants, sentiment analysis, and automated translation. This document provides a practical overview of NLP concepts, pipelines, and tools, including hands-on examples with libraries like NLTK and spaCy. It also explores the rise of LLMs, prompt engineering, and modern frameworks that are shaping the future of language-based AI systems.&#10;&#10;# 1. Introduction&#10;---&#10;## NLP Tasks&#10;- **Language Modelling** : Predict the next word based on the sequence of words that already occurs in a given language. Application: speech recognition, OCR Translation etc.&#10;- **Text classification** : Assigning a text into one of the known categories based on content. Application: Email spam, sentiment analysis etc&#10;- **Information Extraction** : Extracting important information from a text. Application: extracting user's intent from input text, calendar etc.&#10;- **Information Retrieval** : Finding data based on user query. Application: Used in search engine.&#10;- **Conversational Agent** : A Dialogue system that can converse in a human language. Application: Siri, Alexa etc.&#10;- **Text Summarization** : Short summary of longer documents by retaining the important information. Application: Summary report generation from social media information.&#10;- **Question Answering** : Automatically answer questions posted in Natural Language. Application: Answering a user query based on data from a database.&#10;- **Machine Translation** : Converting a piece of text from one to another language. Application: Google transalator&#10;- **Topic Modelling** : Uncover the topical structure of large collection of text. Application: Text Mining&#10;&#10;## Understanding Human language and its building blocks&#10;- **Language**: words used in a Structured and conventional way and used to convey an idea by speech, writing or gesture.&#10;- **Linguistics**: Scientific study of a language and its structure. Study of language grammer, syntax and phonitics.&#10;  - Building Blocks:&#10;    - Phonemes: smallest unit of speech &amp; sound. English language has 44 of them. Applications: Speech to text transcriptions and text to speech conversations.&#10;    - morphemes and lexemes: Applications: Tokenization, Stemming, lemmatization, word embedding, parts of speech tagging.&#10;       - morphemes: smallest unit of a word. not all morphemes are words but the prefixes and suffixes are. e.g. 'multi' in multistory.&#10;       - lexemes: basic building block of a language. dictionary entries are lexems. lexemes are built on basic form e.g. walk, walking, walked.&#10; - **Syntax**: arragnement of words in a sentence. Representation of sentence is done using parse tree. Entity Extraction and relation extraction.&#10;    - syntax - phrases and sentences&#10;    - context - meaning&#10;    - Syntax Parse Tree:&#10;      ![](/blogs/img/posts/syntax-parse-tree.png)&#10;      - NP - noun phrase&#10;      - VP - verb phrase&#10;      - PP - prepositional phrase&#10;      - S - sentence at the highest level.&#10;- **Context**: words and sentences that surround any part of discourse and that helps determine the meaning. Application: Sarcasm detection, summarization, topic modelling. Made up of:&#10;  - semantics: direct meaning &#10;  - pragmatics: adds world knowledge and external knowledge.&#10;&#10;## Challenges of NLP&#10;- Ambiguity: two or more meanings of a single passage. e.g. we saw her duck. Common knowledge assumptions. e.g he says Sun rises in the west (assumption that a preson knows sun rises in the east)&#10;- Creativity&#10;&#10;---&#10;# 2. Pipeline of NLP&#10;---&#10;## NLP Pipeline&#10;Step by step processing of text is known as NLP Pipeline:&#10;- Data collection (scrapy)&#10;- Text Cleaning &#10;- Pre-processing (stemming and lemmetization)&#10;- Feature engineering (one hot encoding, bag of words technique)&#10;- Modeling&#10;- Evaluation&#10;- Deployment&#10;- Monitoring&#10;---&#10;&#10;---&#10;## NLTK library&#10;NLTK library is most commonly used NLP library. Common text pre-processing steps in NLP: &#10;  - Tokenization: breaking up text into smaller pieces called tokens. &#10;  - Stemming&#10;  - Lemmatization&#10;  - Word Embedding&#10;  - Parts of speech tagging&#10;  - Stop Word removal&#10;  - Word Sence disambiguation&#10;  - Named Entity Recognition (NER)&#10;    &#10;### Tokenization: breaking up text into smaller pieces called tokens. &#10;- 3 types of tokenizers in NLTK&#10;  - word_tokenize()&#10;  - wordpunct_tokenize()&#10;  - sent_tokenize()&#10;- when a tokenization is performed, we get individual tokens. sometimes it is necessary to group multiple tokens into 1.&#10;  - Unigrams: &quot;Steve&quot; &quot;went&quot; &quot;to&quot; &quot;school&quot;&#10;  - Bigrams: tokens of two consequtive words in a sentence; &quot;Steve went&quot; &quot;went to&quot; &quot;to school&quot;&#10;  - Trigrams: tokens of 3; &quot;Steve went to&quot; &quot;went to school&quot;&#10;  - Ngrams: tokens of n&#10;&#10;Setting the stage for tokenization:&#10;```python&#10;import nltk&#10;nltk.download('punkt')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens = nltk.word_tokenize(text)&#10;list(nltk.bigrams(ml_tokens)) # or trigrams&#10;```&#10;### Parts of speech tagging &amp; Stop words removal&#10;- Parts of speech tagging: process of marking words as corresponding to parts of speech, based on both definition and context.&#10;  - e.g. I like(Verb) to read(Verb) books&#10;  - this is helpful in understanding the context in which a word is used.&#10;- stopwords&#10;  - e.g. a, the, is, are&#10;  - not adding any important information, which can be elimiated.&#10;&#10;```python&#10;ml_tokens=nltk.word_tokenize(&quot;Jerry eats a banana&quot;)&#10;nltk.download(&quot;averaged_perceptron_tagger&quot;) # needs to be downloaded for tagging.&#10;for token in ml_tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;This outputs&#10;```python&#10;[('Jerry', 'NN')]&#10;[('eats', 'NNS')]&#10;[('a', 'DT')]&#10;[('banana', 'NN')]&#10;```&#10;pos_tag is a very basic version of the library, see how Jerry and eats is NNS - its tagged as a single term and categorized it as a Noun. In real life we use pos_tag from spacial library or transformers.&#10;&#10;### Regular expression tokenizer&#10;```python&#10;from nltk.tokenize import RegexpTokenizer&#10;sent = &quot;Jerry eats a banana&quot;&#10;reg_tokenizer = RegexpTokenizer('(?u)\W+|\$[\d\.]+|\S+')&#10;tokens = reg_tokenizer.tokenize(sent)&#10;for token in tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;&#10;```python&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens=nltk.word_tokenize(text)&#10;filtered_data = [w for w in ml_tokens if not w in stop_words]&#10;filtered_data&#10;```&#10;&#10;### Stemming, Lemmatization&#10;#### Stemming&#10;Reducing a word or part of a word to its stem or root form. It lowers the inflection (process we do inorder to modify the word in order to communicate mini-gramatical categories like tensors, voices, aspect, gender, mood etc. added to communicate to other person) of words into their root form. This is a pre-processing activity.&#10;Using the same word in different inflected forms in a text can lead to redundancy in natural language processing tasks. By reducing inflection, we decrease the number of unique words that machine learning models need to process.&#10;&#10;**Example 1**&#10;* Without Inflection: Original sentence: &quot;She runs every day, and they are running in the park while he ran yesterday.&quot;&#10;Inflected forms: runs, running, ran&#10;* With Reduced Inflection:Simplified sentence: &quot;She run every day, and they run in the park while he run yesterday.&quot; In this simplified version, we use &quot;run&quot; for all forms.&#10;* Impact: Original sentence has three different inflected forms, which can create redundancy for a natural language processing model.&#10;Simplified sentence reduces the variety of words, making it easier for the model to analyze the core action (running) without getting bogged down by different forms.&#10;&#10;**Example 2**&#10;&#10;* after stemming Generate → Generat also Generation → Generat&#10;* Stemming can create non-dictionary forms (like &quot;generat&quot;). It's important to note that in stemming, the goal is to reduce words to their root form, which might not always be a valid dictionary word. The main purpose of stemming is to reduce data redundancy by grouping related words together. The primary aim is to reduce the number of unique words that machine learning models need to process.&#10;&#10;**Uses:**&#10;- SEO&#10;- Text mining&#10;- Web-search&#10;- Indexing&#10;- Tagging.&#10;&#10;**4 Types of Stemming Algorithms:**&#10;* Porter Stemmer: Martin Porter invented it and Original Stemmer algorithm. Ease of use and rapid. &#10;* Snowball Stemmer: Also invented by same guy. more presise than porter stemmer.&#10;* Lancaster Stemmer: Sometimes does over stemming, sometimes non linguistic or meaningless. &#10;* Regex Stemmer: morphological affixes.&#10;&#10;```python&#10;from nltk.stem import PorterStemmer&#10;porter = PorterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {porter.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gener&#10;# generation -&gt; gener&#10;# genorously -&gt; genor&#10;# generate -&gt; gener&#10;from nltk.stem import SnowballStemmer&#10;snowball = SnowballStemmer(language='english')&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {snowball.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generat&#10;# genorously -&gt; genor&#10;# generate -&gt; generat&#10;from nltk.stem import LancasterStemmer&#10;lancaster = LancasterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lancaster.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gen&#10;# generation -&gt; gen&#10;# genorously -&gt; gen&#10;# generate -&gt; gen&#10;from nltk.stem import RegexpStemmer&#10;regex = RegexpStemmer('ing|s$|able$',min=4)&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {regex.stem(word)}&quot;)&#10;#Output&#10;# generous -&gt; generou&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;#### Lemmatization&#10;Converting the words into root word using Parts of Speech (POS) tag as well as context as a base. Similar to stemming but brings context to the words and the result is a word in the dictionary. &#10;* Applications e.g. search engine and compacting&#10;**Example:**&#10;* eats → eat&#10;* ate → eat&#10;* ate → eat&#10;* eating → eat&#10;&#10;```python&#10;import nltk&#10;from nltk.stem import WordNetLemmatizer&#10;nltk.download('wordnet')&#10;lemma = WordNetLemmatizer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lemma.lemmatize(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;&#10;# Named Entity Recognition&#10;* First step in the information extraction&#10;* NER seeks to locate and classify named entities into pre-defined categories such as names of person, Organization, location etc. e.g. Modi, America, Apple Inc, Tesla&#10;&#10;## Challenges:&#10;- Word sense disambiguiation: method by which meaning of the word is determined from the context it is used.&#10;- Example: bark, cinnamon bark or sound made by dog is bark.&#10;- when the two sentences passed to the algorithm word sense disambiguiation comes into picture, it removes the ambiguity. &#10;&#10;## Application:&#10;* Text mining&#10;* Information extraction&#10;* used alongside with Lexicography&#10;* Information retrieval process&#10;&#10;## Word Sence disambiguation:&#10;**Lesk Algorithm:** based on the idea that words in each region will have a similar meaning.&#10;```python&#10;from nltk.wsd import lesk&#10;from nltk.tokenize import word_tokenize&#10;nltk.download('punkt')&#10;a1=lesk(word_tokenize('The building has a device to jam the signal'), 'jam')&#10;print(a1, a1.definition())&#10;a2=lesk(word_tokenize('I am stuck in a traffic jam'), 'jam')&#10;print(a2, a2.definition())&#10;a3=lesk(word_tokenize('I like to eat jam with bread'), 'jam')&#10;print(a3, a3.definition())&#10;#Output&#10;# Synset('jamming.n.01') deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems&#10;# Synset('jam.v.05') get stuck and immobilized&#10;# Synset('jam.v.06') crowd or pack to capacity --&gt; Somehow this isn't coming correct&#10;```&#10;&#10;## Named Entity Recognition&#10;```python&#10;nltk.download('averaged_perceptron_tagger')&#10;nltk.download('maxent_ne_chunker')&#10;nltk.download('words')&#10;text=&quot;Apple is an American company based out of California&quot;&#10;for w in nltk.word_tokenize(text):&#10;  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))):&#10;    if hasattr(chunk, 'label'):&#10;      print(chunk.label(), ' '.join(c[0] for c in chunk))&#10;# Output - GPE stands for Geo political entity&#10;# GPE Apple&#10;# GPE American&#10;# GPE California&#10;``` &#10;# spaCy Library&#10;spaCy is a free open source library for advaned Natural Language Processing in python for production use. NLTK was for research purpose. spaCy is for production use. Can handle and process large volume of text.&#10;## Features&#10;- Tokenization &#10;- Parts of Speech Tagging - word types of tokens, like verb or noun.&#10;- Dependency Parsing&#10;- Lemmatization&#10;- Sentence Boundary Detection (SBD) - finding and segmenting individual sentences.&#10;- Named Entity Recognition&#10;- Entity Linking (EL)- Disambiguating texual entities to unique identifiers ina knowledge base.&#10;- Similarity - comparing words, text apans and documents and how similar they are to each other.&#10;- Text Classification - assigning caterfores or labels to a whole document or parts of it.&#10;- Rule based Matching - finding sequence of token based on their texts and linguistic annotations, similar to regular expressions.&#10;- Training - updating and improving a statstical models predictions&#10;- Serialization - Saving objects to files or byte string.&#10;```python&#10;import spacy&#10;nlp = spacy.load(&quot;en_core_web_sm&quot;) # verson of spacy library - english small model&#10;doc = nlp(&quot;Apple is looking at buying U.K startup for $1 billion&quot;) # by default the spacy applies tagger, parser, ner&#10;&#10;for token in doc:&#10;  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)&#10;&#10;# Output&#10;# Apple Apple PROPN NNP nsubj Xxxxx True False&#10;# is be AUX VBZ aux xx True True&#10;# looking look VERB VBG ROOT xxxx True False&#10;# at at ADP IN prep xx True True&#10;# buying buy VERB VBG pcomp xxxx True False&#10;# U.K U.K PROPN NNP dobj X.X False False&#10;# startup startup VERB VB dep xxxx True False&#10;# for for ADP IN prep xxx True True&#10;# $ $ SYM $ quantmod $ False False&#10;# 1 1 NUM CD compound d False False&#10;# billion billion NUM CD pobj xxxx True False&#10;```&#10;* in the above by default the spacy applies tagger, parser, ner. The steps however can be added or replaced.&#10;![](https://spacy.io/images/pipeline.svg)&#10;&#10;* first step is tokenization&#10;![](https://spacy.io/images/tokenization.svg)&#10;&#10;```python&#10;text=&quot;Mission impossible is one of the best movies I have watched. I love it.&quot;&#10;print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(&quot;text&quot;, &quot;lemmatization&quot;, &quot;partofspeech&quot;, &quot;TAG&quot;, &quot;DEP&quot;, &quot;SHAPE&quot;, &quot;ALPHA&quot;, &quot;STOP&quot;))&#10;doc = nlp(text)&#10;for token in doc:&#10;  print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop))&#10;&#10;# text      |lemmatization  |partofspeech   |TAG       |DEP       |SHAPE     |ALPHA     |STOP      &#10;# Mission   |mission        |NOUN           |NN        |nsubj     |Xxxxx     |         1|         0&#10;# impossible|impossible     |ADJ            |JJ        |amod      |xxxx      |         1|         0&#10;# is        |be             |AUX            |VBZ       |ROOT      |xx        |         1|         1&#10;# one       |one            |NUM            |CD        |attr      |xxx       |         1|         1&#10;# of        |of             |ADP            |IN        |prep      |xx        |         1|         1&#10;# the       |the            |DET            |DT        |det       |xxx       |         1|         1&#10;# best      |good           |ADJ            |JJS       |amod      |xxxx      |         1|         0&#10;# movies    |movie          |NOUN           |NNS       |pobj      |xxxx      |         1|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# have      |have           |AUX            |VBP       |aux       |xxxx      |         1|         1&#10;# watched   |watch          |VERB           |VBN       |relcl     |xxxx      |         1|         0&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# love      |love           |VERB           |VBP       |ROOT      |xxxx      |         1|         0&#10;# it        |it             |PRON           |PRP       |dobj      |xx        |         1|         1&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I you do not understand sonething&#10;print(spacy.explain('nsubj')) #nominal subject&#10;print(spacy.explain('pobj')) #object of preposition&#10;# print entities&#10;```&#10;* Extracting the Named Entities&#10;```python&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia&quot;&#10;doc = nlp(text)&#10;for token in doc.ents:&#10;  print(token)&#10;# Output&#10;# Narendra Modi&#10;# India&#10;# Asia&#10;```&#10;&#10;* If you want to see a colourful version of the named entities then,&#10;```python&#10;from spacy import displacy&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia which embraces Machine Learning&quot;&#10;doc=nlp(text)&#10;displacy.render(docs=doc, style=&quot;ent&quot;,jupyter=True)&#10;spacy.explain('GPE') #Geo Political Entity&#10;```&#10;&#10;# NLP Text Vectorization&#10;Convertion of raw text into numerical form is called Text Vectorization. Machine learning expects text in numerical form. This is also called Feature Extraction.&#10;Many ways of achieving feature extraction:&#10;1. One Hot Encoding&#10;2. Count Vectorizer&#10;3. TF-IDF&#10;4. Word Embeddings&#10;&#10;## One Hot Encoding&#10;Every word including symbols are written in the vector form. This vector will only have 0 &amp; 1s. each word is written or encoded as a one hot vector, each word will have different vector representation. example:&#10;&#10;| Color  | Red | Blue | Green |&#10;|--------|-----|------|-------|&#10;| Red    |  1  |  0   |   0   |&#10;| Blue   |  0  |  1   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;| Red    |  1  |  0   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;&#10;```python&#10;corpus = ['dog eats meat','man eats meat']&#10;from sklearn.preprocessing import OneHotEncoder&#10;one_hot = OneHotEncoder()&#10;all_in_one = [indi.split() for indi in corpus]&#10;one_hot.fit_transform(all_in_one).toarray()&#10;#Output&#10;# [['dog', 'eats', 'meat'], ['man', 'eats', 'meat']]&#10;# array([[1., 0., 1., 1.],&#10;#        [0., 1., 1., 1.]])&#10;```&#10;&#10;we generally dont use the scikitlearn onehotencoding directly as it's mainly for structured data not for unstructured data.&#10;&#10;### Disadvantages&#10;* Size of the one hot encoding is propotional to the size of the vocabulary.&#10;* Sparse representation of data&#10;* Insufficent in storing, computing and learning from data.&#10;* No sequence of words is considered and is ignored.&#10;* If words outside the vocabulary exists there is no way to deal with it.&#10;* Word context is not considered in the representation.&#10;&#10;## Bag of Words technique (BoW)&#10;NLP pipeline has multiple steps as mentioned above. This step comes in the feature engineering step. Classical text represenation technique. Representation of the text under the consideration of bag of words. Text is characterised by a unique set of words. e.g. movie was bad; movie was excellent. This is characterised by the unique set of words not based on where it occurs in the sentence. so if the word bad it will be in one bag and excellent it will be in a different bag.&#10;&#10;**Application:** Sentiment analysis (positive and negative sentiments). Harry potter was good, a movie was good - they are classified into the same bag.&#10;&#10;### Write your own Bow Representation&#10;```python&#10;# if you are adventrous and dont want to use the Count Vectorizer.&#10;import pandas as pd&#10;import re&#10;t1 = &quot;dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower()).split()]&#10;&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;&#10;def bow(all, sentences):&#10;  results = []&#10;  for sentence in sentences:&#10;    result = {word: 0 for word in all}&#10;    for word in sentence:&#10;      result[word] = 1&#10;    results.append(result)&#10;  print(pd.DataFrame(results))&#10;&#10;bow(all_words, sentences)&#10;# Output&#10;# dog  eats  meat  everyday  man  once  in  a  while  rarely&#10;# 0    1     1     1         1    0     0   0  0      0       0&#10;# 1    0     1     1         0    1     1   1  1      1       0&#10;# 2    1     1     0         0    1     0   0  0      0       1&#10;```&#10;### Disadvantages:&#10;* Size of the vector increases with the size of the vocabulary&#10;* Sparsity (property of being scattered) is still an issue.&#10;* Does not capture the similarity between words (not context aware). 'I eat', 'I ate', 'I ran' Bag of Words Vectors for all the three documents will be equally apart - in layman terms - 'eat and ran' and 'eat and ate' will be same distance apart.&#10;&#10;&#10;```python&#10;# use the countvectorize or just write your own python code after finding the unique words&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;man eats meat once in a while.&quot;&#10;t3 = &quot;man eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;# vectorizer = CountVectorizer(binary=True) --&gt; use this for sentiment analysis  &#10;vectorizer = CountVectorizer()  &#10;X = vectorizer.fit_transform([t1, t2, t3])&#10;print(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;pd.DataFrame(bag_of_words, columns=feature_names)&#10;# Output&#10;# dog&#9;eats&#9;everyday&#9;in&#9;man&#9;meat&#9;once&#9;rarely&#9;while&#10;# 0&#9;5&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;1&#9;0&#10;```&#10;```Note: ``` vectorizer = CountVectorizer(**binary=True**)``` if you dont want actual counts but just 1s and 0s. This is a technique used specific to sentiment classification&#10;&#10;&#10;Now even if you want it as a unigram, bigram and trigram thats also possible.&#10;```python&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] &#10;unique_words = set(all_words)&#10;vectorizer = CountVectorizer(ngram_range=(1,3)) # See here &lt;--&#10;X = vectorizer.fit_transform(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;print(&quot;Feature Names (Vocabulary):&quot;, feature_names)&#10;print(&quot;Bag of Words Representation:&quot;)&#10;pd.DataFrame(bag_of_words)&#10;#Output&#10;# Feature Names (Vocabulary): ['dog' 'dog dog' 'dog dog dog' 'dog dog eats' 'dog eats' 'dog eats meat'&#10;#  'dog rarely' 'eats' 'eats dog' 'eats dog rarely' 'eats meat'&#10;#  'eats meat everyday' 'eats meat once' 'everyday' 'in' 'in while' 'man'&#10;#  'man eats' 'man eats dog' 'man eats meat' 'meat' 'meat everyday'&#10;#  'meat once' 'meat once in' 'once' 'once in' 'once in while' 'rarely'&#10;#  'while']&#10;# Bag of Words Representation:&#10;# 0&#9;1&#9;2&#9;3&#9;4&#9;5&#9;6&#9;7&#9;8&#9;9&#9;...&#9;19&#9;20&#9;21&#9;22&#9;23&#9;24&#9;25&#9;26&#9;27&#9;28&#10;# 0&#9;5&#9;4&#9;3&#9;1&#9;1&#9;1&#9;0&#9;1&#9;0&#9;0&#9;...&#9;0&#9;1&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#9;0&#9;...&#9;1&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;1&#9;1&#9;1&#9;...&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#10;# 3 rows × 29 columns&#10;```&#10;### Pros and Cons&#10;- has the ability to capture the context and word order information in the form of n-grams&#10;- Documents  having the same ngrams will have vectors closer to each other in euclidean space as compared to documents with different ngrams.&#10;- As n increased the dimensionlity (sparsity) increases&#10;- issue related to out of vocabulary problem exists&#10;&#10;## TF-IDF &#10;- a word most repeated in one document but not in any other documents are considered more  important. Stop words however dont fall into this category. &#10;- Term Frequency (TF) * Inverse Document Frequency (IDF)&#10;- quantify a word in a set of documents.&#10;- importance of words in the given context is represented here.&#10;&#10;**Terminology**&#10;t - term&#10;d - document (set of words)&#10;N - count of corpus&#10;corpus - the total document set.&#10;e.g. 'This Dress is so beautiful' - how is the computer to know that the important words here are dress and beautiful? thats where TF*IDF shines.&#10;&#10;* TF - number of times a particular word appears in a sentence.&#10;e.g. Sun rises in East; frequency of Sun - 1/4&#10;* IDF - Dress is beautiful; is isn't adding any importance. stop words needs to be weightage reduced particularly when these words are used more freqently it's importance will increase. IDF measures the informativeness of term t. it will be low for stop words. inverse document frequency ```formula: idf(t) = log(N/(df+1))```&#10;&#10;```&#10;IDF(word)=log10(total number of documents/ (1+number of documents containing the word))&#10;```&#10;&#10;hence, ```TF-IDF formula: tf-idf(t,d) = tf(t,d) * log(N/(df+1)) ```&#10;where, **N** - total number of documents in the corpus &amp; **df** - number of document with term t.&#10; e.g. lets say sentences: &#10;&#10; ```python&#10; import math&#10;s1='man eats pizza'&#10;s2='dog eats food'&#10;s3='ant eats pizza'&#10;# for man in s1 → tf = 1/3 &#10;# idf = log₂(3/1) &#10;tf = 1/3 &#10;idf = math.log(3/2)&#10;tf_idf = tf *  idf&#10;print(tf_idf) # 0.13515503603605478&#10;# for eats in s1 → tf = 1/3 &#10;tf = 1/3&#10;idf = math.log(3/4)&#10;tf_idf = tf*idf&#10;print(tf_idf) #-0.09589402415059363&#10;# hence eats is not a very important word.&#10;```&#10;## tf-idf hands on&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;freq1 = {}&#10;freq2 = {}&#10;for word in vocabulary:&#10;  freq1[word] = word_dict1[word]/len(first_sent)&#10;  freq2[word] = word_dict2[word]/len(second_sent)&#10;&#10;pd.DataFrame([freq1, freq2])&#10;```&#10;## implement the tf-idf using scikit&#10;it is supposed to be something like this. Below isn't fully working need to check why.&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data machine science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;def calculateTF(doc):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateIDF(docs):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateTFIDF(tfBagOfWords, idfs):&#10;  print(idfs)&#10;  tfIdf = {}&#10;  for word, value in tfBagOfWords.items():&#10;    tfIdf[word] = value*idfs[word]&#10;  return tfIdf&#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;pd.DataFrame([&#10;    calculateTFIDF(calculateTF(word_dict1), calculateIDF([word_dict1, word_dict2])),&#10;    calculateTFIDF(calculateTF(word_dict2), calculateIDF([word_dict1, word_dict2]))&#10;    ])&#10;```&#10;&#10;## implement using sklearn&#10;```python&#10;import sklearn&#10;import pandas as pd&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;vec = TfidfVectorizer()&#10;result = vec.fit_transform([first_sent, second_sent])&#10;pd.DataFrame(result.toarray(), columns=vec.get_feature_names_out())&#10;# Output&#10;# amazing&#9;an&#9;career&#9;current&#9;data&#9;deep&#9;in&#9;is&#9;learning&#9;machine&#9;of&#9;science&#9;subset&#9;the&#9;world&#10;# 0&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.230768&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.324336&#10;# 1&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.243956&#9;0.685743&#9;0.342871&#9;0.342871&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.000000&#10;```&#10;&#10;## Pros and cons of the tf-idf technique&#10;### Advantages&#10;- use this to calculate the similarity between two texts using similarity measures like Cosing similarity/Euclidean distance&#10;- has application in text classification, information retrieval etc.&#10;- better than earlier methods.&#10;### Disadvantages&#10;- high dimentionality&#10;- They are still discrete representation of units of text, hence unable to capture relation between words&#10;- sparse and high dimension&#10;- cannot handle OOV (Out of Vocabulary) words.&#10;---&#10;# TBC&#10;---&#10;https://www.youtube.com/watch?v=tFHeUSJAYbE&amp;list=PLz-ep5RbHosU2hnz5ejezwaYpdMutMVB0&#10;# Large Language Models (LLMs)&#10;is a type of Language Model. Quantatively it is the number of model parameters vary from 10 to 100 billion parameters per model. Qualitatively also called emergent properties starts emerging - properties in large language model that do not appear in small language models, e.g. zero shot learning - capability of a model to complete a task it is not explicitely trained to do.&#10;&#10;In the earlier days the model was trained using supervised learning we use thousands if not millions of examples - but with LLMs we use self-supervised learning. Train a very large model in a very large corpus of data. In self-supervised learning doesn't require manual labelling of each example. The labels or the targets of the model defined from the inherent structure of the data itself.&#10;&#10;One of the popular way of doing this is &quot;next word prediction paradigm&quot;. There is not just one word but many that can go after ***listen to your....***. What the llm would do is to use probablistic distribution of next word given the previous word. in the above example the words could be heart or gut or body or parents etc.. each with different probability distribution. Its essentially trained on a large set of data with so many examples of corpus of data - so it can statistically predict the next set of data. Important thing is the context matters - if for example we add the word ***don't*** in front of ***listen to your...***, the probably distribution will entirely change.&#10;&#10;Autoregression Task formula: ***P(tn | tn-1,..., tn-m)*** P(tn) given n tokens.&#10;&#10;This is how LLMs like chatgpt works.&#10;# 3 levels of Using LLMs&#10;- Level 1: Prompt Engineering&#10;  - using LLM out of the box - not changing any model parameters. Two ways to do this &#10;    - using an agent like chatgpt&#10;    - using open AI API or hugging face tranformers library: help to interact with LLMs programmatically using python for example. Pay per api call in case of open API. Hugging face transformer library is an open source option, you can run the models locally in this case so no need to send your proprietary data into 3rd party or open ai.&#10;- Level 2: Model Fine Tuning&#10;  - adjusting model parameters for a particular tasks.&#10;  - steps&#10;    - Step 1: pre-trained models are obtained. (usually trained by self supervised learning). in this step the base model is learning useful representations for a wide variety of tasks.&#10;    - Step 2: update model parameters given task-specific examples (trained by supervised learning or even reinforcement learning).e.g. chatgpt, the model we use here is a fine tuned model learnt by reinforcement learning. Some techniques is lora or low range adaptation. another technique is reinforcement learning based on human feedback (RLHF).&#10;    - Step 3: Deploy the fine tuned large language model.&#10;- Level 3: Build your own.&#10;  - This is only for 1% of all usecases. &#10;  - One example usecase: in a large company we dont want to use open source models where security is a concern, dont want to send data to 3rd party via an API. &#10;  - Another usecase is you want to create your own model and commercialize it.&#10;  - At a high level steps are:&#10;    - get the data or corpus.&#10;    - pre process and refine it &#10;    - model training&#10;    - pre trained llm.&#10;    - then go to step 2.&#10;&#10;## Connecting to AI using API, Programmatically&#10;### OpenAIs Python API&#10;It's similar to chatGPT but with Python. In both we pass a request and use the language modelling to predict the next word. Apart from the difference in the web interface in chatgpt and here programmatically some differences are as follows. most of the below aren't possible with chatgpt but programmatically possible with openai python.&#10;1) Customizable System message: Message or prompt or a set of instructions that help define the tone, personality and functionality of the model during a conversation. This helps model how to respond to user input and what constraints to follow. I customized the message in chatgpt first to give back sarcastic answers.&#10;![](/blogs/img/posts/chatgpt-customized-system-message.png)&#10;![](/blogs/img/posts/chatgpt-customized-system-message-output.png)&#10;Then i changed the message to give negative and dark response. This time the results were entirely opposite.&#10;![](/blogs/img/posts/chatgpt-customized-system-message-dark.png)&#10;2) Adjust input parameter &#10;  - max response length: response length sent back by model&#10;  - number of responses: (number of outputs you may want to programmatically select one of the response e.g.)&#10;  - temperature: randomness of response generated by the model.&#10;3) Process image and other types&#10;4) extact helful word embeddings for downstream tasks&#10;5) input audio for transcription and tranlations&#10;6) model fine tuning functionality.&#10;7) with chatgpt can only use GPT 3.5 or 4, with openai several other models are available read: https://platform.openai.com/docs/models&#10;### Costing:&#10;Tokens &amp; Pricing:&#10;same as tokenization above a given text is converted and represented as numbers. Pricing is based on the tokens, bigger prompts will incur larger costs. To use we have to get the Secret key to make API calls.&#10;&#10;```python&#10;import openai&#10;from openai import OpenAI&#10;from sk import openai_key # my own file with a variable openai_key='sk-proj-4D1ID8ZeQ...'&#10;&#10;client = OpenAI(api_key=openai_key)&#10;response = client.chat.completions.create(&#10;    model=&quot;gpt-3.5-turbo&quot;,&#10;    max_tokens=2,&#10;    temperature=2, # degree of randomness, 0 is predictable.&#10;    n=3,  &#10;    messages=[&#10;        {&#10;            &quot;role&quot;: &quot;user&quot;, &#10;            &quot;content&quot;: &quot;where there is a will there is a &quot;&#10;        }&#10;    ]&#10;)&#10;&#10;for idx, choice in enumerate(response.choices):&#10;    print(f&quot;Response {idx+1}: {choice['message']['content']}&quot;)&#10;&#10;# Output &#10;# Response 1: way.&#10;# Response 2: plan.&#10;# Response 3: chance.&#10;# note that 2 tokens - 'way' and '.'&#10;```&#10;## Hugging face Transformer library&#10;Major hub for open source Machine learning (ML) like Dockerhub for docker. It has models, dataset (its own data used to train models) and spaces (for building and deploying machine learning applications).&#10;&#10;### Transformers library&#10;Downloading and training machine learning models in python. Like NLP, computer vision, audio processing etc. E.g. for sentiment analysis - find the model that does sentiment analysis classification task then you have to take raw text convert into numerical value that is then passed to the model; finally decode the numerical output of the output to get the label of the text. This can be done easily in the transformers library using a pipeline function. &#10;other things that can be done &#10;- sentiment analysis&#10;- summarization&#10;- translation&#10;- question-answering&#10;- feature extraction &#10;- text generation etc.&#10;&#10;```python&#10;! pip install transformers&#10;from transformers import pipeline&#10;sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;)&#10;# sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;, model=&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;)&#10;texts = [&#10;    &quot;&quot;&quot;One is that the mining giant's shares are pushing higher this morning.&#10;        In early trade, the Big Australian's shares are 1.5% higher to $45.74.&#10;        This means that the BHP share price is now up 13% over the past two weeks.&quot;&quot;&quot;&#10;]&#10;results = sentiment_pipeline(texts)&#10;for text, result in zip(texts, results):&#10;    print(f&quot;Text: {text}\nSentiment: {result['label']}, Score: {result['score']}\n&quot;)&#10;```&#10;**Question**: How does it decide if a text is positive or negative without perception?&#10;### signup and logininto huggingface&#10;- lookup for transformer tag and select a model. Then you will check also for pytorch tag. This is because hugging face also supports models which aren't just compatible with pytorch and transformers but also others.&#10;- The train button on the right will have options like Amazon Sagemaker, NVIDIA NDX Cloud, AutoTrain which will help jump start the model finetuning part.&#10;-  &#10;### Getting started&#10;to get started copy the [hf-env.yml](https://github.com/ShawhinT/YouTube-Blog/blob/26dff2786a7d64620e5e7dd71fcd51a416aad1db/LLMs/hugging-face/hf-env.yml) file into your code repository.&#10;&#10;```bash&#10;conda env create --file hf-env.yml&#10;```&#10;another example for text-classification&#10;```python&#10;from transformers import pipeline&#10;classifier = pipeline(task=&quot;text-classification&quot;, model=&quot;SamLowe/roberta-base-go_emotions&quot;, top_k=None)&#10;sentences = [&quot;I am not having a great day&quot;]&#10;model_outputs = classifier(sentences)&#10;print(model_outputs[0])&#10;# Output&#10;# [{'label': 'disappointment', 'score': 0.4666951894760132}, {'label': 'sadness', 'score': 0.39849498867988586}, {'label': 'annoyance', 'score': 0.06806593388319016}, {'label': 'neutral', 'score': 0.05703023821115494}, {'label': 'disapproval', 'score': 0.044239308685064316}, {'label': 'nervousness', 'score': 0.014850745908915997}, {'label': 'realization', 'score': 0.014059904962778091}, {'label': 'approval', 'score': 0.0112674655392766}, {'label': 'joy', 'score': 0.006303396541625261}, {'label': 'remorse', 'score': 0.006221492309123278}, {'label': 'caring', 'score': 0.006029403302818537}, {'label': 'embarrassment', 'score': 0.0052654859609901905}, {'label': 'anger', 'score': 0.004981426056474447}, {'label': 'disgust', 'score': 0.004259029403328896}, {'label': 'grief', 'score': 0.0040021371096372604}, {'label': 'confusion', 'score': 0.003382918192073703}, {'label': 'relief', 'score': 0.0031405005138367414}, {'label': 'desire', 'score': 0.00282747158780694}, {'label': 'admiration', 'score': 0.002815794898197055}, {'label': 'fear', 'score': 0.002707520266994834}, {'label': 'optimism', 'score': 0.0026164911687374115}, {'label': 'love', 'score': 0.0024883910082280636}, {'label': 'excitement', 'score': 0.0024494787212461233}, {'label': 'curiosity', 'score': 0.0023743617348372936}, {'label': 'amusement', 'score': 0.001746696187183261}, {'label': 'surprise', 'score': 0.0014529851032420993}, {'label': 'gratitude', 'score': 0.0006464761681854725}, {'label': 'pride', 'score': 0.00055424973834306}]&#10;```&#10;yet another example for summarization&#10;```python&#10;from transformers import pipeline&#10;summarizer = pipeline(&quot;summarization&quot;, model=&quot;Falconsai/text_summarization&quot;)&#10;ARTICLE = &quot;&quot;&quot; &#10;Hugging Face: Revolutionizing Natural Language Processing&#10;Introduction&#10;In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.&#10;The Birth of Hugging Face&#10;Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name &quot;Hugging Face&quot; was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.&#10;Transformative Innovations&#10;Hugging Face is best known for its open-source contributions, particularly the &quot;Transformers&quot; library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.&#10;Key Contributions:&#10;1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.&#10;2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.&#10;3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.&#10;Democratizing AI&#10;Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.&#10;By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.&#10;Industry Adoption&#10;The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.&#10;Future Directions&#10;Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.&#10;Conclusion&#10;Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.&#10;&quot;&quot;&quot;&#10;print(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))&#10;&gt;&gt;&gt; [{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name &quot;Hugging Face&quot; was chosen to reflect the company\'s mission of making AI models more accessible and friendly to humans .'}]&#10;```&#10;Other transformers like ```Falconsai/text_summarization``` to use is the ```facebook/bart-large-cnn``` for text summarization.&#10;&#10;finally, you can chain together multiple objects for example first do a text summarization and then do a sentiment analysis. &#10;Another interesting task is conversational text. For this we can use the ```facebook/blenderbot-400M-distill```. There is supposed to be a class called ```Conversation``` (also imported from transformers) which is supposed to be a container for conversation. &#10;```python&#10;from transformers import pipeline&#10;&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;conversation_history = &quot;Hello, how are you?&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;&#10;# Continue the conversation&#10;conversation_history += f&quot; {response[0]['generated_text']}&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;```&#10;There is a library called **Gradio** to make it conversational. Gradio is very similar to streamlit. &#10;```python&#10;import gradio as gr&#10;from transformers import pipeline&#10;&#10;# Load the chatbot model&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;&#10;# Function to handle chatbot conversation&#10;def respond(user_input, history=[]):&#10;    # Add the user input to the conversation history&#10;    history = history or []&#10;    history.append(f&quot;User: {user_input}&quot;)&#10;    print(history)&#10;    # Generate a response&#10;    response = chatbot(user_input)&#10;    bot_reply = response[0]['generated_text']&#10;    print(bot_reply)&#10;&#10;    # Add the bot reply to the history&#10;    history.append(f&quot;Bot: {bot_reply}&quot;)&#10;    &#10;    # Return the entire conversation history as a string&#10;    return &quot;\n&quot;.join(history), history&#10;&#10;# Create the Gradio interface&#10;demo = gr.Interface(&#10;    fn=respond,  # The function that processes input&#10;    inputs=[gr.Textbox(label=&quot;Your Message here:&quot;), gr.State([])],  # Input is a message and conversation history&#10;    outputs=[gr.Textbox(label=&quot;Response here:&quot;), gr.State([])],  # Output is updated conversation and history&#10;    title=&quot;AI Chatbot&quot;&#10;)&#10;&#10;# Launch the interface&#10;demo.launch()&#10;```&#10;![](/blogs/img/posts/gradio-initial.png)&#10;you can post this in hugging face spaces or [hf.co/spaces](hf.co/spaces). They allow to create ML applications and host it here.&#10;example of this [Llama chatbot](https://huggingface.co/spaces/huggingface-projects/llama-3.2-vision-11B).&#10;* Go to hf.co/spaces and click on create new space.&#10;* follow the instructions to clone the repo and push your code.&#10;&#10;# Prompt Engineering&#10;## What is Prompt Engineering&#10;Prompt engineering refers to the process of designing and refining the input (or &quot;prompt&quot;) given to an AI language model, like GPT, to produce desired outputs. It's kind of the future of computer programming in Natural Language. Language models are not designed to peform a task, all that it does is to predict the next token, thus you can trick the model into solving your problem.&#10;Example of a prompt:&#10;```&#10;---&#10;&#10;**Prompt:**&#10;&#10;You are an intelligent system that processes natural language queries and selects the most relevant SQL query from a given list. Based on the user's question, match the correct SQL query that will retrieve the desired information from a database.&#10;&#10;**Input:**&#10;&#10;- **User Query (NLP):** The user asks a question in natural language, describing the data they want from the database.&#10;- **SQL Queries List:** A list of SQL queries is provided as possible answers.&#10;&#10;**Task:**&#10;&#10;- Analyze the user's natural language question.&#10;- Select the most appropriate SQL query from the list that best answers the user's question.&#10;&#10;**Example:**&#10;&#10;- **User Query:** &quot;What are the names and email addresses of all customers who made a purchase in the last 30 days?&quot;&#10;- **SQL Queries List:**&#10;    1. `SELECT * FROM customers WHERE purchase_date &gt; '2023-09-01';`&#10;    2. `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;    3. `SELECT id, name FROM orders WHERE status = 'complete';`&#10;    4. `SELECT email FROM customers WHERE created_at &gt; NOW() - INTERVAL 1 YEAR;`&#10;&#10;**Expected Output:**&#10;&#10;- The system should select query 2: `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;```&#10;## Two ways of implementing Prompt Enginner&#10;* Easy way - using an Agent like ChatGPT. You can't really use it to integrate it into another app.&#10;* Programmatically integrate using python or similar.&#10;&#10;## 7 Tricks for prompt engineering&#10;1. Be Descriptive - give a context around the problem&#10;2. Give Examples&#10;3. Use Structured Text&#10;    ```&#10;    give me the recipe for making chocolate cookies, give it in the format&#10;    **Title**: Chocolate Cookie Recipe&#10;    **Description**: .......&#10;    ```&#10;5. Chain of Thoughts&#10;    ```&#10;    Make me a resume for a job application at Google.&#10;    Step 1: Write an objective&#10;    Step 2: Write an introduction about my overall work experience. they are...&#10;    Step 3: Write in detail each experience.&#10;    Step 4: Summary and conclusion.&#10;    ```&#10;6. Chatbot personas: &#10;    ```&#10;    Act as an travel guide who knows everything about Sydney. Make me a travel itenaryfor weekend in Sydney in your Aussie Accent.&#10;    ```&#10;7. Flipped Approach:&#10;    The generic response might not be of interest to you hence we have depend on a conversational model. This is useful when you dont know what exactly you want. e.g.&#10;    ```&#10;    I want you to ask me questions to help me come up with an LLM based application idea. Ask me one question at a time to keep things conversational..&#10;    ```&#10;8. Reflective, Review and Refine&#10;&#10;## ChatGPT v/s GPT3.0&#10;ChatGPT is a finetuned model - easy to get useful responses, however with GPT 3.0 that isn't the case and more work is to be done on prompt engineering side - it just does work prediction. &#10;&#10;## LangChain&#10;LangChain is a framework designed to help developers build applications that leverage language models (like GPT) more effectively by integrating them with other tools, data sources, and workflows. It simplifies the process of creating applications that combine various natural language processing tasks with external data, APIs, and user interactions.&#10;```shell&#10;pip install langchain&#10;pip install langchain-community langchain-core&#10;pip install huggingface_hub&#10;```&#10;&#10;```python&#10;from langchain import HuggingFaceHub # or use openai&#10;from langchain.prompts import PromptTemplate&#10;from langchain.chains import LLMChain&#10;import os&#10;&#10;os.environ['HUGGINGFACEHUB_API_TOKEN'] = '&lt;your hf token&gt;'&#10;hugging_face_llm = HuggingFaceHub(repo_id=&quot;google/flan-t5-base&quot;, model_kwargs={&quot;temperature&quot;: 0.5})&#10;&#10;prompt_template = PromptTemplate(&#10;    input_variables=[&quot;question&quot;],&#10;    template=&quot;&quot;&quot;You are the teacher, and you are running a surprise test to see who are the attentive kids. &#10;    The questions will be in the form &#10;    :\nQuestion: {question}&quot;&quot;&quot;&#10;)&#10;&#10;qa_chain = LLMChain(llm=hugging_face_llm, prompt=prompt_template)&#10;question = &quot;What is the capital of France?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;question = &quot;What is the name of indias PM?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;# Output&#10;# Paris&#10;# Narendra Modi&#10;```&#10;## Model fine Tuning&#10;A smaller fine tuned model can outperform a larger base model. This involves taking an existing or pre-trained model like GPT 3 for a specific usecase like ChatGPT (GPT-3.5-turbo).&#10;3 ways to fine tune:&#10;1. Self Supervised learning &#10;    - you get the Training Corpus of data can cater to your usecase.&#10;    - you then use this corpus of text and you train the model in a self supervise way.&#10;2. Supervised Learning &#10;    - here you have a set of inputs and outputs e.g. feed in who is the 35th president of the US? and output JFK.&#10;    - So having these question answer pairs we can train the model how to answer questions.&#10;    - One way of doing this is via prompt templates.&#10;      ```text&#10;        Please answer the following questions&#10;        Q: {Question}&#10;        A: {Answer}&#10;      ```&#10;    -  Through this process we could translate the training data set to a series of prompts and generate a training corpus and go back to the self supervised process.&#10;3. Reinforcement Learning -&#10;    - Supervised Fine tunning, two steps:&#10;        - curating your training dataset&#10;        - Fine tuning the model.&#10;        - done in 5 steps:&#10;          1. Choose fine tuning task. it could be anything e.g.&#10;              - could be text summarization&#10;              - could be text generation&#10;              - text/binary classification what ever you want to do..&#10;          2. Prepare training dataset.&#10;              - e.g. if text summarization then the input/output pairs of text in desired summarization generate a training corpus using for e.g. a prompt template &#10;          3. Choose a base model.&#10;              - lots of foundantal llms for e.g. or fine tuned llms.&#10;              - use this as the starting point.&#10;          4. Fine-tune model via supervised learning&#10;              - There are 3 different options:&#10;                  1. retrain all the parameters: here we tweak all the parameters, the computation cost is very very very high.&#10;                  2. transfer learning: here we freeze most of the parameters only fine tune the head. cheaper than full retaining all the parameters.&#10;                  3. Parameter Efficient Fine Tuning (PEFT): here we freeze all the weights or parameters. Instead we augment the model with additional parameters that are trainable. Advantage is we can fine tune the model with a relatively small set of model parameters as against the above approaches.&#10;                  - One of the ways to do this is LoRA (Low Rank Adaptation). In short fine tune model by adding new trainable parameters.&#10;                  ![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*GmCISYhd-JLqHNEvAQU1tQ.png)&#10;                  the first component here h(x) = Wox is what looks like a model without LoRA. Wo has weights that are all trainable. her Wo is a d by k (dxk) matrix with d*k trainable parameters. e.g. d=1000, k=1000 Wo is a 1,000,000 trainable parameters. But with LoRA the ΔWx, another weight matrix with the same shape as Wo. &#10;&#10;                  To simplify things lets just represent ΔW as a product of two terms B and A (BA) hence we can represent ΔW in terms of a 2 one dimentional arrays or vectors A and B, which essentially generates the new h(x).&#10;&#10;                  In this case Wox itself if frozen but B and A are trainable. hence in the above context d = 1000 and k = 1000 hence (d*r)+(r*k) for intrensic rank or r = 2 which translates into 4000 trainable parameters as against the million parameters.&#10;          5. Evaluate the model performance.&#10;    - Train Reward model&#10;        - generating a score for language models completetion. highscore for correct answer and low score for an incorrect answer.&#10;        - start with a prompt pass it into supervised fine tuned model. this you do multiple times and then assign human labels and then use the ranking to train the rewards model. &#10;    - Reinforcement learning with Favourite algorithm&#10;        - example in the case of ChatGPT it uses PPO or Proximal Policy Optimization&#10;        - you give the prompt and pass it into supervised fine tuned model and pass it back to reward model. The reward model then will give feedback to the finetuned model, this is how you update the model parameters. &#10;## Practical Supervised Fine tunning&#10;**First thing first**&#10;These are some of the classes and it's uses.&#10;| Class                                      | Description                                                             |&#10;|--------------------------------------------|-------------------------------------------------------------------------|&#10;| `AutoModel`                                | Automatically loads a pre-trained model for various tasks.             |&#10;| `AutoModelForSequenceClassification`      | Loads a model specifically for sequence classification tasks.           |&#10;| `AutoModelForTokenClassification`         | Loads a model for token classification tasks (e.g., NER).              |&#10;| `AutoModelForQuestionAnswering`           | Loads a model for question answering tasks.                             |&#10;| `AutoModelForCausalLM`                    | Loads a model for causal language modeling tasks (e.g., text generation).|&#10;| `AutoModelForMaskedLM`                    | Loads a model for masked language modeling tasks.                      |&#10;| `AutoModelForImageClassification`         | Loads a model for image classification tasks.                          |&#10;| `AutoTokenizer`                            | Automatically loads a tokenizer corresponding to a pre-trained model.  |&#10;| `AutoFeatureExtractor`                     | Loads a feature extractor for models that require image preprocessing.  |&#10;| `AutoConfig`                               | Loads configuration settings for a model.                              |&#10;| `AutoPipeline`                             | Automatically creates a pipeline for various tasks using a model.     |&#10;| `AutoModelForSpeechSeq2Seq`               | Loads a model for speech-to-text sequence generation tasks.            |&#10;| `AutoModelForAudioClassification`         | Loads a model for audio classification tasks.                          |&#10;| `AutoModelForSeq2SeqLM`                   | Loads a model for sequence-to-sequence tasks (e.g., translation).     |&#10;| `AutoModelForImageSegmentation`           | Loads a model for image segmentation tasks.                            |&#10;| `AutoModelForImageToText`                 | Loads a model for image-to-text generation tasks.                      |&#10;| `AutoModelForTextToImage`                 | Loads a model for text-to-image generation tasks.                      |&#10;| `AutoModelForTextClassification`          | A more general class for text classification tasks.                   |&#10;| `AutoModelForConversational`               | Loads a model designed for conversational tasks.                       |&#10;&#10;&#10;**ChatGPT generated code for Sentiment analysis and then Finetuning using LoRA.**&#10;you can upload your dataset into huggingface like in here.&#10;![](/blogs/img/posts/huggingface-dataset-shawhin.png)&#10;you can access it using &#10;&#10;```python&#10;import torch&#10;from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments&#10;from peft import get_peft_model, LoraConfig, TaskType&#10;&#10;# Load the tokenizer and base model&#10;model_name = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;&#10;tokenizer = AutoTokenizer.from_pretrained(model_name)&#10;model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)&#10;&#10;# Define LoRA configuration&#10;lora_config = LoraConfig(&#10;    r=4,                   # Intrinsic rank&#10;    lora_alpha=32,        # Scaling factor&#10;    lora_dropout=0.01,    # Dropout rate&#10;    target_modules=[&quot;q_lin&quot;],  # Target modules for LoRA&#10;    task_type=TaskType.SEQ_CLS&#10;)&#10;&#10;# Wrap the model with LoRA&#10;lora_model = get_peft_model(model, lora_config)&#10;&#10;# Example training data&#10;train_texts = [&#10;    &quot;It was good.&quot;,&#10;    &quot;Not a fan, don't recommend.&quot;,&#10;    &quot;Better than the first one.&quot;,&#10;    &quot;This is not worth watching even once.&quot;,&#10;    &quot;This one is a pass.&quot;&#10;]&#10;train_labels = [1, 0, 1, 0, 0]  # Example labels corresponding to the texts&#10;&#10;# Example evaluation data&#10;eval_texts = [&#10;    &quot;A fantastic experience!&quot;,&#10;    &quot;Horrible movie, would not watch again.&quot;,&#10;]&#10;eval_labels = [1, 0]  # Example labels for evaluation&#10;&#10;# Tokenize the training and evaluation data&#10;train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;&#10;# Create PyTorch datasets&#10;class SentimentDataset(torch.utils.data.Dataset):&#10;    def __init__(self, encodings, labels):&#10;        self.encodings = encodings&#10;        self.labels = labels&#10;&#10;    def __getitem__(self, idx):&#10;        item = {key: val[idx] for key, val in self.encodings.items()}&#10;        item['labels'] = torch.tensor(self.labels[idx])&#10;        return item&#10;&#10;    def __len__(self):&#10;        return len(self.labels)&#10;&#10;# Create datasets&#10;train_dataset = SentimentDataset(train_encodings, train_labels)&#10;eval_dataset = SentimentDataset(eval_encodings, eval_labels)&#10;&#10;# Define training arguments&#10;training_args = TrainingArguments(&#10;    output_dir=&quot;./lora_finetuned_model&quot;,&#10;    evaluation_strategy=&quot;epoch&quot;,&#10;    learning_rate=5e-5,&#10;    per_device_train_batch_size=2,&#10;    per_device_eval_batch_size=2,  # Add eval batch size&#10;    num_train_epochs=3,&#10;    weight_decay=0.01,&#10;)&#10;&#10;# Define Trainer&#10;trainer = Trainer(&#10;    model=lora_model,&#10;    args=training_args,&#10;    train_dataset=train_dataset,&#10;    eval_dataset=eval_dataset,  # Provide eval dataset&#10;)&#10;&#10;# Fine-tune the model&#10;trainer.train()&#10;&#10;# Save the fine-tuned model&#10;trainer.save_model(&quot;./lora_finetuned_model&quot;)&#10;&#10;&#10;eval_texts = [&#10;    &quot;An absolutely stunning film! The visuals were breathtaking, and the storyline kept me engaged the entire time.&quot;,&#10;    &quot;I was really disappointed with this film. The plot was weak and the characters were poorly developed.&quot;,&#10;    &quot;A heartwarming story that left me in tears. The performances were phenomenal and truly captured the essence of the characters.&quot;,&#10;    &quot;A boring and predictable movie that dragged on for too long. I wouldn't recommend it to anyone.&quot;,&#10;    &quot;This movie exceeded my expectations! The plot twists were fantastic, and the acting was top-notch. Highly recommend!&quot;,&#10;    &quot;The special effects couldn't save this film. It was a chore to sit through, and I found myself looking at my watch constantly.&quot;,&#10;    &quot;An inspiring tale that resonates on many levels. The direction was exceptional, and the soundtrack was unforgettable.&quot;,&#10;    &quot;An absolute disaster! The acting was cringe-worthy and the story made no sense whatsoever.&quot;,&#10;    &quot;A brilliant blend of action and comedy. I couldn't stop laughing, and the action scenes were exhilarating!&quot;,&#10;    &quot;I expected much more from this film. It felt like a cash grab with no real substance or originality.&quot;&#10;]&#10;&#10;# Get predictions&#10;predicted_sentiments = predict_sentiment(eval_texts)&#10;&#10;# Print the results&#10;for text, sentiment in zip(eval_texts, predicted_sentiments):&#10;    print(f&quot;Text: \&quot;{text}\&quot; - Sentiment: {sentiment}&quot;)&#10;```" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/content/posts/system_design/system_design.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/system_design/system_design.md" />
              <option name="originalContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'System Design Interview Concepts'&#10;tags = ['Event Driven Architecture', 'Microservices', 'Interview']&#10;+++&#10;&#10;System design is a critical aspect of software engineering that involves creating scalable, reliable, and efficient systems. This document explores key concepts and strategies required for designing robust systems, including networking, databases, scalability, caching, and modern architectural patterns. It serves as a comprehensive guide for understanding the foundational and advanced principles of system design.&#10;&#10;# Table of Contents&#10;&#10;- [Approach](#approach)&#10;- [HTTPS Certificates in System Design](#https-certificates-in-system-design)&#10;- [Networking and Communication](#networking-and-communication)&#10;  - [Client Server Architecture](#client-server-architecture)&#10;  - [IP Address](#ip-address)&#10;  - [DNS](#dns)&#10;  - [Proxy/Reverse Proxy](#proxyreverse-proxy)&#10;  - [Latency](#latency)&#10;  - [HTTP/HTTPS and MASL (Mutual Authentication Security Layers)](#httphttps-and-masl-mutual-authentication-security-layers)&#10;  - [WebSockets](#websockets)&#10;  - [Webhooks](#webhooks)&#10;- [APIs and Integration](#apis-and-integration)&#10;  - [APIs](#apis)&#10;  - [REST API](#rest-api)&#10;  - [GraphQL](#graphql)&#10;  - [API Gateway](#api-gateway)&#10;  - [Idempotency](#idempotency)&#10;  - [REST and REST Maturity Model](#rest-and-rest-maturity-model)&#10;- [Databases and Storage](#databases-and-storage)&#10;  - [Databases](#databases)&#10;  - [SQL vs NoSQL vs Object Store](#sql-vs-nosql-vs-object-store)&#10;  - [Database Indexing](#database-indexing)&#10;  - [Replication](#replication)&#10;  - [Sharding](#sharding)&#10;  - [Vertical Partitioning](#vertical-partitioning)&#10;  - [Caching](#caching)&#10;  - [ACID vs BASE](#acid-vs-base)&#10;  - [Normalization vs Denormalization](#normalization-vs-denormalization)&#10;  - [CAP Theorem](#cap-theorem)&#10;  - [Consistency Models: Linearizability vs Causal Consistency](#consistency-models-linearizability-vs-causal-consistency)&#10;  - [Blob Storage](#blob-storage)&#10;- [Scalability and Performance](#scalability-and-performance)&#10;  - [Vertical and Horizontal Scaling](#vertical-and-horizontal-scaling)&#10;  - [Algorithmic Scaling](#algorithmic-scaling)&#10;  - [Load Balancers](#load-balancers)&#10;  - [Rate Limiting](#rate-limiting)&#10;  - [Content Delivery Optimization](#content-delivery-optimization)&#10;  - [Zero Downtime Deployment](#zero-downtime-deployment)&#10;- [Common Design Patterns and Architecture](#common-design-patterns-and-architecture)&#10;  - [Event-Driven Architecture](#event-driven-architecture)&#10;  - [Data Partitioning Strategies](#data-partitioning-strategies)&#10;  - [Eventual Consistency](#eventual-consistency)&#10;  - [Leader Election](#leader-election)&#10;  - [Circuit Breaker Pattern](#circuit-breaker-pattern)&#10;  - [Throttling and Backpressure](#throttling-and-backpressure)&#10;  - [Service Discovery](#service-discovery)&#10;  - [Microservices](#microservices)&#10;  - [Message Queues](#message-queues)&#10;- [Monitoring, Resiliency, and Security](#monitoring-resiliency-and-security)&#10;  - [Monitoring and Observability](#monitoring-and-observability)&#10;  - [Data Compression](#data-compression)&#10;  - [Authentication and Authorization](#authentication-and-authorization)&#10;  - [Data Backup and Recovery](#data-backup-and-recovery)&#10;  - [Chaos Engineering](#chaos-engineering)&#10;- [Development and Deployment](#development-and-deployment)&#10;  - [Concurrency Control](#concurrency-control)&#10;  - [Immutable Infrastructure](#immutable-infrastructure)&#10;  - [Blue-Green Deployment](#blue-green-deployment)&#10;- [Theoretical Concepts](#theoretical-concepts)&#10;  - [Search Systems](#search-systems)&#10;- [Data Processing](#data-processing)&#10;  - [Data Streaming](#data-streaming)&#10;- [Miscellaneous](#miscellaneous)&#10;  - [Rate Shaping](#rate-shaping)&#10;&#10;## Approach&#10;- Do not directly start designing; every problem is unique. Think of every problem as designing and building a bridge. You must understand:&#10;  - **Whom or What you are building for**  &#10;    - target users or audience and their count.  &#10;    - Expected traffic or load  &#10;    - User behavior e.g. Celebrity Problem &#10;    - Interaction patterns.  &#10;    - Account for user demographics and geographic distribution.  &#10;    - Evaluate the specific needs or goals of the users.&#10;  - **Where you are building** (the environment or constraints).&#10;  - **How you build**.&#10;&#10;## Steps:&#10;- **Understand your use case**: Clearly define the problem and its requirements.&#10;- **Ask the right questions**: Gather Necessary details, NO assumptions.&#10;- **Decide the modules**: Break the big problem into smaller, manageable parts (e.g., defining context boundaries). Ask which they want you to tackle first.&#10;- **Design with key considerations**:  Address other critical &quot;ilities&quot;&#10;     - **Availability**&#10;     - **Reliability**&#10;     - **Maintainability**&#10;     - **Performance**&#10;     - **Scalability**&#10;     - **Cost-efficiency**&#10;     - **Security**&#10;     - **Flexibility**&#10;     (Always remind yourself - everything fails)&#10;---&#10;## HTTPS Certificates in System Design&#10;&#10;In system design, **HTTPS certificates** are essential for securing communication between clients (e.g., web browsers, mobile apps) and servers. They are used to encrypt data, verify server identity, and ensure secure communication channels in modern web and microservice architectures.&#10;&#10;---&#10;&#10;## Key Points about HTTPS Certificates&#10;&#10;### 1. **What is an HTTPS Certificate?**&#10;- **HTTPS certificates** are **X.509 certificates** used in **TLS (Transport Layer Security)** for encrypting and securing HTTP traffic.&#10;- They contain the **public key** of a server, and are signed by a trusted **Certificate Authority (CA)** to confirm the authenticity of the server.&#10;&#10;### 2. **Role in HTTPS Communication:**&#10;- **Encryption**: HTTPS certificates use **TLS** to encrypt data in transit, ensuring that information exchanged between client and server is private and secure.&#10;- **Authentication**: The certificate proves the server's identity, assuring clients that they are communicating with the correct, trusted server.&#10;- **Data Integrity**: It ensures that data cannot be tampered with while in transit.&#10;&#10;---&#10;&#10;## HTTPS Flow in System Design&#10;&#10;1. **DNS Resolution** → The domain name (e.g., `example.com`) is resolved to an IP address.&#10;2. **TCP Handshake** → A 3-way handshake is established between the client and server.&#10;3. **TLS Handshake**:&#10;   - The client requests a secure connection and receives the server's certificate.&#10;   - The client verifies the certificate's authenticity (checking the CA and validity period).&#10;   - The client and server exchange keys to encrypt further communication.&#10;4. **Secure Communication**: The HTTP request and response occur over the encrypted TLS channel.&#10;5. **Connection Termination**: Once communication is complete, the connection is securely closed.&#10;&#10;---&#10;&#10;## HTTPS Certificate Components&#10;&#10;1. **Public Key**: Used for encryption and establishing a secure connection.&#10;2. **Issuer**: The Certificate Authority (CA) that issued the certificate.&#10;3. **Subject**: The entity (e.g., website, server) being identified by the certificate.&#10;4. **Validity Period**: The certificate’s expiration date.&#10;5. **Signature**: A digital signature from the CA, ensuring the certificate's authenticity.&#10;6. **Extensions**: Additional metadata, such as **Subject Alternative Names (SANs)**, which allow a single certificate to cover multiple domains.&#10;&#10;---&#10;&#10;## Use Cases in System Design&#10;&#10;### 1. **Web Applications**&#10;- HTTPS certificates are used to secure user data, such as login credentials and payment details, during transmission between the browser and server.&#10;- **SSL/TLS** ensures that users can trust the site and prevents **man-in-the-middle attacks**.&#10;&#10;### 2. **API Security**&#10;- APIs use HTTPS certificates to secure communication between clients and services, ensuring that data transmitted between services is encrypted and authenticated.&#10;- **API Gateways** often enforce HTTPS for all incoming and outgoing traffic to secure internal and external communications.&#10;&#10;### 3. **Microservices Communication**&#10;- In microservices architectures, services communicate securely using **TLS** certificates.&#10;- Certificates can be used with **mTLS (Mutual TLS)**, where both the client and the server authenticate each other.&#10;- This is common for ensuring trust between services within a **private network**.&#10;&#10;### 4. **Certificate Pinning**&#10;- To prevent attacks, some systems implement **certificate pinning** to ensure that only a specific, trusted certificate can be used, even if it’s issued by a trusted CA.&#10;&#10;---&#10;&#10;## Design Considerations for HTTPS Certificates&#10;&#10;### 1. **Certificate Management:**&#10;   - **Renewal**: Certificates must be renewed periodically (typically every 1-2 years).&#10;   - **Revocation**: Certificates must be revoked if compromised, and Certificate Revocation Lists (CRLs) or **OCSP (Online Certificate Status Protocol)** can be used to check the certificate status.&#10;&#10;### 2. **Load Balancers and API Gateways:**&#10;   - **SSL Termination**: In many architectures, HTTPS connections are terminated at a **load balancer** or **API Gateway**. This means the secure connection between the client and the gateway is decrypted, and the communication between services may continue over plain HTTP or encrypted further.&#10;&#10;### 3. **Security Best Practices:**&#10;   - Use strong encryption algorithms (e.g., **TLS 1.2 or 1.3**).&#10;   - **Perfect Forward Secrecy (PFS)** should be enabled to ensure that past sessions are not compromised even if the server's private key is leaked.&#10;   - Regularly update certificates and private keys.&#10;   - Store private keys securely and limit access.&#10;&#10;---&#10;&#10;## Key Takeaways for System Design&#10;&#10;- **HTTPS certificates** are crucial for securing **web traffic** and **API communications** in modern system architectures.&#10;- They ensure **confidentiality**, **integrity**, and **authentication** between clients and servers.&#10;- Proper **certificate management** (renewal, revocation, etc.) is key for maintaining security.&#10;- **SSL/TLS termination** at **API Gateways** or **load balancers** can simplify management but must be carefully designed to ensure traffic is encrypted when needed.&#10;- **mTLS** can be used for mutual authentication between services, adding an additional layer of security in microservices architectures.&#10;&#10;&#10;## Networking and Communication&#10;### Client Server Architecture&#10;### IP Address&#10;### DNS&#10;### Proxy/Reverse Proxy&#10;### Latency&#10;### HTTP/HTTPS and MASL (Mutual Authentication Security Layers)&#10;&#10;##  1. HTTP / HTTPS in System Design&#10;&#10;###  HTTP&#10;- Stateless protocol for transferring hypertext and media between client and server.&#10;- Operates over **TCP** (usually port **80**).&#10;- Requests consist of **methods** (GET, POST, PUT, DELETE), **headers**, and optionally a **body**.&#10;- No built-in encryption → data sent in plaintext.&#10;&#10;###  HTTPS&#10;- HTTP over **TLS (Transport Layer Security)** → operates on port **443**.&#10;&#10;**Provides:**&#10;-  **Confidentiality:** Encrypts data.&#10;- ️ **Integrity:** Detects tampering.&#10;-  **Authentication:** Validates server identity via SSL/TLS certificates.&#10;&#10;**Used In:**&#10;- Web apps  &#10;- REST APIs  &#10;- Microservices communication  &#10;- IoT and mobile devices  &#10;&#10;##  Details of handshake &#10;&lt;img width=&quot;3000&quot; height=&quot;3336&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/2e8bf550-4932-4c84-a2ca-b3034b57c1f8&quot; /&gt;&#10;&#10;### 1. Client Hello&#10;The client initiates the handshake by sending:&#10;- A list of supported **cipher suites** (algorithms for encryption, key exchange, etc.).&#10;- A **`client_random`** value — a 32-byte random number.&#10;&#10;####  Why `client_random`?&#10;- Adds **entropy** to the key derivation process.&#10;- Ensures each session is **unique**, even if the same algorithms are used.&#10;- Helps prevent **replay attacks** by making the handshake unpredictable.&#10;&#10;### 2. Server Hello + Certificate&#10;The server responds with:&#10;- A selected **cipher suite** from the client's list.&#10;- Its own **`server_random`** value.&#10;- A **digital certificate** (usually X.509) containing its public key and identity.&#10;&#10;####  Why the Certificate?&#10;- Allows the client to **authenticate** the server.&#10;- The client checks:&#10;  - Is the certificate signed by a trusted Certificate Authority (CA)?&#10;  - Is it still valid (not expired)?&#10;  - Does the domain match?&#10;&#10;### 3. Key Exchange and Session Key Derivation&#10;Depending on the chosen cipher suite (e.g., ECDHE), the client and server:&#10;- Exchange **ephemeral public keys**.&#10;- Each side uses its private key and the other’s public key to compute a **shared secret**.&#10;- They use a **Key Derivation Function (KDF)** to combine:&#10;  - The shared secret&#10;  - `client_random`&#10;  - `server_random`&#10;  → to derive **symmetric session keys**.&#10;####  How the Shared Secret Is Computed in ECDHE&#10;&#10; Key Generation&#10;**Client generates:**&#10;&#10;- Private key: a&#10;- Public key: A = aG&#10;**Server generates:**&#10;&#10;- Private key: b&#10;- Public key: B = bG&#10;Here, G is a known base point on the elliptic curve.&#10;- **Client computes**:  &#10;  `S = a * B = a * (bG) = abG`&#10;- **Server computes**:  &#10;  `S = b * A = b * (aG) = abG`&#10;✅ Both arrive at the **same shared secret** `S = abG`.&#10;&#10;####  Why Ephemeral Keys (ECDHE)?&#10;- Provides **forward secrecy**: even if long-term keys are compromised, past sessions remain secure.&#10;- Ensures that each session has **unique encryption keys**.&#10;&#10;### 4. Finished Messages&#10;- Both sides send encrypted &quot;Finished&quot; messages to confirm that the handshake was successful.&#10;- From this point on, all communication is encrypted using the derived session keys.&#10;&#10;---&#10;&#10;###  Summary of Key Components&#10;&#10;| Component        | Purpose                                      |&#10;|------------------|----------------------------------------------|&#10;| `client_random`  | Adds entropy, uniqueness, and prevents replay attacks |&#10;| `server_random`  | Same as above, from the server side          |&#10;| Certificate      | Authenticates the server (and optionally the client) |&#10;| Ephemeral Keys   | Used to compute a shared secret securely     |&#10;| Session Keys     | Encrypt and authenticate all further communication |&#10;&#10;&#10;---&#10;&#10;##  What is Mutual Authentication (mTLS)?&#10;&#10;###  Common Use Cases&#10;&#10;| Use Case                      | Why Use mTLS?                                     |&#10;|------------------------------|---------------------------------------------------|&#10;| Service-to-service (microservices) | Ensure only trusted services communicate          |&#10;| APIs for fintech / healthcare | Regulatory compliance (HIPAA, PCI-DSS)            |&#10;| IoT Devices ↔ Cloud          | Authenticate individual devices securely          |&#10;| Enterprise internal apps     | Add trust within a private/internal network       |&#10;&#10;##  Details of handshake &#10;&lt;img width=&quot;1452&quot; height=&quot;2322&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/93556c5e-3e0b-42a2-90e0-aae7c0ad1da6&quot; /&gt;&#10;&#10;### 1. Client Hello&#10;- Client sends supported cipher suites and `client_random`.&#10;&#10;### 2. Server Hello + Certificate&#10;- Server responds with:&#10;  - Chosen cipher suite&#10;  - `server_random`&#10;  - Server certificate&#10;&#10;### 3. Server Requests Client Certificate&#10;- Server sends a `CertificateRequest` message.&#10;&#10;### 4. Client Sends Certificate&#10;- Client sends its certificate for authentication.&#10;&#10;### 5. Key Exchange&#10;- Both sides exchange ephemeral public keys (e.g., via ECDHE).&#10;- Each side computes the shared secret using its private key and the other’s public key.&#10;&#10;### 6. Certificate Verification&#10;- Server verifies the client’s certificate.&#10;- Client verifies the server’s certificate.&#10;&#10;### 7. Finished Messages&#10;- Both sides send encrypted &quot;Finished&quot; messages.&#10;- Secure communication begins using derived session keys.&#10;&#10;&#10;&#10;### WebSockets&#10;---&#10;### Webhooks&#10;---&#10;## APIs and Integration&#10;### APIs&#10;---&#10;### REST API&#10;---&#10;### GraphQL&#10;---&#10;### API Gateway&#10;---&#10;### Idempotency&#10;&#10;## REST and REST Maturity Model&#10;### What is REST?&#10;REST (Representational State Transfer) is an architectural style for designing networked applications. It relies on stateless communication and standard HTTP methods to enable interaction between clients and servers. RESTful APIs are widely used for their simplicity, scalability, and compatibility with web standards.&#10;&#10;Key principles of REST include:&#10;- **Statelessness**: Each request from a client to a server must contain all the information needed to understand and process the request.&#10;- **Client-Server Architecture**: Separation of concerns between the client and server, allowing them to evolve independently.&#10;- **Uniform Interface**: A consistent and standardized way of interacting with resources.&#10;- **Resource-Based**: Resources are identified using URIs (Uniform Resource Identifiers).&#10;- **Cacheability**: Responses must define whether they are cacheable to improve performance.&#10;- **Layered System**: The architecture can have multiple layers, such as load balancers and proxies, to improve scalability and security.&#10;&#10;### REST Maturity Model&#10;The REST Maturity Model, introduced by Leonard Richardson, defines levels of maturity for RESTful APIs. It helps evaluate how closely an API adheres to REST principles.&#10;&#10;![](../REST.png)&#10;&#10;#### Level 0: The Swamp of POX&#10;- APIs at this level use a single URI and rely on HTTP POST for all interactions.&#10;- They often resemble RPC (Remote Procedure Call) or SOAP (Simple Object Access Protocol).&#10;&#10;#### Level 1: Resources&#10;- Introduces the concept of resources, each identified by a unique URI.&#10;- HTTP methods are not yet fully utilized.&#10;&#10;#### Level 2: HTTP Verbs&#10;- Uses standard HTTP methods (GET, POST, PUT, DELETE, etc.) to perform operations on resources.&#10;- Improves clarity and aligns with REST principles.&#10;&#10;#### Level 3: Hypermedia Controls (HATEOAS)&#10;- Hypermedia as the Engine of Application State (HATEOAS) is implemented.&#10;- Clients can navigate the API dynamically using links provided in responses.&#10;- This level achieves full REST maturity by enabling discoverability and self-documentation.&#10;&#10;### Benefits of REST Maturity&#10;- **Scalability**: Higher levels of maturity improve scalability by leveraging HTTP standards.&#10;- **Interoperability**: Adherence to REST principles ensures compatibility across different platforms.&#10;- **Maintainability**: A well-designed RESTful API is easier to maintain and extend.&#10;- **Discoverability**: HATEOAS enables clients to discover available actions dynamically.&#10;&#10;By understanding and applying the REST Maturity Model, developers can design APIs that are robust, scalable, and aligned with modern web standards.&#10;&#10;---&#10;## Databases and Storage&#10;### Databases&#10;---&#10;### SQL vs NoSQL vs Object Store&#10;&#10;When designing a system, choosing the right data storage solution is crucial. The three main categories are **SQL databases**, **NoSQL databases**, and **Object Stores**. Each serves different use cases and has unique characteristics.&#10;&#10;#### SQL Databases (Relational Databases)&#10;&#10;- **Examples:** MySQL, PostgreSQL, Oracle, Microsoft SQL Server&#10;- **Data Model:** Structured, tabular data with predefined schemas (tables, rows, columns)&#10;- **Query Language:** SQL (Structured Query Language)&#10;- **Transactions:** Strong ACID guarantees (Atomicity, Consistency, Isolation, Durability)&#10;- **Use Cases:** Applications requiring complex queries, joins, and strong consistency (e.g., banking, ERP, CRM)&#10;- **Strengths:** Data integrity, complex querying, relationships, mature ecosystem&#10;- **Limitations:** Vertical scaling, rigid schema, less suited for unstructured or rapidly evolving data&#10;&#10;#### NoSQL Databases&#10;&#10;- **Examples:** MongoDB (Document), Cassandra (Wide-column), Redis (Key-Value), Neo4j (Graph)&#10;- **Data Model:** Flexible, can be document, key-value, column-family, or graph-based&#10;- **Schema:** Schema-less or dynamic schemas; can handle semi-structured or unstructured data&#10;- **Transactions:** Typically BASE properties (Basically Available, Soft state, Eventually consistent)&#10;- **Use Cases:** High scalability, large volumes of diverse data, real-time analytics, IoT, social networks&#10;- **Strengths:** Horizontal scaling, flexible data models, high write/read throughput&#10;- **Limitations:** Weaker consistency (eventual consistency), limited support for complex joins, less mature tooling&#10;&#10;#### Object Store&#10;&#10;- **Examples:** Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO&#10;- **Data Model:** Stores data as objects (blobs) with metadata and a unique identifier; no schema or tables&#10;- **Access:** Accessed via APIs (REST, SDKs); not a database—optimized for storing and retrieving large files&#10;- **Transactions:** No ACID/BASE guarantees; eventual consistency for some operations&#10;- **Use Cases:** Storing unstructured data (images, videos, backups, logs, large files), data lakes, static website hosting&#10;- **Strengths:** Virtually unlimited scalability, low cost for large data, durability, global access&#10;- **Limitations:** Not suitable for transactional data or complex queries; eventual consistency; slower for small, frequent reads/writes&#10;&#10;#### Comparison Table&#10;&#10;| Feature         | SQL (Relational)         | NoSQL                    | Object Store                |&#10;|-----------------|-------------------------|--------------------------|-----------------------------|&#10;| **Data Model**  | Tables (rows/columns)   | Flexible (JSON, KV, etc) | Objects (blobs + metadata)  |&#10;| **Schema**      | Fixed                   | Dynamic/Schema-less      | None                        |&#10;| **Query**       | SQL                     | Varies (NoSQL APIs)      | API (REST/S3)               |&#10;| **Transactions**| ACID                    | BASE (usually)           | None                        |&#10;| **Scaling**     | Vertical (mostly)       | Horizontal               | Horizontal                  |&#10;| **Best For**    | Structured, relational  | Semi/unstructured, scale | Unstructured, large files   |&#10;| **Examples**    | MySQL, PostgreSQL       | MongoDB, Cassandra       | S3, GCS, Azure Blob         |&#10;&#10;**Summary:**  &#10;- Use **SQL** for structured data and strong consistency.&#10;- Use **NoSQL** for flexible, scalable, high-throughput needs.&#10;- Use **Object Store** for unstructured, large-scale file storage—not as a database.&#10;&#10;---&#10;### Database Indexing&#10;---&#10;### Replication&#10;---&#10;### Sharding&#10;---&#10;### Vertical Partitioning&#10;---&#10;### Caching&#10;&#10;Caching is the process of storing frequently accessed data in a temporary storage layer to improve system performance and reduce latency. &#10;&#10;#### Advantages of Caching&#10;&#10;1. **Improved Performance**: Reduces response time by serving data from faster storage layers (e.g., memory).  &#10;&#10;2. **Reduced Latency**: Minimizes delays in data retrieval, enhancing user experience.  &#10;&#10;3. **Lower Database Load**: Decreases the number of direct queries to the database, reducing resource usage.  &#10;&#10;4. **Scalability**: Helps handle increased traffic by offloading requests from the primary data source.  &#10;&#10;5. **Cost Efficiency**: Reduces operational costs by optimizing resource utilization.  &#10;&#10;#### Disadvantages of Caching&#10;&#10;1. **Data Staleness**: Cached data may become outdated if not properly invalidated or refreshed.  &#10;&#10;2. **Complexity**: Implementing and managing caching layers adds complexity to the system.  &#10;&#10;3. **Cache Misses**: If data is not found in the cache, it can lead to slower performance as the system falls back to the original data source.  &#10;&#10;4. **Memory Overhead**: Caching requires additional memory, which can increase infrastructure costs.  &#10;&#10;5. **Consistency Challenges**: Ensuring data consistency between the cache and the source of truth can be difficult.  &#10;&#10;#### Strategies&#10;&#10;1. **Cache-Aside**: The application checks the cache first. If the data is not found, it fetches from the database and updates the cache. Commonly used for read-heavy workloads.  &#10;&#10;2. **Write-Through**: Data is written to the cache and the database simultaneously. Ensures data consistency but may introduce higher write latency.  &#10;&#10;3. **Write-Behind**: Data is written to the cache first and asynchronously updated in the database. Improves write performance but risks data loss during failures.  &#10;&#10;4. **Read-Through**: The application interacts only with the cache. If the data is not in the cache, the cache fetches it from the database. Simplifies application logic but adds complexity to the caching layer.  &#10;&#10;#### Measuring Cache Effectiveness&#10;&#10;1. **Calculate the Cache Hit Rate**: Measure the percentage of requests served from the cache versus the total requests. A high hit rate indicates effective caching.  &#10;&#10;2. **Analyze Cache Eviction Rate**: Monitor how often data is evicted from the cache due to capacity limits. Optimize cache size and eviction policies to reduce unnecessary evictions.  &#10;&#10;3. **Monitor Data Consistency**: Ensure that cached data remains consistent with the source of truth (e.g., database). Use appropriate invalidation and expiration mechanisms.  &#10;&#10;4. **Determine the Right Cache Expiration Time**: Set expiration times based on data usage patterns and freshness requirements. Avoid stale data while minimizing unnecessary cache misses.&#10;&#10;#### Example Use Cases for Caching&#10;&#10;1. **URL Shortener**: Cache `ShortCode → URL` mappings. Strategy: LRU for frequently accessed URLs.&#10;&#10;2. **User Profile Service**: Cache user profiles with TTL for frequent reads. Challenge: Cache invalidation and consistency.&#10;&#10;3. **Weather Forecast API**: Cache responses based on `city+date`. Set TTL based on forecast freshness.&#10;&#10;4. **Rate Limiter Service**: Cache token bucket or sliding window counters per user. Use Redis or in-memory store with expiration.&#10;&#10;5. **Product Catalog**: Cache product details at edge/CDN. Strategy: Write-through or refresh-on-write.&#10;&#10;6. **Twitter Feed**: Cache user timelines and precompute recent tweets. Eviction policy: LRU or LFU.&#10;&#10;7. **Geolocation Service**: Cache frequently accessed IP ranges. Use TTL for DNS/IP lookups.&#10;&#10;8. **Session Management**: Store sessions in Redis with TTL. Trade-off: In-memory vs database storage.&#10;&#10;9. **Distributed Cache System**: Handle replication vs partitioning. Prevent hot keys and cache stampede.&#10;&#10;10. **Online Code Editor**: Cache user preferences and recent submissions. Use client-side and server-side caching.&#10;&#10;#### References&#10;&#10;- [Cache Strategies - Medium](https://medium.com/@mmoshikoo/cache-strategies-996e91c80303)&#10;&#10;---&#10;### ACID vs BASE&#10;&#10;#### ACID Properties&#10;ACID stands for Atomicity, Consistency, Isolation, and Durability. These properties are essential for traditional relational databases to ensure reliable transactions:&#10;- **Atomicity**: Ensures that a transaction is all-or-nothing. If one part fails, the entire transaction is rolled back.&#10;- **Consistency**: Guarantees that a transaction brings the database from one valid state to another, maintaining all defined rules.Always preserve the data integrity.&#10;- **Isolation**: Ensures that concurrent transactions do not interfere with each other. Don't step on each other shoes. The various problems&#10;&#10;  | Isolation Level      | Dirty Reads | Non-Repeatable Reads | Phantom Reads | Description |&#10;  |----------------------|-------------|-----------------------|----------------|-------------|&#10;  | **Read Uncommitted** | ✅ Allowed  | ✅ Allowed            | ✅ Allowed     | Minimal isolation, allows all anomalies. |&#10;  | **Read Committed**   | ❌ Prevented| ✅ Allowed            | ✅ Allowed     | Only committed data is visible. Default in many databases. |&#10;  | **Repeatable Read**  | ❌ Prevented| ❌ Prevented          | ✅ Allowed     | Rows cannot change, but new rows may appear (phantoms). |&#10;  | **Serializable**     | ❌ Prevented| ❌ Prevented          | ❌ Prevented   | Full isolation, transactions execute as if sequentially. |&#10;&#10;  - **Dirty Read**: Transaction reads data written by another uncommitted transaction.Example: T1 reads a value updated by T2, but T2 hasn't committed. **Solution**: Use `Read Committed` or higher.&#10;  &#10;  - **Non-Repeatable Read**: A row is read twice and returns different data due to an update by another transaction. T1 reads a row, T2 updates and commits it, T1 reads again and gets different data. **Solution**: Use `Repeatable Read` or `Serializable`.&#10;&#10;  - **Phantom Read**: A query returns a different set of rows when re-executed because another transaction inserted/deleted matching rows.Example: T1 runs a query with a condition; T2 inserts a new matching row; T1 reruns and sees new row. **Solution**: Use `Serializable`, or databases supporting MVCC (like PostgreSQL or Oracle).&#10;&#10;- **Durability**: Once a transaction is committed, it remains so, even in the event of a system failure.&#10;### Distributed Transaction Protocols &amp; Patterns&#10;&#10;#### 1. Two-Phase Commit (2PC)&#10;&#10;**Goal:**  &#10;Ensure all participants in a distributed transaction either all commit or all roll back.&#10;&#10;**Roles:**&#10;- **Coordinator** — orchestrates the commit.&#10;- **Participants (Resource Managers)** — e.g., databases, queues.&#10;&#10;**Phases:**&#10;1. **Prepare phase**  &#10;   - Coordinator → Participants: *&quot;Can you commit?&quot;*  &#10;   - Participants:  &#10;     - Validate transaction feasibility (constraints, locks).  &#10;     - If OK → reply **YES** (and lock resources so they can commit later).  &#10;     - If not OK → reply **NO**.&#10;2. **Commit/Abort phase**  &#10;   - If **all YES** → Coordinator sends **COMMIT** to all.  &#10;   - If **any NO** → Coordinator sends **ROLLBACK** to all.&#10;&#10;**Pros:**&#10;- Strong consistency.&#10;- Simple to reason about.&#10;&#10;**Cons:**&#10;- **Blocking** — If coordinator crashes after prepare but before commit, participants wait indefinitely.&#10;- Locks held across prepare → commit can hurt performance.&#10;&#10;---&#10;&#10;#### 2. Three-Phase Commit (3PC)&#10;&#10;**Goal:**  &#10;Reduce 2PC blocking by adding a pre-commit phase.&#10;&#10;**Phases:**&#10;1. **Can Commit**  &#10;   - Same as 2PC’s prepare phase — ask if ready.&#10;2. **Pre-Commit**  &#10;   - If all **YES**: Coordinator sends **PRE-COMMIT** to participants.  &#10;   - Participants acknowledge, enter a state where they can commit without coordinator.&#10;3. **Do Commit**  &#10;   - Coordinator sends **COMMIT**.  &#10;   - If coordinator fails, participants can still commit safely after a timeout (based on pre-commit state).&#10;&#10;**Pros:**&#10;- Less blocking than 2PC.&#10;- Participants can make progress after coordinator failure.&#10;&#10;**Cons:**&#10;- Requires synchronous clocks and reliable network assumptions (rare in real-world WAN).&#10;- More message overhead.&#10;&#10;---&#10;&#10;#### 3. XA Transactions&#10;&#10;**Goal:**  &#10;Provide a standard API for distributed transactions across multiple resource managers.&#10;&#10;**Key Points:**&#10;- Defined by **X/Open XA** spec.&#10;- Involves:&#10;  - **Application** — business logic.&#10;  - **Transaction Manager (TM)** — controls transaction boundaries.&#10;  - **Resource Managers (RM)** — e.g., databases, message brokers.&#10;&#10;**Flow:**&#10;1. Application starts transaction (via TM).&#10;2. Application interacts with multiple RMs.&#10;3. TM calls RMs using XA API to prepare/commit.&#10;4. Under the hood, TM uses **2PC** protocol (almost always).&#10;&#10;**Important:**  &#10;XA is **not** a commit algorithm — it’s a coordination API/spec. But in practice, **XA + 2PC** is the norm.&#10;&#10;#### BASE Properties&#10;BASE stands for Basically Available, Soft state, and Eventually consistent. These properties are common in distributed systems and NoSQL databases:&#10;- **Basically Available**: The system guarantees availability, even in the presence of partial failures.&#10;- **Soft State**: The state of the system may change over time, even without input, due to eventual consistency.&#10;- **Eventually Consistent**: The system will become consistent over time, given that no new updates are made.&#10;&#10;&#10;| Feature                | ACID                                                                                     | BASE                                                                                     |&#10;|------------------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|&#10;| **Definition**         | Ensures reliable transactions with strong consistency and integrity.                     | Focuses on availability and eventual consistency in distributed systems.                 |&#10;| **Consistency**        | Strong consistency; the database is always in a valid state after a transaction.         | Eventual consistency; the system becomes consistent over time.                          |&#10;| **Availability**       | May sacrifice availability for consistency.                                              | Prioritizes availability, even during partial failures.                                  |&#10;| **Data Integrity**     | High data integrity; suitable for critical applications like banking.                    | Lower data integrity; suitable for scalable systems like social media.                  |&#10;| **Transaction Model**  | Transactions are all-or-nothing (atomic).                                                | Transactions may be partial or eventual.                                                |&#10;| **Use Case**           | Ideal for OLTP systems requiring strict data accuracy.                                   | Ideal for distributed systems requiring high scalability and availability.              |&#10;| **Examples**           | Relational databases like MySQL, PostgreSQL.                                             | NoSQL databases like Cassandra, DynamoDB.                                               |&#10;&#10;---&#10;### Normalization vs Denormalization&#10;&#10;#### Purpose of Normalization&#10;- Organize data to **reduce redundancy** and **improve integrity**.&#10;- Avoid:&#10;  - **Update anomalies**&#10;  - **Insertion anomalies**&#10;  - **Deletion anomalies**&#10;- Achieved by splitting data into well-structured tables and defining relationships.&#10;&#10;---&#10;&#10;#### 1. First Normal Form (1NF)&#10;**Rule:**&#10;- Each column contains **atomic values** (no repeating groups, no arrays).&#10;- Each row-column intersection holds **a single value**.&#10;- Each record must be **unique** (primary key present).&#10;&#10;**Example:**&#10;❌ `Hobbies: [Reading, Swimming]`  &#10;✅  &#10;| ID | Hobby    |&#10;|----|----------|&#10;| 1  | Reading  |&#10;| 1  | Swimming |&#10;&#10;---&#10;&#10;#### 2. Second Normal Form (2NF)&#10;**Prerequisite:** Must be in **1NF**  &#10;**Rule:**&#10;- No **partial dependency** — non-key attributes must depend on the **whole** primary key.&#10;- Applies only to tables with a **composite primary key**.&#10;&#10;**Example:**&#10;❌ `OrderID + ProductID → Quantity`, but `ProductName` depends only on `ProductID`.  &#10;✅ Move product details to a separate **Product** table.&#10;&#10;---&#10;&#10;#### 3. Third Normal Form (3NF)&#10;**Prerequisite:** Must be in **2NF**  &#10;**Rule:**&#10;- No **transitive dependency** — non-key attributes must depend **only** on the primary key.&#10;&#10;**Example:**&#10;❌ `StudentID → DepartmentID → DepartmentName`  &#10;✅ Store `DepartmentID → DepartmentName` in a separate table.&#10;&#10;---&#10;&#10;#### 4. Boyce–Codd Normal Form (BCNF)&#10;**Prerequisite:** Must be in **3NF**  &#10;**Rule:**&#10;- For **every functional dependency (X → Y)**, X must be a **superkey**.&#10;- Stricter than 3NF — resolves anomalies that 3NF may allow.&#10;&#10;---&#10;&#10;#### 5. Fourth Normal Form (4NF)&#10;**Prerequisite:** Must be in **BCNF**  &#10;**Rule:**&#10;- No **multi-valued dependencies** unless they are part of a candidate key.&#10;- Prevents storing unrelated multi-valued facts in the same table.&#10;&#10;**Example:**&#10;If a teacher teaches multiple subjects **and** speaks multiple languages:  &#10;- Store them in separate tables to avoid cross-product redundancy.&#10;&#10;---&#10;&#10;#### 6. Fifth Normal Form (5NF / Project-Join Normal Form)&#10;**Prerequisite:** Must be in **4NF**  &#10;**Rule:**&#10;- No **join dependency** — table should not be reconstructable from smaller tables in any **non-trivial** way.&#10;- Deals with complex relationships broken into **three or more** tables.&#10;&#10;---&#10;&#10;#### Quick Comparison Table&#10;&#10;| Form  | Removes…                 | Focus Area                     |&#10;|-------|--------------------------|---------------------------------|&#10;| 1NF   | Repeating groups, arrays | Atomic data                     |&#10;| 2NF   | Partial dependency       | Full key dependency             |&#10;| 3NF   | Transitive dependency    | Direct PK dependency            |&#10;| BCNF  | Any non-superkey FD      | Strict key dependency           |&#10;| 4NF   | Multi-valued dependency  | No unrelated multi-values       |&#10;| 5NF   | Join dependency          | Complex table reconstruction    |&#10;&#10;&#10;Normalization is the process of organizing data to reduce redundancy and improve data integrity, while denormalization involves combining data to optimize read performance by reducing the number of joins.&#10;&#10;| Feature                | Normalization                                                                 | Denormalization                                                              |&#10;|------------------------|-------------------------------------------------------------------------------|------------------------------------------------------------------------------|&#10;| **Definition**         | Organizing data to reduce redundancy and improve data integrity.             | Combining data to reduce the number of joins and improve read performance.   |&#10;| **Data Redundancy**    | Minimal redundancy; data is stored in separate, related tables.               | Increased redundancy; data is duplicated across tables.                      |&#10;| **Performance**        | Optimized for write operations and data integrity.                            | Optimized for read operations and query performance.                         |&#10;| **Complexity**         | Higher complexity due to multiple tables and relationships.                   | Lower complexity for queries but higher complexity for updates.              |&#10;| **Use Case**           | Suitable for OLTP systems where data integrity and consistency are critical.  | Suitable for OLAP systems where fast read performance is required.           |&#10;| **Storage**            | Requires less storage due to reduced redundancy.                              | Requires more storage due to duplicated data.                                |&#10;| **Maintenance**        | Easier to maintain data integrity and consistency.                            | Harder to maintain consistency due to data duplication.                      |&#10;&#10;---&#10;### CAP Theorem&#10;&#10;The **CAP Theorem**—also known as **Brewer’s Theorem**—states that in any distributed data system, it is **impossible to simultaneously guarantee** all three of the following properties:&#10;&#10;- **C** — **Consistency** - Every read receives the most recent write or an error. Equivalent to strong consistency across nodes.&#10;- **A** — **Availability** - Every request (read or write) receives a non-error response, without the guarantee that it contains the most recent write. The system is responsive even under stress.&#10;- **P** — **Partition Tolerance** - The system continues to operate despite arbitrary partitioning (network failures/loss of connectivity between nodes). Must handle message loss or delay. In any real-world distributed system. So the real choice is between **Consistency** and **Availability**.&#10;&#10;&#10;In practice, a system can **only guarantee two out of the three** at any given time. However with some complimenting strategies you can close to achieve the best of all for a given usecase. Remember the strongly and eventually consistant modes in case of Dynamodb.&#10;&#10;#### Design Trade-offs: Choosing Two&#10;&#10;| Type       | Properties Chosen | Trade-off |&#10;|------------|-------------------|-----------|&#10;| **CP**     | Consistency + Partition Tolerance | May reject requests during partition to preserve data integrity. |&#10;| **CA**     | Consistency + Availability | Not realistic in distributed systems since network partitions are unavoidable. |&#10;| **AP**     | Availability + Partition Tolerance | System may serve stale data or become eventually consistent. |&#10;&#10;#### Design Perspective: What to Choose?&#10;&#10;| Use Case | Recommended Trade-off | Reason |&#10;|----------|------------------------|--------|&#10;| **Banking/Financial Systems** | **CP** | Strong consistency is critical for correctness. |&#10;| **Social Media Feeds**        | **AP** | Availability is prioritized; slight staleness is acceptable. |&#10;| **E-commerce Carts**          | **AP** or **CP** | Depends on whether consistency (inventory) or uptime is more important. |&#10;| **Real-time Messaging**       | **AP** | Users expect availability; some eventual consistency is acceptable. |&#10;&#10;![](../cap.png)&#10;&#10;&gt; ⚠️ **Note**: CAP is a simplified model. In practice, systems also consider latency, throughput, durability, and more advanced consistency models like **Causal Consistency**, **Eventual Consistency**, and **Linearizability**.&#10;&#10;#### Linearizability (Strong Consistency)&#10;- Guarantees that all **operations appear to happen atomically and in a single, global order**.&#10;- Once a write completes, all subsequent reads must return that value or a newer one.&#10;- Operations appear **instantaneous** from the perspective of all clients.&#10;&#10;**Example**:&#10;- User A transfers $100 from Account X to Y.&#10;- User B queries Account X and sees the debited balance immediately.&#10;- No matter which server or region the users connect to, the order is preserved.&#10;&#10;**Pros**:&#10;- Predictable and intuitive behavior.&#10;- Ideal for critical systems (e.g., banking, ledgers).&#10;&#10;**Cons**:&#10;- Slower performance due to coordination overhead.&#10;- Difficult to scale globally.&#10;&#10;#### Causal Consistency (Eventual Consistency)&#10;- Guarantees that **causally related operations are seen in the same order by all nodes**.&#10;- Independent operations may be seen in different orders by different nodes.&#10;&#10;**Example**:&#10;- Alice posts: &quot;I love this product!&quot;&#10;- Bob replies: &quot;Me too!&quot;&#10;- Everyone should see Alice’s post **before** Bob’s reply — because the reply is causally dependent.&#10;&#10;**Pros**:&#10;- Faster and more scalable.&#10;- Sufficient for collaborative apps, chat, social networks.&#10;&#10;**Cons**:&#10;- Weaker guarantee: simultaneous updates may appear in different orders to different users.&#10;- Not suitable for systems needing strong accuracy guarantees.&#10;&#10;---&#10;&#10;### Blob Storage&#10;---&#10;## Scalability and Performance&#10;### Vertical and Horizontal Scaling&#10;&#10;Scaling is a critical aspect of system design that ensures a system can handle increased load or demand. There are two primary types of scaling: vertical scaling and horizontal scaling.&#10;Increase the power of a single machine.&#10;&#10;- **Methods:** Add CPU, RAM, faster storage, GPUs, etc.&#10;- **Pros:**&#10;  - Simple to implement&#10;  - Often no code change required&#10;- **Cons:**&#10;  - Hardware limits&#10;  - Expensive&#10;  - Single point of failure remains&#10;- **Example:** Upgrading a database server from 8 cores to 64 cores&#10;&#10;#### Vertical Scaling&#10;Vertical scaling, also known as &quot;scaling up,&quot; involves adding more resources (e.g., CPU, RAM, or storage) to a single machine. This approach is straightforward and often requires minimal changes to the application.&#10;&#10;**Advantages:**&#10;- Simplicity: Easier to implement and manage e.g. Postgres RDBMS Scaling up and down.&#10;- No need for distributed systems: Avoids complexities like data partitioning and synchronization.&#10;- Suitable for monolithic applications.&#10;&#10;**Disadvantages:**&#10;- Hardware limitations: There is a physical limit to how much a single machine can be upgraded.&#10;- Single point of failure: If the machine fails, the entire system goes down.&#10;- Cost: High-end hardware can be expensive.&#10;&#10;#### Horizontal Scaling&#10;Horizontal scaling, or &quot;scaling out,&quot; involves adding more machines to distribute the load. This approach is commonly used in distributed systems and cloud environments.&#10;&#10;**Advantages:**&#10;- Scalability: Can handle virtually unlimited growth by adding more machines.&#10;- Fault tolerance: Reduces the risk of a single point of failure.&#10;- Cost efficiency: Commodity hardware can be used instead of expensive high-end machines.&#10;&#10;**Disadvantages:**&#10;- Complexity: Requires distributed systems, load balancing, and data partitioning.&#10;- Consistency challenges: Ensuring data consistency across multiple nodes can be difficult.&#10;- Network overhead: Communication between nodes can introduce latency.&#10;&#10;#### Choosing Between Vertical and Horizontal Scaling&#10;- **When to use vertical scaling:**&#10;  - When the application is monolithic and not designed for distributed systems.&#10;  - When the load is predictable and within the limits of a single machine.&#10;  - When simplicity and quick implementation are priorities.&#10;&#10;- **When to use horizontal scaling:**&#10;  - When the system needs to handle unpredictable or massive growth.&#10;  - When fault tolerance and high availability are critical.&#10;  - When the application is designed as a distributed system.&#10;  - Easy in Cloud infrastructure - Pay per use or Pay as you go.&#10;&#10;#### Combining Vertical and Horizontal Scaling&#10;In practice, systems often use a combination of vertical and horizontal scaling. For example:&#10;- Start with vertical scaling for simplicity and quick deployment.&#10;- Transition to horizontal scaling as the system grows and requires higher availability.&#10;&#10;By understanding the trade-offs and leveraging efficient algorithms, you can design systems that scale effectively to meet user demands.&#10;&#10;### Vertical vs Horizontal Scaling&#10;&#10;Vertical scaling and horizontal scaling are two approaches to handle increased system load. Here's a comparison:&#10;&#10;| Feature                | Vertical Scaling                                                              | Horizontal Scaling                                                             |&#10;|------------------------|-------------------------------------------------------------------------------|------------------------------------------------------------------------------|&#10;| **Definition**         | Adding more resources (CPU, RAM, etc.) to a single machine.                  | Adding more machines to distribute the load.                                 |&#10;| **Scalability**        | Limited by the hardware capacity of a single machine.                        | Virtually unlimited by adding more machines.                                 |&#10;| **Fault Tolerance**    | Single point of failure; if the machine fails, the system goes down.         | Higher fault tolerance; failure of one machine does not affect the system.   |&#10;| **Complexity**         | Simpler to implement and manage.                                             | Requires distributed systems, load balancing, and data partitioning.         |&#10;| **Cost**               | High cost for high-end hardware.                                             | Cost-effective with commodity hardware.                                      |&#10;| **Use Case**           | Suitable for monolithic applications with predictable loads.                 | Suitable for distributed systems with unpredictable or massive growth.       |&#10;&#10;### Algorithmic Scaling&#10;&#10;Algorithmic scaling plays a crucial role in ensuring that systems can handle increased load efficiently. By leveraging efficient algorithms and data structures, you can optimize performance and scalability, often reducing the need for additional hardware or resources.&#10;![](../algoscaling.png)&#10;&#10;**Big O Notation:**&#10;Big O notation is used to describe the efficiency of an algorithm in terms of time and space complexity. Common complexities include:&#10;- **O(1):** Constant time. Example: Hash table lookups.&#10;- **O(log n):** Logarithmic time. Example: Binary search.&#10;- **O(n):** Linear time. Example: Iterating through a list.&#10;- **O(n log n):** Log-linear time. Example: Merge sort.&#10;- **O(n^2):** Quadratic time. Example: Nested loops.&#10;&#10;## 2️⃣ Distributed Scaling — The Scale Cube&#10;(*You already know these, so just the headings for completeness*)&#10;&#10;- **X-axis:** Horizontal duplication&#10;- **Y-axis:** Functional decomposition&#10;- **Z-axis:** Data partitioning&#10;&#10;---&#10;&#10;## 3️⃣ Additional Scaling Techniques (Beyond the Cube)&#10;&#10;### 3.1 Scaling by Caching&#10;Reduce load by storing frequently accessed results.  &#10;**Examples:** CDN, Redis, in-memory caches&#10;&#10;### 3.2 Scaling by Asynchrony &amp; Queues&#10;Smooth out load spikes by processing tasks asynchronously.  &#10;**Examples:** Message brokers, event-driven architecture&#10;&#10;### 3.3 Scaling by Algorithmic Efficiency&#10;Reduce the amount of work or make it faster.  &#10;**Examples:** Better data structures, batching, indexing&#10;&#10;### 3.4 Scaling by Concurrency Model&#10;Handle more work in parallel.  &#10;**Examples:** Async I/O, multi-threading, actor model&#10;&#10;### 3.5 Scaling Geographically&#10;Deploy systems in multiple regions for latency and failover benefits.  &#10;**Examples:** Multi-region deployments, edge computing&#10;&#10;---&#10;&#10;## Layered View&#10;&#10;1. **First Layer** → Vertical scaling (make one box stronger)&#10;2. **Second Layer** → Scale Cube (X, Y, Z axes for distribution)&#10;3. **Third Layer** → Optimizations (caching, async, efficiency, concurrency, geo-distribution)&#10;&#10;### Examples of Big O Notation in Java&#10;&#10;#### O(1) - Constant Time&#10;```java&#10;public int getFirstElement(int[] array) {&#10;    return array[0]; // Accessing the first element is constant time.&#10;}&#10;```&#10;&#10;#### O(log n) - Logarithmic Time&#10;```java&#10;public int binarySearch(int[] array, int target) {&#10;    int left = 0, right = array.length - 1;&#10;    while (left &lt;= right) {&#10;        int mid = left + (right - left) / 2;&#10;        if (array[mid] == target) {&#10;            return mid; // Found the target.&#10;        } else if (array[mid] &lt; target) {&#10;            left = mid + 1; // Search in the right half.&#10;        } else {&#10;            right = mid - 1; // Search in the left half.&#10;        }&#10;    }&#10;    return -1; // Target not found.&#10;}&#10;```&#10;&#10;#### O(n) - Linear Time&#10;```java&#10;public int findMax(int[] array) {&#10;    int max = array[0];&#10;    for (int num : array) {&#10;        if (num &gt; max) {&#10;            max = num; // Update max if a larger number is found.&#10;        }&#10;    }&#10;    return max;&#10;}&#10;```&#10;&#10;#### O(n log n) - Log-Linear Time&#10;```java&#10;import java.util.Arrays;&#10;&#10;public void sortArray(int[] array) {&#10;    Arrays.sort(array); // Sorting an array using a comparison-based algorithm like Merge Sort.&#10;}&#10;```&#10;&#10;#### O(n^2) - Quadratic Time&#10;```java&#10;public void printAllPairs(int[] array) {&#10;    for (int i = 0; i &lt; array.length; i++) {&#10;        for (int j = 0; j &lt; array.length; j++) {&#10;            System.out.println(array[i] + &quot;, &quot; + array[j]); // Print all pairs.&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;#### O(2^n) - Exponential Time&#10;```java&#10;public int fibonacci(int n) {&#10;    if (n &lt;= 1) {&#10;        return n; // Base case.&#10;    }&#10;    return fibonacci(n - 1) + fibonacci(n - 2); // Recursive calls.&#10;}&#10;```&#10;&#10;#### O(n!) - Factorial Time&#10;```java&#10;public void generatePermutations(String str, String perm) {&#10;    if (str.isEmpty()) {&#10;        System.out.println(perm); // Print a permutation.&#10;        return;&#10;    }&#10;    for (int i = 0; i &lt; str.length(); i++) {&#10;        char ch = str.charAt(i);&#10;        String remaining = str.substring(0, i) + str.substring(i + 1);&#10;        generatePermutations(remaining, perm + ch); // Recursive call.&#10;    }&#10;}&#10;```&#10;&#10;### Load Balancers&#10;### Rate Limiting&#10;### Content Delivery Optimization&#10;### Zero Downtime Deployment&#10;---&#10;## Common Design Patterns and Architecture&#10;### Event-Driven Architecture&#10;### Data Partitioning Strategies&#10;### Eventual Consistency&#10;### Leader Election&#10;### Circuit Breaker Pattern&#10;### Throttling and Backpressure&#10;### Service Discovery&#10;### Microservices&#10;### Message Queues&#10;---&#10;## Monitoring, Resiliency, and Security&#10;### Monitoring and Observability&#10;### Data Compression&#10;### Authentication and Authorization&#10;### Data Backup and Recovery&#10;### Chaos Engineering&#10;---&#10;## Development and Deployment&#10;### Concurrency Control&#10;### Immutable Infrastructure&#10;### Blue-Green Deployment&#10;---&#10;## Theoretical Concepts&#10;### Search Systems&#10;---&#10;## Data Processing&#10;### Data Streaming&#10;---&#10;## Miscellaneous&#10;### Rate Shaping&#10;" />
              <option name="updatedContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'System Design Interview Concepts'&#10;tags = ['Event Driven Architecture', 'Microservices', 'Interview']&#10;+++&#10;&#10;System design is a critical aspect of software engineering that involves creating scalable, reliable, and efficient systems. This document explores key concepts and strategies required for designing robust systems, including networking, databases, scalability, caching, and modern architectural patterns. It serves as a comprehensive guide for understanding the foundational and advanced principles of system design.&#10;&#10;# Table of Contents&#10;&#10;- [Approach](#approach)&#10;- [HTTPS Certificates in System Design](#https-certificates-in-system-design)&#10;- [Networking and Communication](#networking-and-communication)&#10;  - [Client Server Architecture](#client-server-architecture)&#10;  - [IP Address](#ip-address)&#10;  - [DNS](#dns)&#10;  - [Proxy/Reverse Proxy](#proxyreverse-proxy)&#10;  - [Latency](#latency)&#10;  - [HTTP/HTTPS and MASL (Mutual Authentication Security Layers)](#httphttps-and-masl-mutual-authentication-security-layers)&#10;  - [WebSockets](#websockets)&#10;  - [Webhooks](#webhooks)&#10;- [APIs and Integration](#apis-and-integration)&#10;  - [APIs](#apis)&#10;  - [REST API](#rest-api)&#10;  - [GraphQL](#graphql)&#10;  - [API Gateway](#api-gateway)&#10;  - [Idempotency](#idempotency)&#10;  - [REST and REST Maturity Model](#rest-and-rest-maturity-model)&#10;- [Databases and Storage](#databases-and-storage)&#10;  - [Databases](#databases)&#10;  - [SQL vs NoSQL vs Object Store](#sql-vs-nosql-vs-object-store)&#10;  - [Database Indexing](#database-indexing)&#10;  - [Replication](#replication)&#10;  - [Sharding](#sharding)&#10;  - [Vertical Partitioning](#vertical-partitioning)&#10;  - [Caching](#caching)&#10;  - [ACID vs BASE](#acid-vs-base)&#10;  - [Normalization vs Denormalization](#normalization-vs-denormalization)&#10;  - [CAP Theorem](#cap-theorem)&#10;  - [Blob Storage](#blob-storage)&#10;- [Scalability and Performance](#scalability-and-performance)&#10;  - [Vertical and Horizontal Scaling](#vertical-and-horizontal-scaling)&#10;  - [Algorithmic Scaling](#algorithmic-scaling)&#10;  - [The Scale Cube](#2️⃣-distributed-scaling--the-scale-cube)&#10;  - [Additional Scaling Techniques](#3️⃣-additional-scaling-techniques-beyond-the-cube)&#10;  - [Layered View](#layered-view)&#10;  - [Examples of Big O Notation in Java](#examples-of-big-o-notation-in-java)&#10;  - [Load Balancers](#load-balancers)&#10;  - [Rate Limiting](#rate-limiting)&#10;  - [Content Delivery Optimization](#content-delivery-optimization)&#10;  - [Zero Downtime Deployment](#zero-downtime-deployment)&#10;- [Common Design Patterns and Architecture](#common-design-patterns-and-architecture)&#10;  - [Event-Driven Architecture](#event-driven-architecture)&#10;  - [Data Partitioning Strategies](#data-partitioning-strategies)&#10;  - [Eventual Consistency](#eventual-consistency)&#10;  - [Leader Election](#leader-election)&#10;  - [Circuit Breaker Pattern](#circuit-breaker-pattern)&#10;  - [Throttling and Backpressure](#throttling-and-backpressure)&#10;  - [Service Discovery](#service-discovery)&#10;  - [Microservices](#microservices)&#10;  - [Message Queues](#message-queues)&#10;- [Monitoring, Resiliency, and Security](#monitoring-resiliency-and-security)&#10;  - [Monitoring and Observability](#monitoring-and-observability)&#10;  - [Data Compression](#data-compression)&#10;  - [Authentication and Authorization](#authentication-and-authorization)&#10;  - [Data Backup and Recovery](#data-backup-and-recovery)&#10;  - [Chaos Engineering](#chaos-engineering)&#10;- [Development and Deployment](#development-and-deployment)&#10;  - [Concurrency Control](#concurrency-control)&#10;  - [Immutable Infrastructure](#immutable-infrastructure)&#10;  - [Blue-Green Deployment](#blue-green-deployment)&#10;- [Theoretical Concepts](#theoretical-concepts)&#10;  - [Search Systems](#search-systems)&#10;- [Data Processing](#data-processing)&#10;  - [Data Streaming](#data-streaming)&#10;- [Miscellaneous](#miscellaneous)&#10;  - [Rate Shaping](#rate-shaping)&#10;&#10;## Approach&#10;- Do not directly start designing; every problem is unique. Think of every problem as designing and building a bridge. You must understand:&#10;  - **Whom or What you are building for**  &#10;    - target users or audience and their count.  &#10;    - Expected traffic or load  &#10;    - User behavior e.g. Celebrity Problem &#10;    - Interaction patterns.  &#10;    - Account for user demographics and geographic distribution.  &#10;    - Evaluate the specific needs or goals of the users.&#10;  - **Where you are building** (the environment or constraints).&#10;  - **How you build**.&#10;&#10;## Steps:&#10;- **Understand your use case**: Clearly define the problem and its requirements.&#10;- **Ask the right questions**: Gather Necessary details, NO assumptions.&#10;- **Decide the modules**: Break the big problem into smaller, manageable parts (e.g., defining context boundaries). Ask which they want you to tackle first.&#10;- **Design with key considerations**:  Address other critical &quot;ilities&quot;&#10;     - **Availability**&#10;     - **Reliability**&#10;     - **Maintainability**&#10;     - **Performance**&#10;     - **Scalability**&#10;     - **Cost-efficiency**&#10;     - **Security**&#10;     - **Flexibility**&#10;     (Always remind yourself - everything fails)&#10;---&#10;## HTTPS Certificates in System Design&#10;&#10;In system design, **HTTPS certificates** are essential for securing communication between clients (e.g., web browsers, mobile apps) and servers. They are used to encrypt data, verify server identity, and ensure secure communication channels in modern web and microservice architectures.&#10;&#10;---&#10;&#10;## Key Points about HTTPS Certificates&#10;&#10;### 1. **What is an HTTPS Certificate?**&#10;- **HTTPS certificates** are **X.509 certificates** used in **TLS (Transport Layer Security)** for encrypting and securing HTTP traffic.&#10;- They contain the **public key** of a server, and are signed by a trusted **Certificate Authority (CA)** to confirm the authenticity of the server.&#10;&#10;### 2. **Role in HTTPS Communication:**&#10;- **Encryption**: HTTPS certificates use **TLS** to encrypt data in transit, ensuring that information exchanged between client and server is private and secure.&#10;- **Authentication**: The certificate proves the server's identity, assuring clients that they are communicating with the correct, trusted server.&#10;- **Data Integrity**: It ensures that data cannot be tampered with while in transit.&#10;&#10;---&#10;&#10;## HTTPS Flow in System Design&#10;&#10;1. **DNS Resolution** → The domain name (e.g., `example.com`) is resolved to an IP address.&#10;2. **TCP Handshake** → A 3-way handshake is established between the client and server.&#10;3. **TLS Handshake**:&#10;   - The client requests a secure connection and receives the server's certificate.&#10;   - The client verifies the certificate's authenticity (checking the CA and validity period).&#10;   - The client and server exchange keys to encrypt further communication.&#10;4. **Secure Communication**: The HTTP request and response occur over the encrypted TLS channel.&#10;5. **Connection Termination**: Once communication is complete, the connection is securely closed.&#10;&#10;---&#10;&#10;## HTTPS Certificate Components&#10;&#10;1. **Public Key**: Used for encryption and establishing a secure connection.&#10;2. **Issuer**: The Certificate Authority (CA) that issued the certificate.&#10;3. **Subject**: The entity (e.g., website, server) being identified by the certificate.&#10;4. **Validity Period**: The certificate’s expiration date.&#10;5. **Signature**: A digital signature from the CA, ensuring the certificate's authenticity.&#10;6. **Extensions**: Additional metadata, such as **Subject Alternative Names (SANs)**, which allow a single certificate to cover multiple domains.&#10;&#10;---&#10;&#10;## Use Cases in System Design&#10;&#10;### 1. **Web Applications**&#10;- HTTPS certificates are used to secure user data, such as login credentials and payment details, during transmission between the browser and server.&#10;- **SSL/TLS** ensures that users can trust the site and prevents **man-in-the-middle attacks**.&#10;&#10;### 2. **API Security**&#10;- APIs use HTTPS certificates to secure communication between clients and services, ensuring that data transmitted between services is encrypted and authenticated.&#10;- **API Gateways** often enforce HTTPS for all incoming and outgoing traffic to secure internal and external communications.&#10;&#10;### 3. **Microservices Communication**&#10;- In microservices architectures, services communicate securely using **TLS** certificates.&#10;- Certificates can be used with **mTLS (Mutual TLS)**, where both the client and the server authenticate each other.&#10;- This is common for ensuring trust between services within a **private network**.&#10;&#10;### 4. **Certificate Pinning**&#10;- To prevent attacks, some systems implement **certificate pinning** to ensure that only a specific, trusted certificate can be used, even if it’s issued by a trusted CA.&#10;&#10;---&#10;&#10;## Design Considerations for HTTPS Certificates&#10;&#10;### 1. **Certificate Management:**&#10;   - **Renewal**: Certificates must be renewed periodically (typically every 1-2 years).&#10;   - **Revocation**: Certificates must be revoked if compromised, and Certificate Revocation Lists (CRLs) or **OCSP (Online Certificate Status Protocol)** can be used to check the certificate status.&#10;&#10;### 2. **Load Balancers and API Gateways:**&#10;   - **SSL Termination**: In many architectures, HTTPS connections are terminated at a **load balancer** or **API Gateway**. This means the secure connection between the client and the gateway is decrypted, and the communication between services may continue over plain HTTP or encrypted further.&#10;&#10;### 3. **Security Best Practices:**&#10;   - Use strong encryption algorithms (e.g., **TLS 1.2 or 1.3**).&#10;   - **Perfect Forward Secrecy (PFS)** should be enabled to ensure that past sessions are not compromised even if the server's private key is leaked.&#10;   - Regularly update certificates and private keys.&#10;   - Store private keys securely and limit access.&#10;&#10;---&#10;&#10;## Key Takeaways for System Design&#10;&#10;- **HTTPS certificates** are crucial for securing **web traffic** and **API communications** in modern system architectures.&#10;- They ensure **confidentiality**, **integrity**, and **authentication** between clients and servers.&#10;- Proper **certificate management** (renewal, revocation, etc.) is key for maintaining security.&#10;- **SSL/TLS termination** at **API Gateways** or **load balancers** can simplify management but must be carefully designed to ensure traffic is encrypted when needed.&#10;- **mTLS** can be used for mutual authentication between services, adding an additional layer of security in microservices architectures.&#10;&#10;&#10;## Networking and Communication&#10;### Client Server Architecture&#10;### IP Address&#10;### DNS&#10;### Proxy/Reverse Proxy&#10;### Latency&#10;### HTTP/HTTPS and MASL (Mutual Authentication Security Layers)&#10;&#10;##  1. HTTP / HTTPS in System Design&#10;&#10;###  HTTP&#10;- Stateless protocol for transferring hypertext and media between client and server.&#10;- Operates over **TCP** (usually port **80**).&#10;- Requests consist of **methods** (GET, POST, PUT, DELETE), **headers**, and optionally a **body**.&#10;- No built-in encryption → data sent in plaintext.&#10;&#10;###  HTTPS&#10;- HTTP over **TLS (Transport Layer Security)** → operates on port **443**.&#10;&#10;**Provides:**&#10;-  **Confidentiality:** Encrypts data.&#10;- ️ **Integrity:** Detects tampering.&#10;-  **Authentication:** Validates server identity via SSL/TLS certificates.&#10;&#10;**Used In:**&#10;- Web apps  &#10;- REST APIs  &#10;- Microservices communication  &#10;- IoT and mobile devices  &#10;&#10;##  Details of handshake &#10;&lt;img width=&quot;3000&quot; height=&quot;3336&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/2e8bf550-4932-4c84-a2ca-b3034b57c1f8&quot; /&gt;&#10;&#10;### 1. Client Hello&#10;The client initiates the handshake by sending:&#10;- A list of supported **cipher suites** (algorithms for encryption, key exchange, etc.).&#10;- A **`client_random`** value — a 32-byte random number.&#10;&#10;####  Why `client_random`?&#10;- Adds **entropy** to the key derivation process.&#10;- Ensures each session is **unique**, even if the same algorithms are used.&#10;- Helps prevent **replay attacks** by making the handshake unpredictable.&#10;&#10;### 2. Server Hello + Certificate&#10;The server responds with:&#10;- A selected **cipher suite** from the client's list.&#10;- Its own **`server_random`** value.&#10;- A **digital certificate** (usually X.509) containing its public key and identity.&#10;&#10;####  Why the Certificate?&#10;- Allows the client to **authenticate** the server.&#10;- The client checks:&#10;  - Is the certificate signed by a trusted Certificate Authority (CA)?&#10;  - Is it still valid (not expired)?&#10;  - Does the domain match?&#10;&#10;### 3. Key Exchange and Session Key Derivation&#10;Depending on the chosen cipher suite (e.g., ECDHE), the client and server:&#10;- Exchange **ephemeral public keys**.&#10;- Each side uses its private key and the other’s public key to compute a **shared secret**.&#10;- They use a **Key Derivation Function (KDF)** to combine:&#10;  - The shared secret&#10;  - `client_random`&#10;  - `server_random`&#10;  → to derive **symmetric session keys**.&#10;####  How the Shared Secret Is Computed in ECDHE&#10;&#10; Key Generation&#10;**Client generates:**&#10;&#10;- Private key: a&#10;- Public key: A = aG&#10;**Server generates:**&#10;&#10;- Private key: b&#10;- Public key: B = bG&#10;Here, G is a known base point on the elliptic curve.&#10;- **Client computes**:  &#10;  `S = a * B = a * (bG) = abG`&#10;- **Server computes**:  &#10;  `S = b * A = b * (aG) = abG`&#10;✅ Both arrive at the **same shared secret** `S = abG`.&#10;&#10;####  Why Ephemeral Keys (ECDHE)?&#10;- Provides **forward secrecy**: even if long-term keys are compromised, past sessions remain secure.&#10;- Ensures that each session has **unique encryption keys**.&#10;&#10;### 4. Finished Messages&#10;- Both sides send encrypted &quot;Finished&quot; messages to confirm that the handshake was successful.&#10;- From this point on, all communication is encrypted using the derived session keys.&#10;&#10;---&#10;&#10;###  Summary of Key Components&#10;&#10;| Component        | Purpose                                      |&#10;|------------------|----------------------------------------------|&#10;| `client_random`  | Adds entropy, uniqueness, and prevents replay attacks |&#10;| `server_random`  | Same as above, from the server side          |&#10;| Certificate      | Authenticates the server (and optionally the client) |&#10;| Ephemeral Keys   | Used to compute a shared secret securely     |&#10;| Session Keys     | Encrypt and authenticate all further communication |&#10;&#10;&#10;---&#10;&#10;##  What is Mutual Authentication (mTLS)?&#10;&#10;###  Common Use Cases&#10;&#10;| Use Case                      | Why Use mTLS?                                     |&#10;|------------------------------|---------------------------------------------------|&#10;| Service-to-service (microservices) | Ensure only trusted services communicate          |&#10;| APIs for fintech / healthcare | Regulatory compliance (HIPAA, PCI-DSS)            |&#10;| IoT Devices ↔ Cloud          | Authenticate individual devices securely          |&#10;| Enterprise internal apps     | Add trust within a private/internal network       |&#10;&#10;##  Details of handshake &#10;&lt;img width=&quot;1452&quot; height=&quot;2322&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/93556c5e-3e0b-42a2-90e0-aae7c0ad1da6&quot; /&gt;&#10;&#10;### 1. Client Hello&#10;- Client sends supported cipher suites and `client_random`.&#10;&#10;### 2. Server Hello + Certificate&#10;- Server responds with:&#10;  - Chosen cipher suite&#10;  - `server_random`&#10;  - Server certificate&#10;&#10;### 3. Server Requests Client Certificate&#10;- Server sends a `CertificateRequest` message.&#10;&#10;### 4. Client Sends Certificate&#10;- Client sends its certificate for authentication.&#10;&#10;### 5. Key Exchange&#10;- Both sides exchange ephemeral public keys (e.g., via ECDHE).&#10;- Each side computes the shared secret using its private key and the other’s public key.&#10;&#10;### 6. Certificate Verification&#10;- Server verifies the client’s certificate.&#10;- Client verifies the server’s certificate.&#10;&#10;### 7. Finished Messages&#10;- Both sides send encrypted &quot;Finished&quot; messages.&#10;- Secure communication begins using derived session keys.&#10;&#10;&#10;&#10;### WebSockets&#10;---&#10;### Webhooks&#10;---&#10;## APIs and Integration&#10;### APIs&#10;---&#10;### REST API&#10;---&#10;### GraphQL&#10;---&#10;### API Gateway&#10;---&#10;### Idempotency&#10;&#10;## REST and REST Maturity Model&#10;### What is REST?&#10;REST (Representational State Transfer) is an architectural style for designing networked applications. It relies on stateless communication and standard HTTP methods to enable interaction between clients and servers. RESTful APIs are widely used for their simplicity, scalability, and compatibility with web standards.&#10;&#10;Key principles of REST include:&#10;- **Statelessness**: Each request from a client to a server must contain all the information needed to understand and process the request.&#10;- **Client-Server Architecture**: Separation of concerns between the client and server, allowing them to evolve independently.&#10;- **Uniform Interface**: A consistent and standardized way of interacting with resources.&#10;- **Resource-Based**: Resources are identified using URIs (Uniform Resource Identifiers).&#10;- **Cacheability**: Responses must define whether they are cacheable to improve performance.&#10;- **Layered System**: The architecture can have multiple layers, such as load balancers and proxies, to improve scalability and security.&#10;&#10;### REST Maturity Model&#10;The REST Maturity Model, introduced by Leonard Richardson, defines levels of maturity for RESTful APIs. It helps evaluate how closely an API adheres to REST principles.&#10;&#10;![](../REST.png)&#10;&#10;#### Level 0: The Swamp of POX&#10;- APIs at this level use a single URI and rely on HTTP POST for all interactions.&#10;- They often resemble RPC (Remote Procedure Call) or SOAP (Simple Object Access Protocol).&#10;&#10;#### Level 1: Resources&#10;- Introduces the concept of resources, each identified by a unique URI.&#10;- HTTP methods are not yet fully utilized.&#10;&#10;#### Level 2: HTTP Verbs&#10;- Uses standard HTTP methods (GET, POST, PUT, DELETE, etc.) to perform operations on resources.&#10;- Improves clarity and aligns with REST principles.&#10;&#10;#### Level 3: Hypermedia Controls (HATEOAS)&#10;- Hypermedia as the Engine of Application State (HATEOAS) is implemented.&#10;- Clients can navigate the API dynamically using links provided in responses.&#10;- This level achieves full REST maturity by enabling discoverability and self-documentation.&#10;&#10;### Benefits of REST Maturity&#10;- **Scalability**: Higher levels of maturity improve scalability by leveraging HTTP standards.&#10;- **Interoperability**: Adherence to REST principles ensures compatibility across different platforms.&#10;- **Maintainability**: A well-designed RESTful API is easier to maintain and extend.&#10;- **Discoverability**: HATEOAS enables clients to discover available actions dynamically.&#10;&#10;By understanding and applying the REST Maturity Model, developers can design APIs that are robust, scalable, and aligned with modern web standards.&#10;&#10;---&#10;## Databases and Storage&#10;### Databases&#10;---&#10;### SQL vs NoSQL vs Object Store&#10;&#10;When designing a system, choosing the right data storage solution is crucial. The three main categories are **SQL databases**, **NoSQL databases**, and **Object Stores**. Each serves different use cases and has unique characteristics.&#10;&#10;#### SQL Databases (Relational Databases)&#10;&#10;- **Examples:** MySQL, PostgreSQL, Oracle, Microsoft SQL Server&#10;- **Data Model:** Structured, tabular data with predefined schemas (tables, rows, columns)&#10;- **Query Language:** SQL (Structured Query Language)&#10;- **Transactions:** Strong ACID guarantees (Atomicity, Consistency, Isolation, Durability)&#10;- **Use Cases:** Applications requiring complex queries, joins, and strong consistency (e.g., banking, ERP, CRM)&#10;- **Strengths:** Data integrity, complex querying, relationships, mature ecosystem&#10;- **Limitations:** Vertical scaling, rigid schema, less suited for unstructured or rapidly evolving data&#10;&#10;#### NoSQL Databases&#10;&#10;- **Examples:** MongoDB (Document), Cassandra (Wide-column), Redis (Key-Value), Neo4j (Graph)&#10;- **Data Model:** Flexible, can be document, key-value, column-family, or graph-based&#10;- **Schema:** Schema-less or dynamic schemas; can handle semi-structured or unstructured data&#10;- **Transactions:** Typically BASE properties (Basically Available, Soft state, Eventually consistent)&#10;- **Use Cases:** High scalability, large volumes of diverse data, real-time analytics, IoT, social networks&#10;- **Strengths:** Horizontal scaling, flexible data models, high write/read throughput&#10;- **Limitations:** Weaker consistency (eventual consistency), limited support for complex joins, less mature tooling&#10;&#10;#### Object Store&#10;&#10;- **Examples:** Amazon S3, Google Cloud Storage, Azure Blob Storage, MinIO&#10;- **Data Model:** Stores data as objects (blobs) with metadata and a unique identifier; no schema or tables&#10;- **Access:** Accessed via APIs (REST, SDKs); not a database—optimized for storing and retrieving large files&#10;- **Transactions:** No ACID/BASE guarantees; eventual consistency for some operations&#10;- **Use Cases:** Storing unstructured data (images, videos, backups, logs, large files), data lakes, static website hosting&#10;- **Strengths:** Virtually unlimited scalability, low cost for large data, durability, global access&#10;- **Limitations:** Not suitable for transactional data or complex queries; eventual consistency; slower for small, frequent reads/writes&#10;&#10;#### Comparison Table&#10;&#10;| Feature         | SQL (Relational)         | NoSQL                    | Object Store                |&#10;|-----------------|-------------------------|--------------------------|-----------------------------|&#10;| **Data Model**  | Tables (rows/columns)   | Flexible (JSON, KV, etc) | Objects (blobs + metadata)  |&#10;| **Schema**      | Fixed                   | Dynamic/Schema-less      | None                        |&#10;| **Query**       | SQL                     | Varies (NoSQL APIs)      | API (REST/S3)               |&#10;| **Transactions**| ACID                    | BASE (usually)           | None                        |&#10;| **Scaling**     | Vertical (mostly)       | Horizontal               | Horizontal                  |&#10;| **Best For**    | Structured, relational  | Semi/unstructured, scale | Unstructured, large files   |&#10;| **Examples**    | MySQL, PostgreSQL       | MongoDB, Cassandra       | S3, GCS, Azure Blob         |&#10;&#10;**Summary:**  &#10;- Use **SQL** for structured data and strong consistency.&#10;- Use **NoSQL** for flexible, scalable, high-throughput needs.&#10;- Use **Object Store** for unstructured, large-scale file storage—not as a database.&#10;&#10;---&#10;### Database Indexing&#10;---&#10;### Replication&#10;---&#10;### Sharding&#10;---&#10;### Vertical Partitioning&#10;---&#10;### Caching&#10;&#10;Caching is the process of storing frequently accessed data in a temporary storage layer to improve system performance and reduce latency. &#10;&#10;#### Advantages of Caching&#10;&#10;1. **Improved Performance**: Reduces response time by serving data from faster storage layers (e.g., memory).  &#10;&#10;2. **Reduced Latency**: Minimizes delays in data retrieval, enhancing user experience.  &#10;&#10;3. **Lower Database Load**: Decreases the number of direct queries to the database, reducing resource usage.  &#10;&#10;4. **Scalability**: Helps handle increased traffic by offloading requests from the primary data source.  &#10;&#10;5. **Cost Efficiency**: Reduces operational costs by optimizing resource utilization.  &#10;&#10;#### Disadvantages of Caching&#10;&#10;1. **Data Staleness**: Cached data may become outdated if not properly invalidated or refreshed.  &#10;&#10;2. **Complexity**: Implementing and managing caching layers adds complexity to the system.  &#10;&#10;3. **Cache Misses**: If data is not found in the cache, it can lead to slower performance as the system falls back to the original data source.  &#10;&#10;4. **Memory Overhead**: Caching requires additional memory, which can increase infrastructure costs.  &#10;&#10;5. **Consistency Challenges**: Ensuring data consistency between the cache and the source of truth can be difficult.  &#10;&#10;#### Strategies&#10;&#10;1. **Cache-Aside**: The application checks the cache first. If the data is not found, it fetches from the database and updates the cache. Commonly used for read-heavy workloads.  &#10;&#10;2. **Write-Through**: Data is written to the cache and the database simultaneously. Ensures data consistency but may introduce higher write latency.  &#10;&#10;3. **Write-Behind**: Data is written to the cache first and asynchronously updated in the database. Improves write performance but risks data loss during failures.  &#10;&#10;4. **Read-Through**: The application interacts only with the cache. If the data is not in the cache, the cache fetches it from the database. Simplifies application logic but adds complexity to the caching layer.  &#10;&#10;#### Measuring Cache Effectiveness&#10;&#10;1. **Calculate the Cache Hit Rate**: Measure the percentage of requests served from the cache versus the total requests. A high hit rate indicates effective caching.  &#10;&#10;2. **Analyze Cache Eviction Rate**: Monitor how often data is evicted from the cache due to capacity limits. Optimize cache size and eviction policies to reduce unnecessary evictions.  &#10;&#10;3. **Monitor Data Consistency**: Ensure that cached data remains consistent with the source of truth (e.g., database). Use appropriate invalidation and expiration mechanisms.  &#10;&#10;4. **Determine the Right Cache Expiration Time**: Set expiration times based on data usage patterns and freshness requirements. Avoid stale data while minimizing unnecessary cache misses.&#10;&#10;#### Example Use Cases for Caching&#10;&#10;1. **URL Shortener**: Cache `ShortCode → URL` mappings. Strategy: LRU for frequently accessed URLs.&#10;&#10;2. **User Profile Service**: Cache user profiles with TTL for frequent reads. Challenge: Cache invalidation and consistency.&#10;&#10;3. **Weather Forecast API**: Cache responses based on `city+date`. Set TTL based on forecast freshness.&#10;&#10;4. **Rate Limiter Service**: Cache token bucket or sliding window counters per user. Use Redis or in-memory store with expiration.&#10;&#10;5. **Product Catalog**: Cache product details at edge/CDN. Strategy: Write-through or refresh-on-write.&#10;&#10;6. **Twitter Feed**: Cache user timelines and precompute recent tweets. Eviction policy: LRU or LFU.&#10;&#10;7. **Geolocation Service**: Cache frequently accessed IP ranges. Use TTL for DNS/IP lookups.&#10;&#10;8. **Session Management**: Store sessions in Redis with TTL. Trade-off: In-memory vs database storage.&#10;&#10;9. **Distributed Cache System**: Handle replication vs partitioning. Prevent hot keys and cache stampede.&#10;&#10;10. **Online Code Editor**: Cache user preferences and recent submissions. Use client-side and server-side caching.&#10;&#10;#### References&#10;&#10;- [Cache Strategies - Medium](https://medium.com/@mmoshikoo/cache-strategies-996e91c80303)&#10;&#10;---&#10;### ACID vs BASE&#10;&#10;#### ACID Properties&#10;ACID stands for Atomicity, Consistency, Isolation, and Durability. These properties are essential for traditional relational databases to ensure reliable transactions:&#10;- **Atomicity**: Ensures that a transaction is all-or-nothing. If one part fails, the entire transaction is rolled back.&#10;- **Consistency**: Guarantees that a transaction brings the database from one valid state to another, maintaining all defined rules.Always preserve the data integrity.&#10;- **Isolation**: Ensures that concurrent transactions do not interfere with each other. Don't step on each other shoes. The various problems&#10;&#10;  | Isolation Level      | Dirty Reads | Non-Repeatable Reads | Phantom Reads | Description |&#10;  |----------------------|-------------|-----------------------|----------------|-------------|&#10;  | **Read Uncommitted** | ✅ Allowed  | ✅ Allowed            | ✅ Allowed     | Minimal isolation, allows all anomalies. |&#10;  | **Read Committed**   | ❌ Prevented| ✅ Allowed            | ✅ Allowed     | Only committed data is visible. Default in many databases. |&#10;  | **Repeatable Read**  | ❌ Prevented| ❌ Prevented          | ✅ Allowed     | Rows cannot change, but new rows may appear (phantoms). |&#10;  | **Serializable**     | ❌ Prevented| ❌ Prevented          | ❌ Prevented   | Full isolation, transactions execute as if sequentially. |&#10;&#10;  - **Dirty Read**: Transaction reads data written by another uncommitted transaction.Example: T1 reads a value updated by T2, but T2 hasn't committed. **Solution**: Use `Read Committed` or higher.&#10;  &#10;  - **Non-Repeatable Read**: A row is read twice and returns different data due to an update by another transaction. T1 reads a row, T2 updates and commits it, T1 reads again and gets different data. **Solution**: Use `Repeatable Read` or `Serializable`.&#10;&#10;  - **Phantom Read**: A query returns a different set of rows when re-executed because another transaction inserted/deleted matching rows.Example: T1 runs a query with a condition; T2 inserts a new matching row; T1 reruns and sees new row. **Solution**: Use `Serializable`, or databases supporting MVCC (like PostgreSQL or Oracle).&#10;&#10;- **Durability**: Once a transaction is committed, it remains so, even in the event of a system failure.&#10;### Distributed Transaction Protocols &amp; Patterns&#10;&#10;#### 1. Two-Phase Commit (2PC)&#10;&#10;**Goal:**  &#10;Ensure all participants in a distributed transaction either all commit or all roll back.&#10;&#10;**Roles:**&#10;- **Coordinator** — orchestrates the commit.&#10;- **Participants (Resource Managers)** — e.g., databases, queues.&#10;&#10;**Phases:**&#10;1. **Prepare phase**  &#10;   - Coordinator → Participants: *&quot;Can you commit?&quot;*  &#10;   - Participants:  &#10;     - Validate transaction feasibility (constraints, locks).  &#10;     - If OK → reply **YES** (and lock resources so they can commit later).  &#10;     - If not OK → reply **NO**.&#10;2. **Commit/Abort phase**  &#10;   - If **all YES** → Coordinator sends **COMMIT** to all.  &#10;   - If **any NO** → Coordinator sends **ROLLBACK** to all.&#10;&#10;**Pros:**&#10;- Strong consistency.&#10;- Simple to reason about.&#10;&#10;**Cons:**&#10;- **Blocking** — If coordinator crashes after prepare but before commit, participants wait indefinitely.&#10;- Locks held across prepare → commit can hurt performance.&#10;&#10;---&#10;&#10;#### 2. Three-Phase Commit (3PC)&#10;&#10;**Goal:**  &#10;Reduce 2PC blocking by adding a pre-commit phase.&#10;&#10;**Phases:**&#10;1. **Can Commit**  &#10;   - Same as 2PC’s prepare phase — ask if ready.&#10;2. **Pre-Commit**  &#10;   - If all **YES**: Coordinator sends **PRE-COMMIT** to participants.  &#10;   - Participants acknowledge, enter a state where they can commit without coordinator.&#10;3. **Do Commit**  &#10;   - Coordinator sends **COMMIT**.  &#10;   - If coordinator fails, participants can still commit safely after a timeout (based on pre-commit state).&#10;&#10;**Pros:**&#10;- Less blocking than 2PC.&#10;- Participants can make progress after coordinator failure.&#10;&#10;**Cons:**&#10;- Requires synchronous clocks and reliable network assumptions (rare in real-world WAN).&#10;- More message overhead.&#10;&#10;---&#10;&#10;#### 3. XA Transactions&#10;&#10;**Goal:**  &#10;Provide a standard API for distributed transactions across multiple resource managers.&#10;&#10;**Key Points:**&#10;- Defined by **X/Open XA** spec.&#10;- Involves:&#10;  - **Application** — business logic.&#10;  - **Transaction Manager (TM)** — controls transaction boundaries.&#10;  - **Resource Managers (RM)** — e.g., databases, message brokers.&#10;&#10;**Flow:**&#10;1. Application starts transaction (via TM).&#10;2. Application interacts with multiple RMs.&#10;3. TM calls RMs using XA API to prepare/commit.&#10;4. Under the hood, TM uses **2PC** protocol (almost always).&#10;&#10;**Important:**  &#10;XA is **not** a commit algorithm — it’s a coordination API/spec. But in practice, **XA + 2PC** is the norm.&#10;&#10;#### BASE Properties&#10;BASE stands for Basically Available, Soft state, and Eventually consistent. These properties are common in distributed systems and NoSQL databases:&#10;- **Basically Available**: The system guarantees availability, even in the presence of partial failures.&#10;- **Soft State**: The state of the system may change over time, even without input, due to eventual consistency.&#10;- **Eventually Consistent**: The system will become consistent over time, given that no new updates are made.&#10;&#10;&#10;| Feature                | ACID                                                                                     | BASE                                                                                     |&#10;|------------------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|&#10;| **Definition**         | Ensures reliable transactions with strong consistency and integrity.                     | Focuses on availability and eventual consistency in distributed systems.                 |&#10;| **Consistency**        | Strong consistency; the database is always in a valid state after a transaction.         | Eventual consistency; the system becomes consistent over time.                          |&#10;| **Availability**       | May sacrifice availability for consistency.                                              | Prioritizes availability, even during partial failures.                                  |&#10;| **Data Integrity**     | High data integrity; suitable for critical applications like banking.                    | Lower data integrity; suitable for scalable systems like social media.                  |&#10;| **Transaction Model**  | Transactions are all-or-nothing (atomic).                                                | Transactions may be partial or eventual.                                                |&#10;| **Use Case**           | Ideal for OLTP systems requiring strict data accuracy.                                   | Ideal for distributed systems requiring high scalability and availability.              |&#10;| **Examples**           | Relational databases like MySQL, PostgreSQL.                                             | NoSQL databases like Cassandra, DynamoDB.                                               |&#10;&#10;---&#10;### Normalization vs Denormalization&#10;&#10;#### Purpose of Normalization&#10;- Organize data to **reduce redundancy** and **improve integrity**.&#10;- Avoid:&#10;  - **Update anomalies**&#10;  - **Insertion anomalies**&#10;  - **Deletion anomalies**&#10;- Achieved by splitting data into well-structured tables and defining relationships.&#10;&#10;---&#10;&#10;#### 1. First Normal Form (1NF)&#10;**Rule:**&#10;- Each column contains **atomic values** (no repeating groups, no arrays).&#10;- Each row-column intersection holds **a single value**.&#10;- Each record must be **unique** (primary key present).&#10;&#10;**Example:**&#10;❌ `Hobbies: [Reading, Swimming]`  &#10;✅  &#10;| ID | Hobby    |&#10;|----|----------|&#10;| 1  | Reading  |&#10;| 1  | Swimming |&#10;&#10;---&#10;&#10;#### 2. Second Normal Form (2NF)&#10;**Prerequisite:** Must be in **1NF**  &#10;**Rule:**&#10;- No **partial dependency** — non-key attributes must depend on the **whole** primary key.&#10;- Applies only to tables with a **composite primary key**.&#10;&#10;**Example:**&#10;❌ `OrderID + ProductID → Quantity`, but `ProductName` depends only on `ProductID`.  &#10;✅ Move product details to a separate **Product** table.&#10;&#10;---&#10;&#10;#### 3. Third Normal Form (3NF)&#10;**Prerequisite:** Must be in **2NF**  &#10;**Rule:**&#10;- No **transitive dependency** — non-key attributes must depend **only** on the primary key.&#10;&#10;**Example:**&#10;❌ `StudentID → DepartmentID → DepartmentName`  &#10;✅ Store `DepartmentID → DepartmentName` in a separate table.&#10;&#10;---&#10;&#10;#### 4. Boyce–Codd Normal Form (BCNF)&#10;**Prerequisite:** Must be in **3NF**  &#10;**Rule:**&#10;- For **every functional dependency (X → Y)**, X must be a **superkey**.&#10;- Stricter than 3NF — resolves anomalies that 3NF may allow.&#10;&#10;---&#10;&#10;#### 5. Fourth Normal Form (4NF)&#10;**Prerequisite:** Must be in **BCNF**  &#10;**Rule:**&#10;- No **multi-valued dependencies** unless they are part of a candidate key.&#10;- Prevents storing unrelated multi-valued facts in the same table.&#10;&#10;**Example:**&#10;If a teacher teaches multiple subjects **and** speaks multiple languages:  &#10;- Store them in separate tables to avoid cross-product redundancy.&#10;&#10;---&#10;&#10;#### 6. Fifth Normal Form (5NF / Project-Join Normal Form)&#10;**Prerequisite:** Must be in **4NF**  &#10;**Rule:**&#10;- No **join dependency** — table should not be reconstructable from smaller tables in any **non-trivial** way.&#10;- Deals with complex relationships broken into **three or more** tables.&#10;&#10;---&#10;&#10;#### Quick Comparison Table&#10;&#10;| Form  | Removes…                 | Focus Area                     |&#10;|-------|--------------------------|---------------------------------|&#10;| 1NF   | Repeating groups, arrays | Atomic data                     |&#10;| 2NF   | Partial dependency       | Full key dependency             |&#10;| 3NF   | Transitive dependency    | Direct PK dependency            |&#10;| BCNF  | Any non-superkey FD      | Strict key dependency           |&#10;| 4NF   | Multi-valued dependency  | No unrelated multi-values       |&#10;| 5NF   | Join dependency          | Complex table reconstruction    |&#10;&#10;&#10;Normalization is the process of organizing data to reduce redundancy and improve data integrity, while denormalization involves combining data to optimize read performance by reducing the number of joins.&#10;&#10;| Feature                | Normalization                                                                 | Denormalization                                                              |&#10;|------------------------|-------------------------------------------------------------------------------|------------------------------------------------------------------------------|&#10;| **Definition**         | Organizing data to reduce redundancy and improve data integrity.             | Combining data to reduce the number of joins and improve read performance.   |&#10;| **Data Redundancy**    | Minimal redundancy; data is stored in separate, related tables.               | Increased redundancy; data is duplicated across tables.                      |&#10;| **Performance**        | Optimized for write operations and data integrity.                            | Optimized for read operations and query performance.                         |&#10;| **Complexity**         | Higher complexity due to multiple tables and relationships.                   | Lower complexity for queries but higher complexity for updates.              |&#10;| **Use Case**           | Suitable for OLTP systems where data integrity and consistency are critical.  | Suitable for OLAP systems where fast read performance is required.           |&#10;| **Storage**            | Requires less storage due to reduced redundancy.                              | Requires more storage due to duplicated data.                                |&#10;| **Maintenance**        | Easier to maintain data integrity and consistency.                            | Harder to maintain consistency due to data duplication.                      |&#10;&#10;---&#10;### CAP Theorem&#10;&#10;The **CAP Theorem**—also known as **Brewer’s Theorem**—states that in any distributed data system, it is **impossible to simultaneously guarantee** all three of the following properties:&#10;&#10;- **C** — **Consistency** - Every read receives the most recent write or an error. Equivalent to strong consistency across nodes.&#10;- **A** — **Availability** - Every request (read or write) receives a non-error response, without the guarantee that it contains the most recent write. The system is responsive even under stress.&#10;- **P** — **Partition Tolerance** - The system continues to operate despite arbitrary partitioning (network failures/loss of connectivity between nodes). Must handle message loss or delay. In any real-world distributed system. So the real choice is between **Consistency** and **Availability**.&#10;&#10;&#10;In practice, a system can **only guarantee two out of the three** at any given time. However with some complimenting strategies you can close to achieve the best of all for a given usecase. Remember the strongly and eventually consistant modes in case of Dynamodb.&#10;&#10;#### Design Trade-offs: Choosing Two&#10;&#10;| Type       | Properties Chosen | Trade-off |&#10;|------------|-------------------|-----------|&#10;| **CP**     | Consistency + Partition Tolerance | May reject requests during partition to preserve data integrity. |&#10;| **CA**     | Consistency + Availability | Not realistic in distributed systems since network partitions are unavoidable. |&#10;| **AP**     | Availability + Partition Tolerance | System may serve stale data or become eventually consistent. |&#10;&#10;#### Design Perspective: What to Choose?&#10;&#10;| Use Case | Recommended Trade-off | Reason |&#10;|----------|------------------------|--------|&#10;| **Banking/Financial Systems** | **CP** | Strong consistency is critical for correctness. |&#10;| **Social Media Feeds**        | **AP** | Availability is prioritized; slight staleness is acceptable. |&#10;| **E-commerce Carts**          | **AP** or **CP** | Depends on whether consistency (inventory) or uptime is more important. |&#10;| **Real-time Messaging**       | **AP** | Users expect availability; some eventual consistency is acceptable. |&#10;&#10;![](../cap.png)&#10;&#10;&gt; ⚠️ **Note**: CAP is a simplified model. In practice, systems also consider latency, throughput, durability, and more advanced consistency models like **Causal Consistency**, **Eventual Consistency**, and **Linearizability**.&#10;&#10;#### Linearizability (Strong Consistency)&#10;- Guarantees that all **operations appear to happen atomically and in a single, global order**.&#10;- Once a write completes, all subsequent reads must return that value or a newer one.&#10;- Operations appear **instantaneous** from the perspective of all clients.&#10;&#10;**Example**:&#10;- User A transfers $100 from Account X to Y.&#10;- User B queries Account X and sees the debited balance immediately.&#10;- No matter which server or region the users connect to, the order is preserved.&#10;&#10;**Pros**:&#10;- Predictable and intuitive behavior.&#10;- Ideal for critical systems (e.g., banking, ledgers).&#10;&#10;**Cons**:&#10;- Slower performance due to coordination overhead.&#10;- Difficult to scale globally.&#10;&#10;#### Causal Consistency (Eventual Consistency)&#10;- Guarantees that **causally related operations are seen in the same order by all nodes**.&#10;- Independent operations may be seen in different orders by different nodes.&#10;&#10;**Example**:&#10;- Alice posts: &quot;I love this product!&quot;&#10;- Bob replies: &quot;Me too!&quot;&#10;- Everyone should see Alice’s post **before** Bob’s reply — because the reply is causally dependent.&#10;&#10;**Pros**:&#10;- Faster and more scalable.&#10;- Sufficient for collaborative apps, chat, social networks.&#10;&#10;**Cons**:&#10;- Weaker guarantee: simultaneous updates may appear in different orders to different users.&#10;- Not suitable for systems needing strong accuracy guarantees.&#10;&#10;---&#10;&#10;### Blob Storage&#10;---&#10;## Scalability and Performance&#10;### Vertical and Horizontal Scaling&#10;&#10;Scaling is a critical aspect of system design that ensures a system can handle increased load or demand. There are two primary types of scaling: vertical scaling and horizontal scaling.&#10;Increase the power of a single machine.&#10;&#10;- **Methods:** Add CPU, RAM, faster storage, GPUs, etc.&#10;- **Pros:**&#10;  - Simple to implement&#10;  - Often no code change required&#10;- **Cons:**&#10;  - Hardware limits&#10;  - Expensive&#10;  - Single point of failure remains&#10;- **Example:** Upgrading a database server from 8 cores to 64 cores&#10;&#10;#### Vertical Scaling&#10;Vertical scaling, also known as &quot;scaling up,&quot; involves adding more resources (e.g., CPU, RAM, or storage) to a single machine. This approach is straightforward and often requires minimal changes to the application.&#10;&#10;**Advantages:**&#10;- Simplicity: Easier to implement and manage e.g. Postgres RDBMS Scaling up and down.&#10;- No need for distributed systems: Avoids complexities like data partitioning and synchronization.&#10;- Suitable for monolithic applications.&#10;&#10;**Disadvantages:**&#10;- Hardware limitations: There is a physical limit to how much a single machine can be upgraded.&#10;- Single point of failure: If the machine fails, the entire system goes down.&#10;- Cost: High-end hardware can be expensive.&#10;&#10;#### Horizontal Scaling&#10;Horizontal scaling, or &quot;scaling out,&quot; involves adding more machines to distribute the load. This approach is commonly used in distributed systems and cloud environments.&#10;&#10;**Advantages:**&#10;- Scalability: Can handle virtually unlimited growth by adding more machines.&#10;- Fault tolerance: Reduces the risk of a single point of failure.&#10;- Cost efficiency: Commodity hardware can be used instead of expensive high-end machines.&#10;&#10;**Disadvantages:**&#10;- Complexity: Requires distributed systems, load balancing, and data partitioning.&#10;- Consistency challenges: Ensuring data consistency across multiple nodes can be difficult.&#10;- Network overhead: Communication between nodes can introduce latency.&#10;&#10;#### Choosing Between Vertical and Horizontal Scaling&#10;- **When to use vertical scaling:**&#10;  - When the application is monolithic and not designed for distributed systems.&#10;  - When the load is predictable and within the limits of a single machine.&#10;  - When simplicity and quick implementation are priorities.&#10;&#10;- **When to use horizontal scaling:**&#10;  - When the system needs to handle unpredictable or massive growth.&#10;  - When fault tolerance and high availability are critical.&#10;  - When the application is designed as a distributed system.&#10;  - Easy in Cloud infrastructure - Pay per use or Pay as you go.&#10;&#10;#### Combining Vertical and Horizontal Scaling&#10;In practice, systems often use a combination of vertical and horizontal scaling. For example:&#10;- Start with vertical scaling for simplicity and quick deployment.&#10;- Transition to horizontal scaling as the system grows and requires higher availability.&#10;&#10;By understanding the trade-offs and leveraging efficient algorithms, you can design systems that scale effectively to meet user demands.&#10;&#10;### Vertical vs Horizontal Scaling&#10;&#10;Vertical scaling and horizontal scaling are two approaches to handle increased system load. Here's a comparison:&#10;&#10;| Feature                | Vertical Scaling                                                              | Horizontal Scaling                                                             |&#10;|------------------------|-------------------------------------------------------------------------------|------------------------------------------------------------------------------|&#10;| **Definition**         | Adding more resources (CPU, RAM, etc.) to a single machine.                  | Adding more machines to distribute the load.                                 |&#10;| **Scalability**        | Limited by the hardware capacity of a single machine.                        | Virtually unlimited by adding more machines.                                 |&#10;| **Fault Tolerance**    | Single point of failure; if the machine fails, the system goes down.         | Higher fault tolerance; failure of one machine does not affect the system.   |&#10;| **Complexity**         | Simpler to implement and manage.                                             | Requires distributed systems, load balancing, and data partitioning.         |&#10;| **Cost**               | High cost for high-end hardware.                                             | Cost-effective with commodity hardware.                                      |&#10;| **Use Case**           | Suitable for monolithic applications with predictable loads.                 | Suitable for distributed systems with unpredictable or massive growth.       |&#10;&#10;### Algorithmic Scaling&#10;&#10;Algorithmic scaling plays a crucial role in ensuring that systems can handle increased load efficiently. By leveraging efficient algorithms and data structures, you can optimize performance and scalability, often reducing the need for additional hardware or resources.&#10;![](../algoscaling.png)&#10;&#10;**Big O Notation:**&#10;Big O notation is used to describe the efficiency of an algorithm in terms of time and space complexity. Common complexities include:&#10;- **O(1):** Constant time. Example: Hash table lookups.&#10;- **O(log n):** Logarithmic time. Example: Binary search.&#10;- **O(n):** Linear time. Example: Iterating through a list.&#10;- **O(n log n):** Log-linear time. Example: Merge sort.&#10;- **O(n^2):** Quadratic time. Example: Nested loops.&#10;&#10;## 2️⃣ Distributed Scaling — The Scale Cube&#10;(*You already know these, so just the headings for completeness*)&#10;&#10;- **X-axis:** Horizontal duplication&#10;- **Y-axis:** Functional decomposition&#10;- **Z-axis:** Data partitioning&#10;&#10;---&#10;&#10;## 3️⃣ Additional Scaling Techniques (Beyond the Cube)&#10;&#10;### 3.1 Scaling by Caching&#10;Reduce load by storing frequently accessed results.  &#10;**Examples:** CDN, Redis, in-memory caches&#10;&#10;### 3.2 Scaling by Asynchrony &amp; Queues&#10;Smooth out load spikes by processing tasks asynchronously.  &#10;**Examples:** Message brokers, event-driven architecture&#10;&#10;### 3.3 Scaling by Algorithmic Efficiency&#10;Reduce the amount of work or make it faster.  &#10;**Examples:** Better data structures, batching, indexing&#10;&#10;### 3.4 Scaling by Concurrency Model&#10;Handle more work in parallel.  &#10;**Examples:** Async I/O, multi-threading, actor model&#10;&#10;### 3.5 Scaling Geographically&#10;Deploy systems in multiple regions for latency and failover benefits.  &#10;**Examples:** Multi-region deployments, edge computing&#10;&#10;---&#10;&#10;## Layered View&#10;&#10;1. **First Layer** → Vertical scaling (make one box stronger)&#10;2. **Second Layer** → Scale Cube (X, Y, Z axes for distribution)&#10;3. **Third Layer** → Optimizations (caching, async, efficiency, concurrency, geo-distribution)&#10;&#10;### Examples of Big O Notation in Java&#10;&#10;#### O(1) - Constant Time&#10;```java&#10;public int getFirstElement(int[] array) {&#10;    return array[0]; // Accessing the first element is constant time.&#10;}&#10;```&#10;&#10;#### O(log n) - Logarithmic Time&#10;```java&#10;public int binarySearch(int[] array, int target) {&#10;    int left = 0, right = array.length - 1;&#10;    while (left &lt;= right) {&#10;        int mid = left + (right - left) / 2;&#10;        if (array[mid] == target) {&#10;            return mid; // Found the target.&#10;        } else if (array[mid] &lt; target) {&#10;            left = mid + 1; // Search in the right half.&#10;        } else {&#10;            right = mid - 1; // Search in the left half.&#10;        }&#10;    }&#10;    return -1; // Target not found.&#10;}&#10;```&#10;&#10;#### O(n) - Linear Time&#10;```java&#10;public int findMax(int[] array) {&#10;    int max = array[0];&#10;    for (int num : array) {&#10;        if (num &gt; max) {&#10;            max = num; // Update max if a larger number is found.&#10;        }&#10;    }&#10;    return max;&#10;}&#10;```&#10;&#10;#### O(n log n) - Log-Linear Time&#10;```java&#10;import java.util.Arrays;&#10;&#10;public void sortArray(int[] array) {&#10;    Arrays.sort(array); // Sorting an array using a comparison-based algorithm like Merge Sort.&#10;}&#10;```&#10;&#10;#### O(n^2) - Quadratic Time&#10;```java&#10;public void printAllPairs(int[] array) {&#10;    for (int i = 0; i &lt; array.length; i++) {&#10;        for (int j = 0; j &lt; array.length; j++) {&#10;            System.out.println(array[i] + &quot;, &quot; + array[j]); // Print all pairs.&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;#### O(2^n) - Exponential Time&#10;```java&#10;public int fibonacci(int n) {&#10;    if (n &lt;= 1) {&#10;        return n; // Base case.&#10;    }&#10;    return fibonacci(n - 1) + fibonacci(n - 2); // Recursive calls.&#10;}&#10;```&#10;&#10;#### O(n!) - Factorial Time&#10;```java&#10;public void generatePermutations(String str, String perm) {&#10;    if (str.isEmpty()) {&#10;        System.out.println(perm); // Print a permutation.&#10;        return;&#10;    }&#10;    for (int i = 0; i &lt; str.length(); i++) {&#10;        char ch = str.charAt(i);&#10;        String remaining = str.substring(0, i) + str.substring(i + 1);&#10;        generatePermutations(remaining, perm + ch); // Recursive call.&#10;    }&#10;}&#10;```&#10;&#10;### Load Balancers&#10;### Rate Limiting&#10;### Content Delivery Optimization&#10;### Zero Downtime Deployment&#10;---&#10;## Common Design Patterns and Architecture&#10;### Event-Driven Architecture&#10;### Data Partitioning Strategies&#10;### Eventual Consistency&#10;### Leader Election&#10;### Circuit Breaker Pattern&#10;### Throttling and Backpressure&#10;### Service Discovery&#10;### Microservices&#10;### Message Queues&#10;---&#10;## Monitoring, Resiliency, and Security&#10;### Monitoring and Observability&#10;### Data Compression&#10;### Authentication and Authorization&#10;### Data Backup and Recovery&#10;### Chaos Engineering&#10;---&#10;## Development and Deployment&#10;### Concurrency Control&#10;### Immutable Infrastructure&#10;### Blue-Green Deployment&#10;---&#10;## Theoretical Concepts&#10;### Search Systems&#10;---&#10;## Data Processing&#10;### Data Streaming&#10;---&#10;## Miscellaneous&#10;### Rate Shaping" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/content/posts/system_design/youtube.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/system_design/youtube.md" />
              <option name="originalContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'Youtube System Design Interview'&#10;tags = ['Youtube', 'Interview']&#10;+++&#10;&#10;This document provides a comprehensive, step-by-step breakdown of how to architect a YouTube-scale system, covering requirements, high-level architecture, scaling strategies, microservices patterns, storage, processing pipelines, monitoring, security, and more. The goal is to demonstrate a practical, modern approach to building and scaling a global video platform, suitable for system design interviews.&#10;&#10;## Technical &amp; Business Requirements&#10;&#10;The **platform** must _enable_ **users** to _upload_, _manage_, and _stream_ **video content** at **scale**, _supporting_ a seamless **experience** across **devices** and **geographies**. **Users** should _be able to create_ personal or branded **channels**, _publish_ **videos** with **metadata** (**title**, **description**, **tags**), and _interact_ with **content** through **likes**, **comments**, and **shares**. The **system** must _support_ **user authentication**, personalized **recommendations**, and real-time **notifications** for **subscriptions** and **interactions**. **Search functionality** should _allow_ **users** to _discover_ **content** based on **relevance**, **popularity**, and **preferences**.&#10;&#10;From a **business perspective**, the **system** must _ensure_ high **availability**, **scalability**, and **performance** to _support_ billions of daily **interactions**. It should _enable_ **content moderation workflows** to _maintain_ **community standards** and _comply_ with legal **regulations**. **Analytics capabilities** must _provide_ **insights** into **user engagement**, **video performance**, and **platform health**. The **architecture** should _support_ modular **growth**, _allowing_ **teams** to independently _develop_ and _deploy_ **features** aligned with **business capabilities** such as **video management**, **user engagement**, and **content discovery**.&#10;# Nouns and Verbs from Video Platform Requirements&#10;&#10;## Nouns&#10;&#10;| Noun | Category | Description |&#10;|------|----------|-------------|&#10;| platform | System | The main video streaming system |&#10;| users | People | End users of the platform |&#10;| video content | Media | Videos uploaded to the platform |&#10;| scale | Concept | Large-scale operations |&#10;| experience | Concept | User interaction quality |&#10;| devices | Hardware | User devices (mobile, desktop, etc.) |&#10;| geographies | Location | Global regions |&#10;| channels | Feature | User/brand content channels |&#10;| videos | Media | Individual video files |&#10;| metadata | Data | Video information (title, description, tags) |&#10;| title | Data | Video title |&#10;| description | Data | Video description |&#10;| tags | Data | Video categorization tags |&#10;| content | Media | General platform content |&#10;| likes | Interaction | User approval actions |&#10;| comments | Interaction | User feedback/discussion |&#10;| shares | Interaction | Content sharing actions |&#10;| system | Infrastructure | Platform technical system |&#10;| user authentication | Security | Login/identity verification |&#10;| recommendations | Feature | Personalized content suggestions |&#10;| notifications | Feature | Real-time user alerts |&#10;| subscriptions | Feature | Channel following system |&#10;| interactions | Activity | User engagement activities |&#10;| search functionality | Feature | Content discovery tool |&#10;| relevance | Algorithm | Search ranking factor |&#10;| popularity | Algorithm | Content popularity metric |&#10;| preferences | User Data | User personal choices |&#10;| business perspective | Viewpoint | Commercial considerations |&#10;| availability | Performance | System uptime |&#10;| scalability | Performance | Growth handling capability |&#10;| performance | Quality | System speed/efficiency |&#10;| content moderation workflows | Process | Content review processes |&#10;| community standards | Policy | Platform usage rules |&#10;| regulations | Legal | Legal compliance requirements |&#10;| analytics capabilities | Feature | Data analysis tools |&#10;| insights | Data | Business intelligence |&#10;| user engagement | Metrics | User activity measurements |&#10;| video performance | Metrics | Video success metrics |&#10;| platform health | Metrics | System status indicators |&#10;| architecture | Infrastructure | System design structure |&#10;| growth | Business | Platform expansion |&#10;| teams | People | Development teams |&#10;| features | Functionality | Platform capabilities |&#10;| business capabilities | Concept | Core business functions |&#10;| video management | Capability | Video handling processes |&#10;| user engagement | Capability | User interaction systems |&#10;| content discovery | Capability | Content finding mechanisms |&#10;&#10;## Verbs&#10;&#10;| Verb | Type | Usage Context |&#10;|------|------|---------------|&#10;| enable | Transitive | Allow functionality |&#10;| upload | Transitive | Transfer video files |&#10;| manage | Transitive | Control/organize content |&#10;| stream | Transitive | Deliver video content |&#10;| supporting | Present Participle | Providing assistance |&#10;| create | Transitive | Make new channels |&#10;| publish | Transitive | Make videos available |&#10;| interact | Intransitive | Engage with content |&#10;| support | Transitive | Provide functionality |&#10;| allow | Transitive | Permit actions |&#10;| discover | Transitive | Find content |&#10;| ensure | Transitive | Guarantee outcomes |&#10;| enable | Transitive | Make possible |&#10;| maintain | Transitive | Keep standards |&#10;| comply | Intransitive | Follow regulations |&#10;| provide | Transitive | Supply insights |&#10;| support | Transitive | Enable growth |&#10;| allowing | Present Participle | Permitting teams |&#10;| develop | Transitive | Create features |&#10;| deploy | Transitive | Release features |&#10;&#10;Key DDD Concepts Identified:&#10;&#10;- Entities: Things with identity that change over time (Users, Videos, Channels, Comments)&#10;- Value Objects: Immutable descriptors (Metadata, Tags, Preferences)&#10;- Domain Events: Significant business occurrences (VideoUploaded, UserSubscribed)&#10;- Domain Services: Business logic that doesn't belong to entities (Search, Moderation)&#10;- Aggregates: Consistency boundaries (Video + Metadata, User + Channels)&#10;&#10;8 Bounded Contexts emerged naturally from the requirements:&#10;&#10;- User Management&#10;- Content Management&#10;- Engagement&#10;- Discovery&#10;- Notification&#10;- Analytics&#10;- Moderation&#10;- Content Delivery&#10;&#10;Ubiquitous Language: Defined key terms that should be used consistently across the development team and business stakeholders.&#10;This DDD analysis provides a much stronger foundation for:&#10;&#10;Microservices architecture design&#10;- Team organization&#10;- API boundaries&#10;- Database design&#10;- Business rule implementation&#10;&#10;&#10;## 1. Requirements Gathering&#10;&#10;### Functional Requirements&#10;- **Video Upload**: Users can upload videos (multiple formats, sizes up to 10GB)&#10;- **Video Streaming**: Users can watch videos with adaptive bitrate streaming&#10;- **Video Search**: Search videos by title, description, tags, channel&#10;- **User Management**: Registration, authentication, profiles, subscriptions&#10;- **Social Features**: Comments, likes/dislikes, sharing, playlists&#10;- **Channel Management**: Create channels, manage content, analytics&#10;- **Content Moderation**: Automated and manual content review&#10;- **Notifications**: Subscription updates, comment replies, trending content&#10;&#10;### Non-Functional Requirements&#10;#### User Scale&#10;- **Total registered users:** 2+ billion&#10;- **Daily active users (DAU):** 500+ million&#10;- **Monthly active users (MAU):** 1.5+ billion&#10;- **Concurrent users (peak):** 50+ million&#10;&#10;#### System Throughput&#10;- **Video uploads:** 500+ hours of content uploaded per minute&#10;- **Video consumption:** 1+ billion hours watched daily&#10;- **Search queries:** 1+ billion queries per day&#10;&#10;#### Availability &amp; Reliability&#10;- **Uptime SLA:** 99.9% globally&#10;- **CAP Theorem priority:**&#10;  - Prioritize **Availability** and **Partition Tolerance**&#10;  - Accept **Eventual Consistency** where applicable&#10;&#10;#### Performance &amp; Latency&#10;- **Video start time:** &lt; 2 seconds globally&#10;- **Search results latency:** &lt; 300 milliseconds&#10;- **Upload processing time:** Variable (based on video size and format)&#10;- **Page load time (95th percentile):** &lt; 1 second&#10;&#10;#### Consistency &amp; Data Integrity&#10;- **User authentication &amp; billing:** Strong consistency&#10;- **Social features (likes, comments, views):** Eventually consistent&#10;- **Content metadata updates:** Eventually consistent&#10;&#10;#### Storage &amp; Data Management&#10;- **Video storage:** Exabyte-scale, globally distributed&#10;- **Metadata storage:** Highly available, low-latency key-value store&#10;- **Backup &amp; disaster recovery:** Geo-redundant with RPO &lt; 1 hour&#10;&#10;#### Bandwidth &amp; Traffic&#10;- **Daily data transfer:** Petabyte-scale&#10;- **Peak bandwidth usage:** 100+ Tbps globally&#10;- **CDN usage:** Aggressive caching and edge delivery for video content&#10;&#10;#### Security &amp; Compliance&#10;- **Data encryption:** At rest and in transit&#10;- **Access control:** Role-based and region-aware&#10;- **Compliance:** GDPR, CCPA, and other regional regulations&#10;&#10;### Scale Estimation&#10;- **Users**: 2.7B monthly active users&#10;- **Videos**: 720,000 hours uploaded daily&#10;- **Storage**: 1PB+ new content daily&#10;- **Bandwidth**: 30PB+ daily egress traffic&#10;- **QPS**: 1M+ concurrent video streams&#10;&#10;## 2. High-Level Architecture&#10;&#10;```&#10;[CDN Layer] → [Load Balancers] → [API Gateway] → [Microservices]&#10;                                                      ↓&#10;[Message Queue] ← [Video Processing Pipeline] ← [Object Storage]&#10;                                                      ↓&#10;[Search Engine] ← [Metadata Services] → [Analytics Pipeline]&#10;```&#10;&#10;### Core Components&#10;1. **API Gateway** - Request routing, authentication, rate limiting&#10;2. **Video Upload Service** - Handle video ingestion and initial processing&#10;3. **Video Processing Pipeline** - Transcoding, thumbnail generation, ML analysis&#10;4. **Video Streaming Service** - Adaptive bitrate delivery&#10;5. **Metadata Service** - Video information, user data, social interactions&#10;6. **Search Service** - Video discovery and recommendation&#10;7. **User Service** - Authentication, profiles, subscriptions&#10;8. **Analytics Service** - View tracking, performance metrics&#10;9. **Notification Service** - Real-time updates and alerts&#10;&#10;## 2a. Microservice Decomposition &amp; Hexagonal Architecture (Chris Richardson)&#10;&#10;### Decomposition Strategies (from &quot;Microservices Patterns&quot;)&#10;- **By Business Capability**: Decompose services around business domains (e.g., Video Management, User Management, Social Interactions, Analytics).&#10;- **By Subdomain (DDD)**: Identify core, supporting, and generic subdomains (e.g., Video Processing as core, Notification as supporting).&#10;- **By Transaction Boundary**: Services should own their data and transactional boundaries (e.g., Video Upload and Processing as separate services).&#10;- **By Team Ownership**: Align services with team boundaries for independent delivery.&#10;&#10;### Hexagonal Architecture (Ports &amp; Adapters)&#10;- **Service Core**: Business logic is isolated from external systems.&#10;- **Ports**: Define interfaces for driving (API, UI) and driven (DB, messaging, external APIs) adapters.&#10;- **Adapters**: Implement ports for REST, gRPC, Kafka, databases, etc.&#10;- **Benefits**: Improves testability, flexibility, and separation of concerns.&#10;&#10;#### Example: Video Upload Service (Hexagonal)&#10;- **Core**: Handles upload validation, metadata extraction, and orchestration.&#10;- **Inbound Adapter**: REST API for receiving uploads.&#10;- **Outbound Adapters**: Kafka producer for events, S3 adapter for storage, DB adapter for metadata.&#10;&#10;### Additional Patterns from the Book&#10;- **API Composition**: Aggregate data from multiple services for UI.&#10;- **Database per Service**: Each service owns its schema.&#10;- **Saga Pattern**: Manage distributed transactions (e.g., video upload workflow).&#10;- **CQRS**: Separate read/write models for scalability.&#10;- **Event Sourcing**: Persist state changes as events for auditability.&#10;- **Service Mesh**: Use a service mesh (e.g., Istio, Linkerd) for managing service-to-service communication, observability, and security.&#10;- **Externalized Configuration**: Store configuration outside the service for flexibility and easier deployments.&#10;- **Centralized Log Management**: Aggregate logs for all services to enable monitoring and troubleshooting.&#10;- **Health Check API**: Each service exposes a health check endpoint for orchestration and monitoring.&#10;- **Consumer-Driven Contracts**: Ensure service integrations are reliable and changes do not break consumers.&#10;- **Testing Strategies**: Include contract testing, component testing, and end-to-end testing for microservices.&#10;- **Distributed Tracing**: Implement tracing to follow requests across service boundaries for observability.&#10;- **Service Template/Boilerplate**: Use standardized templates to accelerate new service development and enforce best practices.&#10;&#10;## 3. Scale Cube Application for 10x Growth&#10;&#10;### X-Axis Scaling (Horizontal Duplication)&#10;- **Load Balancers**: Deploy multiple tiers (L4/L7) with auto-scaling&#10;- **API Gateway Clusters**: Regional deployment with intelligent routing&#10;- **Microservice Replicas**: Auto-scaling based on CPU, memory, and queue depth&#10;- **Database Read Replicas**: Multiple read-only instances per region&#10;&#10;### Y-Axis Scaling (Functional Decomposition)&#10;- **Service Decomposition**:&#10;  - Upload Service → Video Ingestion + Metadata Extraction + Storage&#10;  - User Service → Auth + Profile + Subscription + Preferences&#10;  - Social Service → Comments + Likes + Sharing + Community&#10;- **Database Decomposition**: Separate DBs for videos, users, analytics, social&#10;- **Event-Driven Architecture**: Loose coupling via message queues&#10;&#10;### Z-Axis Scaling (Data Partitioning)&#10;- **Video Sharding**: By video ID hash, geographic region, or creator&#10;- **User Sharding**: By user ID hash or geographic region&#10;- **Temporal Sharding**: Hot data (recent) vs cold data (archived)&#10;- **Content-Based Sharding**: By video category, language, or popularity&#10;&#10;## 4. Microservices Design Patterns&#10;&#10;### Service Patterns&#10;- **API Gateway Pattern**: Single entry point with cross-cutting concerns&#10;- **Service Registry &amp; Discovery**: Consul/Eureka for service location&#10;- **Circuit Breaker**: Hystrix for fault tolerance and cascading failure prevention&#10;- **Bulkhead**: Resource isolation between services&#10;- **Retry with Exponential Backoff**: Resilient inter-service communication&#10;&#10;### Data Patterns&#10;- **Database per Service**: Each microservice owns its data&#10;- **Saga Pattern**: Distributed transactions for video upload workflow&#10;- **CQRS**: Separate read/write models for video metadata and analytics&#10;- **Event Sourcing**: Audit trail for user actions and video lifecycle&#10;&#10;### Communication Patterns&#10;- **Asynchronous Messaging**: Kafka for video processing pipeline&#10;- **Request-Response**: HTTP/gRPC for real-time user interactions&#10;- **Publish-Subscribe**: Event notifications for subscriptions&#10;- **Message Routing**: Content-based routing for different video types&#10;&#10;- **Hexagonal Architecture**: Each service is designed using ports and adapters, isolating business logic from infrastructure.&#10;- **Decomposition by Business Capability**: Services are split by domain, following DDD and team boundaries.&#10;- **Saga Pattern**: Used for workflows like video upload and processing.&#10;- **CQRS &amp; Event Sourcing**: Applied for scalability and auditability.&#10;&#10;## 5. Event-Driven Architecture (EDA)&#10;&#10;### Event Streaming Platform&#10;```&#10;Video Upload → [Event Producer] → [Kafka Topics] → [Event Consumers] → Processing Services&#10;```&#10;&#10;### Core Events&#10;- **VideoUploadedEvent**: Triggers transcoding pipeline&#10;- **VideoProcessedEvent**: Updates metadata and makes video available&#10;- **UserActionEvent**: Likes, comments, views for recommendation engine&#10;- **SubscriptionEvent**: Channel subscription/unsubscription&#10;- **ModerationEvent**: Content review results&#10;&#10;### Event Patterns&#10;- **Event Sourcing**: Store all state changes as events&#10;- **CQRS**: Separate command and query responsibility&#10;- **Event Choreography**: Services react to events autonomously&#10;- **Event Orchestration**: Central coordinator for complex workflows&#10;&#10;## 6. CAP Theorem Considerations&#10;&#10;### Design Decisions&#10;- **Partition Tolerance**: Always required in distributed system&#10;- **Availability vs Consistency Trade-offs**:&#10;  - **AP Systems**: Video streaming, comments, likes (eventual consistency)&#10;  - **CP Systems**: User authentication, payment processing&#10;  - **CA Systems**: Single-region components with strong consistency&#10;&#10;### Implementation Strategy&#10;- **Multi-Region Deployment**: Handle network partitions&#10;- **Eventual Consistency**: Social features can tolerate temporary inconsistency&#10;- **Strong Consistency**: Critical operations like user authentication&#10;- **Conflict Resolution**: Last-writer-wins, vector clocks for concurrent updates&#10;&#10;## 7. Storage Architecture&#10;&#10;### Video Storage&#10;- **Object Storage**: S3/GCS for raw and processed video files&#10;- **CDN**: CloudFront/CloudFlare for global content delivery&#10;- **Storage Tiers**: Hot (recent), warm (popular), cold (archived)&#10;- **Compression**: AV1 codec for 30% bandwidth savings&#10;&#10;### Metadata Storage&#10;- **Relational Database**: PostgreSQL for structured data (users, videos)&#10;- **Document Database**: MongoDB for flexible schemas (comments, analytics)&#10;- **Graph Database**: Neo4j for social relationships and recommendations&#10;- **Cache Layer**: Redis for frequently accessed data&#10;&#10;### Search Index&#10;- **Elasticsearch**: Full-text search for videos, channels, playlists&#10;- **Vector Database**: Pinecone for ML-based video recommendations&#10;- **Real-time Indexing**: Stream processing for immediate search availability&#10;&#10;## 8. Video Processing Pipeline&#10;&#10;### Processing Stages&#10;1. **Ingestion**: Upload validation, virus scanning, metadata extraction&#10;2. **Transcoding**: Multiple resolutions, formats, and bitrates&#10;3. **AI Processing**: Content analysis, thumbnail generation, closed captions&#10;4. **Quality Check**: Automated quality assessment and optimization&#10;5. **Distribution**: CDN upload and cache warming&#10;&#10;### Technologies&#10;- **Message Queue**: Apache Kafka for pipeline orchestration&#10;- **Container Orchestration**: Kubernetes for scalable processing&#10;- **Workflow Engine**: Apache Airflow for complex processing workflows&#10;- **ML Platform**: TensorFlow Serving for content analysis&#10;&#10;## 9. Scaling Strategies for 10x Growth&#10;&#10;### Infrastructure Scaling&#10;- **Multi-Cloud**: AWS, GCP, Azure for redundancy and cost optimization&#10;- **Edge Computing**: Process videos closer to users&#10;- **Serverless**: Lambda/Cloud Functions for variable workloads&#10;- **Auto-scaling**: Predictive scaling based on usage patterns&#10;&#10;### Performance Optimization&#10;- **Caching Strategy**: &#10;  - L1: Browser cache (static content)&#10;  - L2: CDN cache (popular videos)&#10;  - L3: Application cache (metadata)&#10;  - L4: Database cache (query results)&#10;&#10;### Data Management&#10;- **Data Archiving**: Move old content to cheaper storage tiers&#10;- **Data Compression**: Advanced codecs and compression algorithms&#10;- **Smart Prefetching**: ML-based content prediction and caching&#10;- **Geographic Optimization**: Content placement based on user location&#10;&#10;## 10. Monitoring and Observability&#10;&#10;### Metrics&#10;- **Golden Signals**: Latency, traffic, errors, saturation&#10;- **Business Metrics**: Video start failures, buffering ratio, user engagement&#10;- **Infrastructure Metrics**: CPU, memory, network, storage utilization&#10;&#10;### Tools&#10;- **Monitoring**: Prometheus, Grafana, DataDog&#10;- **Logging**: ELK Stack (Elasticsearch, Logstash, Kibana)&#10;- **Tracing**: Jaeger, Zipkin for distributed tracing&#10;- **Alerting**: PagerDuty for incident management&#10;&#10;## 11. Security Considerations&#10;&#10;### Content Security&#10;- **DRM**: Widevine, FairPlay for premium content protection&#10;- **Content Filtering**: ML-based inappropriate content detection&#10;- **Access Control**: JWT tokens, OAuth 2.0, rate limiting&#10;&#10;### Infrastructure Security&#10;- **Network Security**: VPC, security groups, WAF&#10;- **Encryption**: TLS in transit, AES-256 at rest&#10;- **Secrets Management**: HashiCorp Vault, AWS Secrets Manager&#10;- **Compliance**: GDPR, COPPA, regional data protection laws&#10;&#10;## 12. Cost Optimization&#10;&#10;### Storage Optimization&#10;- **Intelligent Tiering**: Automatic movement between storage classes&#10;- **Deduplication**: Remove duplicate video segments&#10;- **Compression**: Advanced codecs (AV1, H.265) for bandwidth savings&#10;- **Regional Optimization**: Store content closer to primary audience&#10;&#10;### Compute Optimization&#10;- **Spot Instances**: Use for batch processing jobs&#10;- **Right-sizing**: ML-based instance size recommendations&#10;- **Reserved Capacity**: Long-term commitments for predictable workloads&#10;- **Serverless**: Pay-per-use for variable workloads&#10;&#10;## 13. Disaster Recovery and Business Continuity&#10;&#10;### Backup Strategy&#10;- **Multi-Region Replication**: Critical data replicated across regions&#10;- **Point-in-Time Recovery**: Database snapshots and transaction logs&#10;- **Content Backup**: Multiple copies of popular content&#10;&#10;### Recovery Procedures&#10;- **RTO (Recovery Time Objective)**: 15 minutes for critical services&#10;- **RPO (Recovery Point Objective)**: 5 minutes for user data&#10;- **Failover Automation**: Automated traffic rerouting during outages&#10;- **Chaos Engineering**: Regular disaster simulations&#10;&#10;## 14. Key Principles and Laws Applied&#10;&#10;### Performance Laws&#10;- **Little's Law**: Average number of items in a queuing system equals the average arrival rate multiplied by the average time an item spends in the system.&#10;- **Amdahl's Law**: The speedup of a program from parallelization is limited by the sequential portion.&#10;- **Universal Scalability Law**: Models the impact of contention and coherency delays in distributed systems.&#10;&#10;### Design Principles&#10;- **Single Responsibility Principle**: Each service has one clear purpose.&#10;- **Open/Closed Principle**: Services should be open for extension, closed for modification.&#10;- **Dependency Inversion Principle**: Depend on abstractions, not concretions.&#10;- **Fail Fast**: Detect and report errors immediately.&#10;&#10;### Reliability Patterns&#10;- **Bulkhead**: Isolate resources to prevent cascading failures.&#10;- **Circuit Breaker**: Prevent calls to failing services.&#10;- **Timeout**: Set maximum wait times for all operations.&#10;- **Idempotency**: Safe to retry operations multiple times.&#10;&#10;### Additional Laws and Principles&#10;- **Murphy's Law**: &quot;Anything that can go wrong will go wrong.&quot; Design for failure and recovery.&#10;- **Conway's Law**: System design mirrors the communication structure of the organization.&#10;- **Occam's Razor**: Prefer the simplest solution that works.&#10;- **Robustness Principle (Postel's Law)**: &quot;Be conservative in what you send, be liberal in what you accept.&quot;&#10;- **Law of Demeter**: Minimize coupling by only interacting with immediate collaborators.&#10;- **Hofstadter's Law**: &quot;It always takes longer than you expect, even when you take into account Hofstadter's Law.&quot;&#10;- **Pareto Principle (80/20 Rule)**: 80% of effects come from 20% of causes; optimize for the critical path.&#10;- **Peter Principle**: In hierarchical organizations, people tend to be promoted to their level of incompetence (impacts team/org design).&#10;- **Gall's Law**: A complex system that works is invariably found to have evolved from a simple system that worked.&#10;&#10;## 15. Database Design&#10;&#10;### User Service Database&#10;```sql&#10;-- Users table&#10;CREATE TABLE users (&#10;    user_id BIGINT PRIMARY KEY,&#10;    username VARCHAR(50) UNIQUE NOT NULL,&#10;    email VARCHAR(255) UNIQUE NOT NULL,&#10;    password_hash VARCHAR(255) NOT NULL,&#10;    created_at TIMESTAMP DEFAULT NOW(),&#10;    last_login TIMESTAMP&#10;);&#10;&#10;-- Channels table&#10;CREATE TABLE channels (&#10;    channel_id BIGINT PRIMARY KEY,&#10;    user_id BIGINT REFERENCES users(user_id),&#10;    channel_name VARCHAR(100) NOT NULL,&#10;    description TEXT,&#10;    subscriber_count BIGINT DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;&#10;-- Subscriptions table (sharded by user_id)&#10;CREATE TABLE subscriptions (&#10;    subscription_id BIGINT PRIMARY KEY,&#10;    subscriber_id BIGINT REFERENCES users(user_id),&#10;    channel_id BIGINT REFERENCES channels(channel_id),&#10;    subscribed_at TIMESTAMP DEFAULT NOW(),&#10;    UNIQUE(subscriber_id, channel_id)&#10;);&#10;```&#10;&#10;### Video Service Database&#10;```sql&#10;-- Videos table (sharded by video_id hash)&#10;CREATE TABLE videos (&#10;    video_id BIGINT PRIMARY KEY,&#10;    channel_id BIGINT NOT NULL,&#10;    title VARCHAR(255) NOT NULL,&#10;    description TEXT,&#10;    duration INTEGER, -- in seconds&#10;    view_count BIGINT DEFAULT 0,&#10;    like_count BIGINT DEFAULT 0,&#10;    dislike_count BIGINT DEFAULT 0,&#10;    upload_time TIMESTAMP DEFAULT NOW(),&#10;    processing_status ENUM('uploading', 'processing', 'ready', 'failed'),&#10;    visibility ENUM('public', 'private', 'unlisted')&#10;);&#10;&#10;-- Video metadata table&#10;CREATE TABLE video_metadata (&#10;    video_id BIGINT PRIMARY KEY REFERENCES videos(video_id),&#10;    file_size BIGINT,&#10;    codec VARCHAR(50),&#10;    resolution VARCHAR(20),&#10;    bitrate INTEGER,&#10;    thumbnail_url VARCHAR(500),&#10;    tags TEXT[] -- PostgreSQL array for tags&#10;);&#10;&#10;-- Comments table (sharded by video_id)&#10;CREATE TABLE comments (&#10;    comment_id BIGINT PRIMARY KEY,&#10;    video_id BIGINT NOT NULL,&#10;    user_id BIGINT NOT NULL,&#10;    parent_comment_id BIGINT, -- for replies&#10;    content TEXT NOT NULL,&#10;    like_count INTEGER DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;```&#10;&#10;## 16. API Design&#10;&#10;### REST API Endpoints&#10;&#10;#### Video Operations&#10;```http&#10;# Upload video&#10;POST /api/v1/videos&#10;Content-Type: multipart/form-data&#10;&#10;# Get video details&#10;GET /api/v1/videos/{videoId}&#10;&#10;# Update video metadata&#10;PUT /api/v1/videos/{videoId}&#10;&#10;# Delete video&#10;DELETE /api/v1/videos/{videoId}&#10;&#10;# Search videos&#10;GET /api/v1/videos/search?q={query}&amp;limit={limit}&amp;offset={offset}&#10;&#10;# Get trending videos&#10;GET /api/v1/videos/trending?category={category}&amp;region={region}&#10;```&#10;&#10;#### User Operations&#10;```http&#10;# User registration&#10;POST /api/v1/users/register&#10;&#10;# User login&#10;POST /api/v1/users/login&#10;&#10;# Get user profile&#10;GET /api/v1/users/{userId}&#10;&#10;# Subscribe to channel&#10;POST /api/v1/users/{userId}/subscriptions/{channelId}&#10;&#10;# Get user subscriptions&#10;GET /api/v1/users/{userId}/subscriptions&#10;```&#10;&#10;#### Social Operations&#10;```http&#10;# Like/Unlike video&#10;POST /api/v1/videos/{videoId}/like&#10;DELETE /api/v1/videos/{videoId}/like&#10;&#10;# Add comment&#10;POST /api/v1/videos/{videoId}/comments&#10;&#10;# Get comments&#10;GET /api/v1/videos/{videoId}/comments?limit={limit}&amp;offset={offset}&#10;&#10;# Reply to comment&#10;POST /api/v1/comments/{commentId}/replies&#10;```&#10;&#10;### GraphQL Schema (Alternative)&#10;```graphql&#10;type Video {&#10;  id: ID!&#10;  title: String!&#10;  description: String&#10;  duration: Int!&#10;  viewCount: Int!&#10;  likeCount: Int!&#10;  uploadTime: String!&#10;  channel: Channel!&#10;  comments(first: Int, after: String): CommentConnection&#10;}&#10;&#10;type Channel {&#10;  id: ID!&#10;  name: String!&#10;  subscriberCount: Int!&#10;  videos(first: Int, after: String): VideoConnection&#10;}&#10;&#10;type Query {&#10;  video(id: ID!): Video&#10;  searchVideos(query: String!, first: Int, after: String): VideoConnection&#10;  trendingVideos(category: String, region: String): [Video!]!&#10;}&#10;&#10;type Mutation {&#10;  uploadVideo(input: VideoInput!): Video&#10;  likeVideo(videoId: ID!): Video&#10;  addComment(videoId: ID!, content: String!): Comment&#10;}&#10;```&#10;&#10;## 17. Caching Strategy&#10;&#10;### Multi-Level Caching&#10;```yaml&#10;# Level 1: Browser Cache&#10;- Static assets: 1 year&#10;- Video thumbnails: 1 week&#10;- API responses: 5 minutes&#10;&#10;# Level 2: CDN Cache (CloudFlare/CloudFront)&#10;- Video segments: 1 day&#10;- Thumbnails: 1 week&#10;- API responses: 1 minute&#10;&#10;# Level 3: Application Cache (Redis)&#10;- Popular video metadata: 1 hour&#10;- User sessions: 24 hours&#10;- Search results: 15 minutes&#10;- Trending videos: 30 minutes&#10;&#10;# Level 4: Database Query Cache&#10;- Complex analytics queries: 5 minutes&#10;- User profile data: 30 minutes&#10;- Channel information: 1 hour&#10;```&#10;&#10;### Cache Invalidation Strategy&#10;- **Time-based**: TTL for most cached data&#10;- **Event-based**: Invalidate on video updates, user actions&#10;- **Version-based**: Cache keys include version numbers&#10;- **Write-through**: Update cache and database simultaneously&#10;- **Write-behind**: Async cache updates for non-critical data&#10;&#10;## 18. Message Queue Architecture&#10;&#10;### Kafka Topic Design&#10;```yaml&#10;# Video Processing Topics&#10;video-upload-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 7 days&#10;&#10;video-transcoding-jobs:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 3 days&#10;&#10;video-ready-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# User Activity Topics&#10;user-view-events:&#10;  partitions: 200&#10;  replication-factor: 3&#10;  retention: 90 days&#10;&#10;user-interaction-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# Notification Topics&#10;subscription-notifications:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 7 days&#10;```&#10;&#10;### Event Schema (Avro)&#10;```json&#10;{&#10;  &quot;type&quot;: &quot;record&quot;,&#10;  &quot;name&quot;: &quot;VideoUploadEvent&quot;,&#10;  &quot;fields&quot;: [&#10;    {&quot;name&quot;: &quot;videoId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;userId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;channelId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;filename&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;fileSize&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;contentType&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;uploadTimestamp&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;metadata&quot;, &quot;type&quot;: {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;string&quot;}}&#10;  ]&#10;}&#10;```&#10;&#10;## 19. Load Balancing Strategy&#10;&#10;### Geographic Load Balancing&#10;```yaml&#10;# DNS-based routing&#10;Global Load Balancer:&#10;  - US-East: 40% traffic&#10;  - US-West: 20% traffic&#10;  - Europe: 25% traffic&#10;  - Asia-Pacific: 15% traffic&#10;&#10;# Regional Load Balancers&#10;Regional LB (L7):&#10;  - Path-based routing: /api/upload → Upload Service&#10;  - Header-based routing: User-Agent → Mobile/Web Service&#10;  - Weighted routing: Canary deployments&#10;&#10;# Service Load Balancers (L4)&#10;Service Discovery:&#10;  - Health checks every 30 seconds&#10;  - Circuit breaker: 5 failures in 60 seconds&#10;  - Load balancing algorithms: Weighted round-robin&#10;```&#10;&#10;### Auto-scaling Configuration&#10;```yaml&#10;# Horizontal Pod Autoscaler (Kubernetes)&#10;Video Upload Service:&#10;  minReplicas: 10&#10;  maxReplicas: 100&#10;  targetCPUUtilization: 70%&#10;  targetMemoryUtilization: 80%&#10;  scaleUpStabilization: 60s&#10;  scaleDownStabilization: 300s&#10;&#10;Video Streaming Service:&#10;  minReplicas: 50&#10;  maxReplicas: 500&#10;  targetCPUUtilization: 60%&#10;  customMetrics:&#10;    - concurrent_streams_per_pod: 1000&#10;```&#10;&#10;## 20. Monitoring and Alerting&#10;&#10;### Key Metrics Dashboard&#10;```yaml&#10;# Golden Signals&#10;Latency:&#10;  - Video start time: P50, P95, P99&#10;  - API response time: P50, P95, P99&#10;  - Upload processing time: P50, P95, P99&#10;&#10;Traffic:&#10;  - Requests per second by endpoint&#10;  - Concurrent video streams&#10;  - Upload requests per minute&#10;&#10;Errors:&#10;  - HTTP error rates (4xx, 5xx)&#10;  - Video processing failures&#10;  - Database connection errors&#10;&#10;Saturation:&#10;  - CPU utilization across services&#10;  - Memory usage patterns&#10;  - Queue depth in Kafka topics&#10;  - Storage utilization&#10;```&#10;&#10;### Alert Rules&#10;```yaml&#10;# Critical Alerts (PagerDuty)&#10;Video Start Failure Rate &gt; 1%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;API Error Rate &gt; 5%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;Database Connection Pool &gt; 90%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;# Warning Alerts (Slack)&#10;Upload Processing Time &gt; P95:&#10;  severity: warning&#10;  notification: 5-minute delay&#10;&#10;CDN Cache Hit Rate &lt; 85%:&#10;  severity: warning&#10;  notification: 10-minute delay&#10;```&#10;&#10;## 21. Additional Considerations and Patterns&#10;&#10;### Service Governance &amp; Platform Engineering&#10;- **API Gateway Governance**: Centralized API management, versioning, and security policies.&#10;- **Service Catalog**: Maintain a registry of all services, their owners, and documentation.&#10;- **Platform as a Service (PaaS)**: Internal developer platforms for rapid service deployment and consistency.&#10;- **Service Lifecycle Management**: Automated onboarding, deprecation, and retirement of services.&#10;&#10;### Observability &amp; Operations&#10;- **Distributed Tracing**: End-to-end request tracing across all services.&#10;- **Log Correlation**: Correlate logs, traces, and metrics for faster root cause analysis.&#10;- **Proactive Anomaly Detection**: ML-based monitoring for early detection of issues.&#10;- **Feature Flags**: Gradual rollout and rollback of features without redeployments.&#10;&#10;### Security Enhancements&#10;- **Zero Trust Security Model**: Authenticate and authorize every request, even within the internal network.&#10;- **API Rate Limiting &amp; Throttling**: Prevent abuse and ensure fair usage.&#10;- **Audit Logging**: Immutable logs for all sensitive operations and access.&#10;&#10;### Compliance &amp; Data Privacy&#10;- **Data Residency Controls**: Ensure data is stored and processed in compliance with regional laws.&#10;- **User Data Portability &amp; Deletion**: GDPR-compliant APIs for user data export and erasure.&#10;- **PII Masking**: Mask personally identifiable information in logs and analytics.&#10;&#10;### Advanced Data &amp; ML&#10;- **Real-Time Analytics**: Stream processing for instant insights (e.g., Apache Flink).&#10;- **A/B Testing Platform**: Experiment with new features and recommendation algorithms.&#10;- **Personalization Engine**: ML-driven recommendations, thumbnails, and notifications.&#10;- **Content Fingerprinting**: Detect duplicate or copyrighted content at scale.&#10;&#10;### Reliability Engineering&#10;- **Self-Healing Infrastructure**: Automated detection and remediation of failed components.&#10;- **Chaos Engineering**: Regularly inject failures to test system resilience.&#10;- **Disaster Recovery Drills**: Scheduled simulations to validate recovery procedures.&#10;&#10;### Developer Experience&#10;- **Local Development Environments**: Simulate production-like environments for testing.&#10;- **Automated Code Quality Checks**: Linting, static analysis, and security scanning in CI/CD.&#10;- **Comprehensive Documentation**: Up-to-date API, architecture, and operational docs.&#10;&#10;### Sustainability &amp; Cost Controls&#10;- **Green Computing Initiatives**: Optimize for energy efficiency and carbon footprint.&#10;- **Cost Visibility Dashboards**: Real-time cost tracking per service and feature.&#10;- **Resource Quotas**: Prevent runaway costs by enforcing limits.&#10;&#10;### Community &amp; Ecosystem&#10;- **Open API Standards**: Support for OpenAPI/GraphQL for third-party integrations.&#10;- **Plugin/Extension Framework**: Allow external developers to extend platform capabilities.&#10;- **Community Moderation Tools**: Empower users to help maintain content quality.&#10;&#10;## Conclusion&#10;&#10;This comprehensive YouTube system design demonstrates how to architect a massive-scale video platform that can handle billions of users and petabytes of content. The design incorporates modern distributed systems principles, microservices architecture, event-driven patterns, and advanced scaling techniques.&#10;&#10;The 10x scaling strategy leverages the Scale Cube dimensions, applies CAP theorem principles strategically, and utilizes cutting-edge technologies to ensure the platform remains performant, reliable, and cost-effective as it grows. The architecture is designed to be resilient, observable, and maintainable while providing an excellent user experience globally.&#10;" />
              <option name="updatedContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'Youtube System Design Interview'&#10;tags = ['Youtube', 'Interview']&#10;+++&#10;&#10;# Table of Contents&#10;&#10;- [Technical &amp; Business Requirements](#technical--business-requirements)&#10;- [Nouns and Verbs from Video Platform Requirements](#nouns-and-verbs-from-video-platform-requirements)&#10;- [Requirements Gathering](#1-requirements-gathering)&#10;- [High-Level Architecture](#2-high-level-architecture)&#10;- [Microservice Decomposition &amp; Hexagonal Architecture (Chris Richardson)](#2a-microservice-decomposition--hexagonal-architecture-chris-richardson)&#10;- [Scale Cube Application for 10x Growth](#3-scale-cube-application-for-10x-growth)&#10;- [Microservices Design Patterns](#4-microservices-design-patterns)&#10;- [Event-Driven Architecture (EDA)](#5-event-driven-architecture-eda)&#10;- [CAP Theorem Considerations](#6-cap-theorem-considerations)&#10;- [Storage Architecture](#7-storage-architecture)&#10;- [Video Processing Pipeline](#8-video-processing-pipeline)&#10;- [Scaling Strategies for 10x Growth](#9-scaling-strategies-for-10x-growth)&#10;- [Monitoring and Observability](#10-monitoring-and-observability)&#10;- [Security Considerations](#11-security-considerations)&#10;- [Cost Optimization](#12-cost-optimization)&#10;- [Disaster Recovery and Business Continuity](#13-disaster-recovery-and-business-continuity)&#10;- [Key Principles and Laws Applied](#14-key-principles-and-laws-applied)&#10;- [Database Design](#15-database-design)&#10;- [API Design](#16-api-design)&#10;- [Caching Strategy](#17-caching-strategy)&#10;- [Message Queue Architecture](#18-message-queue-architecture)&#10;- [Load Balancing Strategy](#19-load-balancing-strategy)&#10;- [Monitoring and Alerting](#20-monitoring-and-alerting)&#10;- [Additional Considerations and Patterns](#21-additional-considerations-and-patterns)&#10;- [Conclusion](#conclusion)&#10;&#10;This document provides a comprehensive, step-by-step breakdown of how to architect a YouTube-scale system, covering requirements, high-level architecture, scaling strategies, microservices patterns, storage, processing pipelines, monitoring, security, and more. The goal is to demonstrate a practical, modern approach to building and scaling a global video platform, suitable for system design interviews.&#10;&#10;## Technical &amp; Business Requirements&#10;&#10;The **platform** must _enable_ **users** to _upload_, _manage_, and _stream_ **video content** at **scale**, _supporting_ a seamless **experience** across **devices** and **geographies**. **Users** should _be able to create_ personal or branded **channels**, _publish_ **videos** with **metadata** (**title**, **description**, **tags**), and _interact_ with **content** through **likes**, **comments**, and **shares**. The **system** must _support_ **user authentication**, personalized **recommendations**, and real-time **notifications** for **subscriptions** and **interactions**. **Search functionality** should _allow_ **users** to _discover_ **content** based on **relevance**, **popularity**, and **preferences**.&#10;&#10;From a **business perspective**, the **system** must _ensure_ high **availability**, **scalability**, and **performance** to _support_ billions of daily **interactions**. It should _enable_ **content moderation workflows** to _maintain_ **community standards** and _comply_ with legal **regulations**. **Analytics capabilities** must _provide_ **insights** into **user engagement**, **video performance**, and **platform health**. The **architecture** should _support_ modular **growth**, _allowing_ **teams** to independently _develop_ and _deploy_ **features** aligned with **business capabilities** such as **video management**, **user engagement**, and **content discovery**.&#10;# Nouns and Verbs from Video Platform Requirements&#10;&#10;## Nouns&#10;&#10;| Noun | Category | Description |&#10;|------|----------|-------------|&#10;| platform | System | The main video streaming system |&#10;| users | People | End users of the platform |&#10;| video content | Media | Videos uploaded to the platform |&#10;| scale | Concept | Large-scale operations |&#10;| experience | Concept | User interaction quality |&#10;| devices | Hardware | User devices (mobile, desktop, etc.) |&#10;| geographies | Location | Global regions |&#10;| channels | Feature | User/brand content channels |&#10;| videos | Media | Individual video files |&#10;| metadata | Data | Video information (title, description, tags) |&#10;| title | Data | Video title |&#10;| description | Data | Video description |&#10;| tags | Data | Video categorization tags |&#10;| content | Media | General platform content |&#10;| likes | Interaction | User approval actions |&#10;| comments | Interaction | User feedback/discussion |&#10;| shares | Interaction | Content sharing actions |&#10;| system | Infrastructure | Platform technical system |&#10;| user authentication | Security | Login/identity verification |&#10;| recommendations | Feature | Personalized content suggestions |&#10;| notifications | Feature | Real-time user alerts |&#10;| subscriptions | Feature | Channel following system |&#10;| interactions | Activity | User engagement activities |&#10;| search functionality | Feature | Content discovery tool |&#10;| relevance | Algorithm | Search ranking factor |&#10;| popularity | Algorithm | Content popularity metric |&#10;| preferences | User Data | User personal choices |&#10;| business perspective | Viewpoint | Commercial considerations |&#10;| availability | Performance | System uptime |&#10;| scalability | Performance | Growth handling capability |&#10;| performance | Quality | System speed/efficiency |&#10;| content moderation workflows | Process | Content review processes |&#10;| community standards | Policy | Platform usage rules |&#10;| regulations | Legal | Legal compliance requirements |&#10;| analytics capabilities | Feature | Data analysis tools |&#10;| insights | Data | Business intelligence |&#10;| user engagement | Metrics | User activity measurements |&#10;| video performance | Metrics | Video success metrics |&#10;| platform health | Metrics | System status indicators |&#10;| architecture | Infrastructure | System design structure |&#10;| growth | Business | Platform expansion |&#10;| teams | People | Development teams |&#10;| features | Functionality | Platform capabilities |&#10;| business capabilities | Concept | Core business functions |&#10;| video management | Capability | Video handling processes |&#10;| user engagement | Capability | User interaction systems |&#10;| content discovery | Capability | Content finding mechanisms |&#10;&#10;## Verbs&#10;&#10;| Verb | Type | Usage Context |&#10;|------|------|---------------|&#10;| enable | Transitive | Allow functionality |&#10;| upload | Transitive | Transfer video files |&#10;| manage | Transitive | Control/organize content |&#10;| stream | Transitive | Deliver video content |&#10;| supporting | Present Participle | Providing assistance |&#10;| create | Transitive | Make new channels |&#10;| publish | Transitive | Make videos available |&#10;| interact | Intransitive | Engage with content |&#10;| support | Transitive | Provide functionality |&#10;| allow | Transitive | Permit actions |&#10;| discover | Transitive | Find content |&#10;| ensure | Transitive | Guarantee outcomes |&#10;| enable | Transitive | Make possible |&#10;| maintain | Transitive | Keep standards |&#10;| comply | Intransitive | Follow regulations |&#10;| provide | Transitive | Supply insights |&#10;| support | Transitive | Enable growth |&#10;| allowing | Present Participle | Permitting teams |&#10;| develop | Transitive | Create features |&#10;| deploy | Transitive | Release features |&#10;&#10;Key DDD Concepts Identified:&#10;&#10;- Entities: Things with identity that change over time (Users, Videos, Channels, Comments)&#10;- Value Objects: Immutable descriptors (Metadata, Tags, Preferences)&#10;- Domain Events: Significant business occurrences (VideoUploaded, UserSubscribed)&#10;- Domain Services: Business logic that doesn't belong to entities (Search, Moderation)&#10;- Aggregates: Consistency boundaries (Video + Metadata, User + Channels)&#10;&#10;8 Bounded Contexts emerged naturally from the requirements:&#10;&#10;- User Management&#10;- Content Management&#10;- Engagement&#10;- Discovery&#10;- Notification&#10;- Analytics&#10;- Moderation&#10;- Content Delivery&#10;&#10;Ubiquitous Language: Defined key terms that should be used consistently across the development team and business stakeholders.&#10;This DDD analysis provides a much stronger foundation for:&#10;&#10;Microservices architecture design&#10;- Team organization&#10;- API boundaries&#10;- Database design&#10;- Business rule implementation&#10;&#10;&#10;## 1. Requirements Gathering&#10;&#10;### Functional Requirements&#10;- **Video Upload**: Users can upload videos (multiple formats, sizes up to 10GB)&#10;- **Video Streaming**: Users can watch videos with adaptive bitrate streaming&#10;- **Video Search**: Search videos by title, description, tags, channel&#10;- **User Management**: Registration, authentication, profiles, subscriptions&#10;- **Social Features**: Comments, likes/dislikes, sharing, playlists&#10;- **Channel Management**: Create channels, manage content, analytics&#10;- **Content Moderation**: Automated and manual content review&#10;- **Notifications**: Subscription updates, comment replies, trending content&#10;&#10;### Non-Functional Requirements&#10;#### User Scale&#10;- **Total registered users:** 2+ billion&#10;- **Daily active users (DAU):** 500+ million&#10;- **Monthly active users (MAU):** 1.5+ billion&#10;- **Concurrent users (peak):** 50+ million&#10;&#10;#### System Throughput&#10;- **Video uploads:** 500+ hours of content uploaded per minute&#10;- **Video consumption:** 1+ billion hours watched daily&#10;- **Search queries:** 1+ billion queries per day&#10;&#10;#### Availability &amp; Reliability&#10;- **Uptime SLA:** 99.9% globally&#10;- **CAP Theorem priority:**&#10;  - Prioritize **Availability** and **Partition Tolerance**&#10;  - Accept **Eventual Consistency** where applicable&#10;&#10;#### Performance &amp; Latency&#10;- **Video start time:** &lt; 2 seconds globally&#10;- **Search results latency:** &lt; 300 milliseconds&#10;- **Upload processing time:** Variable (based on video size and format)&#10;- **Page load time (95th percentile):** &lt; 1 second&#10;&#10;#### Consistency &amp; Data Integrity&#10;- **User authentication &amp; billing:** Strong consistency&#10;- **Social features (likes, comments, views):** Eventually consistent&#10;- **Content metadata updates:** Eventually consistent&#10;&#10;#### Storage &amp; Data Management&#10;- **Video storage:** Exabyte-scale, globally distributed&#10;- **Metadata storage:** Highly available, low-latency key-value store&#10;- **Backup &amp; disaster recovery:** Geo-redundant with RPO &lt; 1 hour&#10;&#10;#### Bandwidth &amp; Traffic&#10;- **Daily data transfer:** Petabyte-scale&#10;- **Peak bandwidth usage:** 100+ Tbps globally&#10;- **CDN usage:** Aggressive caching and edge delivery for video content&#10;&#10;#### Security &amp; Compliance&#10;- **Data encryption:** At rest and in transit&#10;- **Access control:** Role-based and region-aware&#10;- **Compliance:** GDPR, CCPA, and other regional regulations&#10;&#10;### Scale Estimation&#10;- **Users**: 2.7B monthly active users&#10;- **Videos**: 720,000 hours uploaded daily&#10;- **Storage**: 1PB+ new content daily&#10;- **Bandwidth**: 30PB+ daily egress traffic&#10;- **QPS**: 1M+ concurrent video streams&#10;&#10;## 2. High-Level Architecture&#10;&#10;```&#10;[CDN Layer] → [Load Balancers] → [API Gateway] → [Microservices]&#10;                                                      ↓&#10;[Message Queue] ← [Video Processing Pipeline] ← [Object Storage]&#10;                                                      ↓&#10;[Search Engine] ← [Metadata Services] → [Analytics Pipeline]&#10;```&#10;&#10;### Core Components&#10;1. **API Gateway** - Request routing, authentication, rate limiting&#10;2. **Video Upload Service** - Handle video ingestion and initial processing&#10;3. **Video Processing Pipeline** - Transcoding, thumbnail generation, ML analysis&#10;4. **Video Streaming Service** - Adaptive bitrate delivery&#10;5. **Metadata Service** - Video information, user data, social interactions&#10;6. **Search Service** - Video discovery and recommendation&#10;7. **User Service** - Authentication, profiles, subscriptions&#10;8. **Analytics Service** - View tracking, performance metrics&#10;9. **Notification Service** - Real-time updates and alerts&#10;&#10;## 2a. Microservice Decomposition &amp; Hexagonal Architecture (Chris Richardson)&#10;&#10;### Decomposition Strategies (from &quot;Microservices Patterns&quot;)&#10;- **By Business Capability**: Decompose services around business domains (e.g., Video Management, User Management, Social Interactions, Analytics).&#10;- **By Subdomain (DDD)**: Identify core, supporting, and generic subdomains (e.g., Video Processing as core, Notification as supporting).&#10;- **By Transaction Boundary**: Services should own their data and transactional boundaries (e.g., Video Upload and Processing as separate services).&#10;- **By Team Ownership**: Align services with team boundaries for independent delivery.&#10;&#10;### Hexagonal Architecture (Ports &amp; Adapters)&#10;- **Service Core**: Business logic is isolated from external systems.&#10;- **Ports**: Define interfaces for driving (API, UI) and driven (DB, messaging, external APIs) adapters.&#10;- **Adapters**: Implement ports for REST, gRPC, Kafka, databases, etc.&#10;- **Benefits**: Improves testability, flexibility, and separation of concerns.&#10;&#10;#### Example: Video Upload Service (Hexagonal)&#10;- **Core**: Handles upload validation, metadata extraction, and orchestration.&#10;- **Inbound Adapter**: REST API for receiving uploads.&#10;- **Outbound Adapters**: Kafka producer for events, S3 adapter for storage, DB adapter for metadata.&#10;&#10;### Additional Patterns from the Book&#10;- **API Composition**: Aggregate data from multiple services for UI.&#10;- **Database per Service**: Each service owns its schema.&#10;- **Saga Pattern**: Manage distributed transactions (e.g., video upload workflow).&#10;- **CQRS**: Separate read/write models for scalability.&#10;- **Event Sourcing**: Persist state changes as events for auditability.&#10;- **Service Mesh**: Use a service mesh (e.g., Istio, Linkerd) for managing service-to-service communication, observability, and security.&#10;- **Externalized Configuration**: Store configuration outside the service for flexibility and easier deployments.&#10;- **Centralized Log Management**: Aggregate logs for all services to enable monitoring and troubleshooting.&#10;- **Health Check API**: Each service exposes a health check endpoint for orchestration and monitoring.&#10;- **Consumer-Driven Contracts**: Ensure service integrations are reliable and changes do not break consumers.&#10;- **Testing Strategies**: Include contract testing, component testing, and end-to-end testing for microservices.&#10;- **Distributed Tracing**: Implement tracing to follow requests across service boundaries for observability.&#10;- **Service Template/Boilerplate**: Use standardized templates to accelerate new service development and enforce best practices.&#10;&#10;## 3. Scale Cube Application for 10x Growth&#10;&#10;### X-Axis Scaling (Horizontal Duplication)&#10;- **Load Balancers**: Deploy multiple tiers (L4/L7) with auto-scaling&#10;- **API Gateway Clusters**: Regional deployment with intelligent routing&#10;- **Microservice Replicas**: Auto-scaling based on CPU, memory, and queue depth&#10;- **Database Read Replicas**: Multiple read-only instances per region&#10;&#10;### Y-Axis Scaling (Functional Decomposition)&#10;- **Service Decomposition**:&#10;  - Upload Service → Video Ingestion + Metadata Extraction + Storage&#10;  - User Service → Auth + Profile + Subscription + Preferences&#10;  - Social Service → Comments + Likes + Sharing + Community&#10;- **Database Decomposition**: Separate DBs for videos, users, analytics, social&#10;- **Event-Driven Architecture**: Loose coupling via message queues&#10;&#10;### Z-Axis Scaling (Data Partitioning)&#10;- **Video Sharding**: By video ID hash, geographic region, or creator&#10;- **User Sharding**: By user ID hash or geographic region&#10;- **Temporal Sharding**: Hot data (recent) vs cold data (archived)&#10;- **Content-Based Sharding**: By video category, language, or popularity&#10;&#10;## 4. Microservices Design Patterns&#10;&#10;### Service Patterns&#10;- **API Gateway Pattern**: Single entry point with cross-cutting concerns&#10;- **Service Registry &amp; Discovery**: Consul/Eureka for service location&#10;- **Circuit Breaker**: Hystrix for fault tolerance and cascading failure prevention&#10;- **Bulkhead**: Resource isolation between services&#10;- **Retry with Exponential Backoff**: Resilient inter-service communication&#10;&#10;### Data Patterns&#10;- **Database per Service**: Each microservice owns its data&#10;- **Saga Pattern**: Distributed transactions for video upload workflow&#10;- **CQRS**: Separate read/write models for video metadata and analytics&#10;- **Event Sourcing**: Audit trail for user actions and video lifecycle&#10;&#10;### Communication Patterns&#10;- **Asynchronous Messaging**: Kafka for video processing pipeline&#10;- **Request-Response**: HTTP/gRPC for real-time user interactions&#10;- **Publish-Subscribe**: Event notifications for subscriptions&#10;- **Message Routing**: Content-based routing for different video types&#10;&#10;- **Hexagonal Architecture**: Each service is designed using ports and adapters, isolating business logic from infrastructure.&#10;- **Decomposition by Business Capability**: Services are split by domain, following DDD and team boundaries.&#10;- **Saga Pattern**: Used for workflows like video upload and processing.&#10;- **CQRS &amp; Event Sourcing**: Applied for scalability and auditability.&#10;&#10;## 5. Event-Driven Architecture (EDA)&#10;&#10;### Event Streaming Platform&#10;```&#10;Video Upload → [Event Producer] → [Kafka Topics] → [Event Consumers] → Processing Services&#10;```&#10;&#10;### Core Events&#10;- **VideoUploadedEvent**: Triggers transcoding pipeline&#10;- **VideoProcessedEvent**: Updates metadata and makes video available&#10;- **UserActionEvent**: Likes, comments, views for recommendation engine&#10;- **SubscriptionEvent**: Channel subscription/unsubscription&#10;- **ModerationEvent**: Content review results&#10;&#10;### Event Patterns&#10;- **Event Sourcing**: Store all state changes as events&#10;- **CQRS**: Separate command and query responsibility&#10;- **Event Choreography**: Services react to events autonomously&#10;- **Event Orchestration**: Central coordinator for complex workflows&#10;&#10;## 6. CAP Theorem Considerations&#10;&#10;### Design Decisions&#10;- **Partition Tolerance**: Always required in distributed system&#10;- **Availability vs Consistency Trade-offs**:&#10;  - **AP Systems**: Video streaming, comments, likes (eventual consistency)&#10;  - **CP Systems**: User authentication, payment processing&#10;  - **CA Systems**: Single-region components with strong consistency&#10;&#10;### Implementation Strategy&#10;- **Multi-Region Deployment**: Handle network partitions&#10;- **Eventual Consistency**: Social features can tolerate temporary inconsistency&#10;- **Strong Consistency**: Critical operations like user authentication&#10;- **Conflict Resolution**: Last-writer-wins, vector clocks for concurrent updates&#10;&#10;## 7. Storage Architecture&#10;&#10;### Video Storage&#10;- **Object Storage**: S3/GCS for raw and processed video files&#10;- **CDN**: CloudFront/CloudFlare for global content delivery&#10;- **Storage Tiers**: Hot (recent), warm (popular), cold (archived)&#10;- **Compression**: AV1 codec for 30% bandwidth savings&#10;&#10;### Metadata Storage&#10;- **Relational Database**: PostgreSQL for structured data (users, videos)&#10;- **Document Database**: MongoDB for flexible schemas (comments, analytics)&#10;- **Graph Database**: Neo4j for social relationships and recommendations&#10;- **Cache Layer**: Redis for frequently accessed data&#10;&#10;### Search Index&#10;- **Elasticsearch**: Full-text search for videos, channels, playlists&#10;- **Vector Database**: Pinecone for ML-based video recommendations&#10;- **Real-time Indexing**: Stream processing for immediate search availability&#10;&#10;## 8. Video Processing Pipeline&#10;&#10;### Processing Stages&#10;1. **Ingestion**: Upload validation, virus scanning, metadata extraction&#10;2. **Transcoding**: Multiple resolutions, formats, and bitrates&#10;3. **AI Processing**: Content analysis, thumbnail generation, closed captions&#10;4. **Quality Check**: Automated quality assessment and optimization&#10;5. **Distribution**: CDN upload and cache warming&#10;&#10;### Technologies&#10;- **Message Queue**: Apache Kafka for pipeline orchestration&#10;- **Container Orchestration**: Kubernetes for scalable processing&#10;- **Workflow Engine**: Apache Airflow for complex processing workflows&#10;- **ML Platform**: TensorFlow Serving for content analysis&#10;&#10;## 9. Scaling Strategies for 10x Growth&#10;&#10;### Infrastructure Scaling&#10;- **Multi-Cloud**: AWS, GCP, Azure for redundancy and cost optimization&#10;- **Edge Computing**: Process videos closer to users&#10;- **Serverless**: Lambda/Cloud Functions for variable workloads&#10;- **Auto-scaling**: Predictive scaling based on usage patterns&#10;&#10;### Performance Optimization&#10;- **Caching Strategy**: &#10;  - L1: Browser cache (static content)&#10;  - L2: CDN cache (popular videos)&#10;  - L3: Application cache (metadata)&#10;  - L4: Database cache (query results)&#10;&#10;### Data Management&#10;- **Data Archiving**: Move old content to cheaper storage tiers&#10;- **Data Compression**: Advanced codecs and compression algorithms&#10;- **Smart Prefetching**: ML-based content prediction and caching&#10;- **Geographic Optimization**: Content placement based on user location&#10;&#10;## 10. Monitoring and Observability&#10;&#10;### Metrics&#10;- **Golden Signals**: Latency, traffic, errors, saturation&#10;- **Business Metrics**: Video start failures, buffering ratio, user engagement&#10;- **Infrastructure Metrics**: CPU, memory, network, storage utilization&#10;&#10;### Tools&#10;- **Monitoring**: Prometheus, Grafana, DataDog&#10;- **Logging**: ELK Stack (Elasticsearch, Logstash, Kibana)&#10;- **Tracing**: Jaeger, Zipkin for distributed tracing&#10;- **Alerting**: PagerDuty for incident management&#10;&#10;## 11. Security Considerations&#10;&#10;### Content Security&#10;- **DRM**: Widevine, FairPlay for premium content protection&#10;- **Content Filtering**: ML-based inappropriate content detection&#10;- **Access Control**: JWT tokens, OAuth 2.0, rate limiting&#10;&#10;### Infrastructure Security&#10;- **Network Security**: VPC, security groups, WAF&#10;- **Encryption**: TLS in transit, AES-256 at rest&#10;- **Secrets Management**: HashiCorp Vault, AWS Secrets Manager&#10;- **Compliance**: GDPR, COPPA, regional data protection laws&#10;&#10;## 12. Cost Optimization&#10;&#10;### Storage Optimization&#10;- **Intelligent Tiering**: Automatic movement between storage classes&#10;- **Deduplication**: Remove duplicate video segments&#10;- **Compression**: Advanced codecs (AV1, H.265) for bandwidth savings&#10;- **Regional Optimization**: Store content closer to primary audience&#10;&#10;### Compute Optimization&#10;- **Spot Instances**: Use for batch processing jobs&#10;- **Right-sizing**: ML-based instance size recommendations&#10;- **Reserved Capacity**: Long-term commitments for predictable workloads&#10;- **Serverless**: Pay-per-use for variable workloads&#10;&#10;## 13. Disaster Recovery and Business Continuity&#10;&#10;### Backup Strategy&#10;- **Multi-Region Replication**: Critical data replicated across regions&#10;- **Point-in-Time Recovery**: Database snapshots and transaction logs&#10;- **Content Backup**: Multiple copies of popular content&#10;&#10;### Recovery Procedures&#10;- **RTO (Recovery Time Objective)**: 15 minutes for critical services&#10;- **RPO (Recovery Point Objective)**: 5 minutes for user data&#10;- **Failover Automation**: Automated traffic rerouting during outages&#10;- **Chaos Engineering**: Regular disaster simulations&#10;&#10;## 14. Key Principles and Laws Applied&#10;&#10;### Performance Laws&#10;- **Little's Law**: Average number of items in a queuing system equals the average arrival rate multiplied by the average time an item spends in the system.&#10;- **Amdahl's Law**: The speedup of a program from parallelization is limited by the sequential portion.&#10;- **Universal Scalability Law**: Models the impact of contention and coherency delays in distributed systems.&#10;&#10;### Design Principles&#10;- **Single Responsibility Principle**: Each service has one clear purpose.&#10;- **Open/Closed Principle**: Services should be open for extension, closed for modification.&#10;- **Dependency Inversion Principle**: Depend on abstractions, not concretions.&#10;- **Fail Fast**: Detect and report errors immediately.&#10;&#10;### Reliability Patterns&#10;- **Bulkhead**: Isolate resources to prevent cascading failures.&#10;- **Circuit Breaker**: Prevent calls to failing services.&#10;- **Timeout**: Set maximum wait times for all operations.&#10;- **Idempotency**: Safe to retry operations multiple times.&#10;&#10;### Additional Laws and Principles&#10;- **Murphy's Law**: &quot;Anything that can go wrong will go wrong.&quot; Design for failure and recovery.&#10;- **Conway's Law**: System design mirrors the communication structure of the organization.&#10;- **Occam's Razor**: Prefer the simplest solution that works.&#10;- **Robustness Principle (Postel's Law)**: &quot;Be conservative in what you send, be liberal in what you accept.&quot;&#10;- **Law of Demeter**: Minimize coupling by only interacting with immediate collaborators.&#10;- **Hofstadter's Law**: &quot;It always takes longer than you expect, even when you take into account Hofstadter's Law.&quot;&#10;- **Pareto Principle (80/20 Rule)**: 80% of effects come from 20% of causes; optimize for the critical path.&#10;- **Peter Principle**: In hierarchical organizations, people tend to be promoted to their level of incompetence (impacts team/org design).&#10;- **Gall's Law**: A complex system that works is invariably found to have evolved from a simple system that worked.&#10;&#10;## 15. Database Design&#10;&#10;### User Service Database&#10;```sql&#10;-- Users table&#10;CREATE TABLE users (&#10;    user_id BIGINT PRIMARY KEY,&#10;    username VARCHAR(50) UNIQUE NOT NULL,&#10;    email VARCHAR(255) UNIQUE NOT NULL,&#10;    password_hash VARCHAR(255) NOT NULL,&#10;    created_at TIMESTAMP DEFAULT NOW(),&#10;    last_login TIMESTAMP&#10;);&#10;&#10;-- Channels table&#10;CREATE TABLE channels (&#10;    channel_id BIGINT PRIMARY KEY,&#10;    user_id BIGINT REFERENCES users(user_id),&#10;    channel_name VARCHAR(100) NOT NULL,&#10;    description TEXT,&#10;    subscriber_count BIGINT DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;&#10;-- Subscriptions table (sharded by user_id)&#10;CREATE TABLE subscriptions (&#10;    subscription_id BIGINT PRIMARY KEY,&#10;    subscriber_id BIGINT REFERENCES users(user_id),&#10;    channel_id BIGINT REFERENCES channels(channel_id),&#10;    subscribed_at TIMESTAMP DEFAULT NOW(),&#10;    UNIQUE(subscriber_id, channel_id)&#10;);&#10;```&#10;&#10;### Video Service Database&#10;```sql&#10;-- Videos table (sharded by video_id hash)&#10;CREATE TABLE videos (&#10;    video_id BIGINT PRIMARY KEY,&#10;    channel_id BIGINT NOT NULL,&#10;    title VARCHAR(255) NOT NULL,&#10;    description TEXT,&#10;    duration INTEGER, -- in seconds&#10;    view_count BIGINT DEFAULT 0,&#10;    like_count BIGINT DEFAULT 0,&#10;    dislike_count BIGINT DEFAULT 0,&#10;    upload_time TIMESTAMP DEFAULT NOW(),&#10;    processing_status ENUM('uploading', 'processing', 'ready', 'failed'),&#10;    visibility ENUM('public', 'private', 'unlisted')&#10;);&#10;&#10;-- Video metadata table&#10;CREATE TABLE video_metadata (&#10;    video_id BIGINT PRIMARY KEY REFERENCES videos(video_id),&#10;    file_size BIGINT,&#10;    codec VARCHAR(50),&#10;    resolution VARCHAR(20),&#10;    bitrate INTEGER,&#10;    thumbnail_url VARCHAR(500),&#10;    tags TEXT[] -- PostgreSQL array for tags&#10;);&#10;&#10;-- Comments table (sharded by video_id)&#10;CREATE TABLE comments (&#10;    comment_id BIGINT PRIMARY KEY,&#10;    video_id BIGINT NOT NULL,&#10;    user_id BIGINT NOT NULL,&#10;    parent_comment_id BIGINT, -- for replies&#10;    content TEXT NOT NULL,&#10;    like_count INTEGER DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;```&#10;&#10;## 16. API Design&#10;&#10;### REST API Endpoints&#10;&#10;#### Video Operations&#10;```http&#10;# Upload video&#10;POST /api/v1/videos&#10;Content-Type: multipart/form-data&#10;&#10;# Get video details&#10;GET /api/v1/videos/{videoId}&#10;&#10;# Update video metadata&#10;PUT /api/v1/videos/{videoId}&#10;&#10;# Delete video&#10;DELETE /api/v1/videos/{videoId}&#10;&#10;# Search videos&#10;GET /api/v1/videos/search?q={query}&amp;limit={limit}&amp;offset={offset}&#10;&#10;# Get trending videos&#10;GET /api/v1/videos/trending?category={category}&amp;region={region}&#10;```&#10;&#10;#### User Operations&#10;```http&#10;# User registration&#10;POST /api/v1/users/register&#10;&#10;# User login&#10;POST /api/v1/users/login&#10;&#10;# Get user profile&#10;GET /api/v1/users/{userId}&#10;&#10;# Subscribe to channel&#10;POST /api/v1/users/{userId}/subscriptions/{channelId}&#10;&#10;# Get user subscriptions&#10;GET /api/v1/users/{userId}/subscriptions&#10;```&#10;&#10;#### Social Operations&#10;```http&#10;# Like/Unlike video&#10;POST /api/v1/videos/{videoId}/like&#10;DELETE /api/v1/videos/{videoId}/like&#10;&#10;# Add comment&#10;POST /api/v1/videos/{videoId}/comments&#10;&#10;# Get comments&#10;GET /api/v1/videos/{videoId}/comments?limit={limit}&amp;offset={offset}&#10;&#10;# Reply to comment&#10;POST /api/v1/comments/{commentId}/replies&#10;```&#10;&#10;### GraphQL Schema (Alternative)&#10;```graphql&#10;type Video {&#10;  id: ID!&#10;  title: String!&#10;  description: String&#10;  duration: Int!&#10;  viewCount: Int!&#10;  likeCount: Int!&#10;  uploadTime: String!&#10;  channel: Channel!&#10;  comments(first: Int, after: String): CommentConnection&#10;}&#10;&#10;type Channel {&#10;  id: ID!&#10;  name: String!&#10;  subscriberCount: Int!&#10;  videos(first: Int, after: String): VideoConnection&#10;}&#10;&#10;type Query {&#10;  video(id: ID!): Video&#10;  searchVideos(query: String!, first: Int, after: String): VideoConnection&#10;  trendingVideos(category: String, region: String): [Video!]!&#10;}&#10;&#10;type Mutation {&#10;  uploadVideo(input: VideoInput!): Video&#10;  likeVideo(videoId: ID!): Video&#10;  addComment(videoId: ID!, content: String!): Comment&#10;}&#10;```&#10;&#10;## 17. Caching Strategy&#10;&#10;### Multi-Level Caching&#10;```yaml&#10;# Level 1: Browser Cache&#10;- Static assets: 1 year&#10;- Video thumbnails: 1 week&#10;- API responses: 5 minutes&#10;&#10;# Level 2: CDN Cache (CloudFlare/CloudFront)&#10;- Video segments: 1 day&#10;- Thumbnails: 1 week&#10;- API responses: 1 minute&#10;&#10;# Level 3: Application Cache (Redis)&#10;- Popular video metadata: 1 hour&#10;- User sessions: 24 hours&#10;- Search results: 15 minutes&#10;- Trending videos: 30 minutes&#10;&#10;# Level 4: Database Query Cache&#10;- Complex analytics queries: 5 minutes&#10;- User profile data: 30 minutes&#10;- Channel information: 1 hour&#10;```&#10;&#10;### Cache Invalidation Strategy&#10;- **Time-based**: TTL for most cached data&#10;- **Event-based**: Invalidate on video updates, user actions&#10;- **Version-based**: Cache keys include version numbers&#10;- **Write-through**: Update cache and database simultaneously&#10;- **Write-behind**: Async cache updates for non-critical data&#10;&#10;## 18. Message Queue Architecture&#10;&#10;### Kafka Topic Design&#10;```yaml&#10;# Video Processing Topics&#10;video-upload-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 7 days&#10;&#10;video-transcoding-jobs:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 3 days&#10;&#10;video-ready-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# User Activity Topics&#10;user-view-events:&#10;  partitions: 200&#10;  replication-factor: 3&#10;  retention: 90 days&#10;&#10;user-interaction-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# Notification Topics&#10;subscription-notifications:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 7 days&#10;```&#10;&#10;### Event Schema (Avro)&#10;```json&#10;{&#10;  &quot;type&quot;: &quot;record&quot;,&#10;  &quot;name&quot;: &quot;VideoUploadEvent&quot;,&#10;  &quot;fields&quot;: [&#10;    {&quot;name&quot;: &quot;videoId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;userId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;channelId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;filename&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;fileSize&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;contentType&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;uploadTimestamp&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;metadata&quot;, &quot;type&quot;: {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;string&quot;}}&#10;  ]&#10;}&#10;```&#10;&#10;## 19. Load Balancing Strategy&#10;&#10;### Geographic Load Balancing&#10;```yaml&#10;# DNS-based routing&#10;Global Load Balancer:&#10;  - US-East: 40% traffic&#10;  - US-West: 20% traffic&#10;  - Europe: 25% traffic&#10;  - Asia-Pacific: 15% traffic&#10;&#10;# Regional Load Balancers&#10;Regional LB (L7):&#10;  - Path-based routing: /api/upload → Upload Service&#10;  - Header-based routing: User-Agent → Mobile/Web Service&#10;  - Weighted routing: Canary deployments&#10;&#10;# Service Load Balancers (L4)&#10;Service Discovery:&#10;  - Health checks every 30 seconds&#10;  - Circuit breaker: 5 failures in 60 seconds&#10;  - Load balancing algorithms: Weighted round-robin&#10;```&#10;&#10;### Auto-scaling Configuration&#10;```yaml&#10;# Horizontal Pod Autoscaler (Kubernetes)&#10;Video Upload Service:&#10;  minReplicas: 10&#10;  maxReplicas: 100&#10;  targetCPUUtilization: 70%&#10;  targetMemoryUtilization: 80%&#10;  scaleUpStabilization: 60s&#10;  scaleDownStabilization: 300s&#10;&#10;Video Streaming Service:&#10;  minReplicas: 50&#10;  maxReplicas: 500&#10;  targetCPUUtilization: 60%&#10;  customMetrics:&#10;    - concurrent_streams_per_pod: 1000&#10;```&#10;&#10;## 20. Monitoring and Alerting&#10;&#10;### Key Metrics Dashboard&#10;```yaml&#10;# Golden Signals&#10;Latency:&#10;  - Video start time: P50, P95, P99&#10;  - API response time: P50, P95, P99&#10;  - Upload processing time: P50, P95, P99&#10;&#10;Traffic:&#10;  - Requests per second by endpoint&#10;  - Concurrent video streams&#10;  - Upload requests per minute&#10;&#10;Errors:&#10;  - HTTP error rates (4xx, 5xx)&#10;  - Video processing failures&#10;  - Database connection errors&#10;&#10;Saturation:&#10;  - CPU utilization across services&#10;  - Memory usage patterns&#10;  - Queue depth in Kafka topics&#10;  - Storage utilization&#10;```&#10;&#10;### Alert Rules&#10;```yaml&#10;# Critical Alerts (PagerDuty)&#10;Video Start Failure Rate &gt; 1%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;API Error Rate &gt; 5%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;Database Connection Pool &gt; 90%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;# Warning Alerts (Slack)&#10;Upload Processing Time &gt; P95:&#10;  severity: warning&#10;  notification: 5-minute delay&#10;&#10;CDN Cache Hit Rate &lt; 85%:&#10;  severity: warning&#10;  notification: 10-minute delay&#10;```&#10;&#10;## 21. Additional Considerations and Patterns&#10;&#10;### Service Governance &amp; Platform Engineering&#10;- **API Gateway Governance**: Centralized API management, versioning, and security policies.&#10;- **Service Catalog**: Maintain a registry of all services, their owners, and documentation.&#10;- **Platform as a Service (PaaS)**: Internal developer platforms for rapid service deployment and consistency.&#10;- **Service Lifecycle Management**: Automated onboarding, deprecation, and retirement of services.&#10;&#10;### Observability &amp; Operations&#10;- **Distributed Tracing**: End-to-end request tracing across all services.&#10;- **Log Correlation**: Correlate logs, traces, and metrics for faster root cause analysis.&#10;- **Proactive Anomaly Detection**: ML-based monitoring for early detection of issues.&#10;- **Feature Flags**: Gradual rollout and rollback of features without redeployments.&#10;&#10;### Security Enhancements&#10;- **Zero Trust Security Model**: Authenticate and authorize every request, even within the internal network.&#10;- **API Rate Limiting &amp; Throttling**: Prevent abuse and ensure fair usage.&#10;- **Audit Logging**: Immutable logs for all sensitive operations and access.&#10;&#10;### Compliance &amp; Data Privacy&#10;- **Data Residency Controls**: Ensure data is stored and processed in compliance with regional laws.&#10;- **User Data Portability &amp; Deletion**: GDPR-compliant APIs for user data export and erasure.&#10;- **PII Masking**: Mask personally identifiable information in logs and analytics.&#10;&#10;### Advanced Data &amp; ML&#10;- **Real-Time Analytics**: Stream processing for instant insights (e.g., Apache Flink).&#10;- **A/B Testing Platform**: Experiment with new features and recommendation algorithms.&#10;- **Personalization Engine**: ML-driven recommendations, thumbnails, and notifications.&#10;- **Content Fingerprinting**: Detect duplicate or copyrighted content at scale.&#10;&#10;### Reliability Engineering&#10;- **Self-Healing Infrastructure**: Automated detection and remediation of failed components.&#10;- **Chaos Engineering**: Regularly inject failures to test system resilience.&#10;- **Disaster Recovery Drills**: Scheduled simulations to validate recovery procedures.&#10;&#10;### Developer Experience&#10;- **Local Development Environments**: Simulate production-like environments for testing.&#10;- **Automated Code Quality Checks**: Linting, static analysis, and security scanning in CI/CD.&#10;- **Comprehensive Documentation**: Up-to-date API, architecture, and operational docs.&#10;&#10;### Sustainability &amp; Cost Controls&#10;- **Green Computing Initiatives**: Optimize for energy efficiency and carbon footprint.&#10;- **Cost Visibility Dashboards**: Real-time cost tracking per service and feature.&#10;- **Resource Quotas**: Prevent runaway costs by enforcing limits.&#10;&#10;### Community &amp; Ecosystem&#10;- **Open API Standards**: Support for OpenAPI/GraphQL for third-party integrations.&#10;- **Plugin/Extension Framework**: Allow external developers to extend platform capabilities.&#10;- **Community Moderation Tools**: Empower users to help maintain content quality.&#10;&#10;## Conclusion&#10;&#10;This comprehensive YouTube system design demonstrates how to architect a massive-scale video platform that can handle billions of users and petabytes of content. The design incorporates modern distributed systems principles, microservices architecture, event-driven patterns, and advanced scaling techniques.&#10;&#10;The 10x scaling strategy leverages the Scale Cube dimensions, applies CAP theorem principles strategically, and utilizes cutting-edge technologies to ensure the platform remains performant, reliable, and cost-effective as it grows. The architecture is designed to be resilient, observable, and maintainable while providing an excellent user experience globally." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/themes/gravionis/layouts/_partials/header.html">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/themes/gravionis/layouts/_partials/header.html" />
              <option name="originalContent" value="&lt;div style=&quot;position: fixed; top: 0; left: 0; height: 100vh; width: 200px; background: #fff; box-shadow: 2px 0 4px rgba(0,0,0,0.03); z-index: 201; display: flex; flex-direction: column; align-items: flex-start;&quot;&gt;&#10;  &lt;h1 style=&quot;margin: 1.5rem 0 0 1rem; font-size: 1.5rem;&quot;&gt;{{ site.Title }}&lt;/h1&gt;&#10;  &lt;nav style=&quot;width: 100%; margin-top: 2rem;&quot;&gt;&#10;    {{ partial &quot;menu.html&quot; (dict &quot;menuID&quot; &quot;main&quot; &quot;page&quot; .) }}&#10;  &lt;/nav&gt;&#10;&lt;/div&gt;&#10;&lt;div style=&quot;margin-left: 200px; width: 100%;&quot;&gt;&#10;  &lt;!-- Main content will be rendered here --&gt;&#10;&lt;/div&gt;&#10;&#10;" />
              <option name="updatedContent" value="&lt;div style=&quot;position: fixed; top: 0; left: 0; height: 100vh; width: 200px; background: #fff; box-shadow: 2px 0 4px rgba(0,0,0,0.03); z-index: 201; display: flex; flex-direction: column; align-items: flex-start;&quot;&gt;&#10;  &lt;h1 style=&quot;margin: 1.5rem 0 0 1rem; font-size: 1.5rem;&quot;&gt;{{ site.Title }}&lt;/h1&gt;&#10;  &lt;nav style=&quot;width: 100%; margin-top: 2rem;&quot;&gt;&#10;    {{ partial &quot;menu.html&quot; (dict &quot;menuID&quot; &quot;main&quot; &quot;page&quot; .) }}&#10;  &lt;/nav&gt;&#10;&lt;/div&gt;&#10;&lt;!-- Add extra left margin to main content to shift blogs a bit more right of the menu --&gt;&#10;&lt;style&gt;&#10;  body {&#10;    margin-left: 230px; /* Increased from 200px to 230px for more space */&#10;  }&#10;  @media (max-width: 600px) {&#10;    body {&#10;      margin-left: 0;&#10;    }&#10;    .sidebar-menu {&#10;      display: none;&#10;    }&#10;  }&#10;&lt;/style&gt;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/themes/gravionis/layouts/_partials/menu.html">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/themes/gravionis/layouts/_partials/menu.html" />
              <option name="originalContent" value="{{- /*&#10;Renders a menu for the given menu ID.&#10;&#10;@context {page} page The current page.&#10;@context {string} menuID The menu ID.&#10;&#10;@example: {{ partial &quot;menu.html&quot; (dict &quot;menuID&quot; &quot;main&quot; &quot;page&quot; .) }}&#10;*/}}&#10;&#10;{{- $page := .page }}&#10;{{- $menuID := .menuID }}&#10;&#10;{{- with index site.Menus $menuID }}&#10;  &lt;nav style=&quot;margin-top:0.5rem;&quot;&gt;&#10;    &lt;ul&gt;&#10;      {{- partial &quot;inline/menu/walk.html&quot; (dict &quot;page&quot; $page &quot;menuEntries&quot; .) }}&#10;    &lt;/ul&gt;&#10;  &lt;/nav&gt;&#10;{{- end }}&#10;&#10;{{- define &quot;_partials/inline/menu/walk.html&quot; }}&#10;  {{- $page := .page }}&#10;  {{- range .menuEntries }}&#10;    {{- $attrs := dict &quot;href&quot; .URL }}&#10;    {{- if $page.IsMenuCurrent .Menu . }}&#10;      {{- $attrs = merge $attrs (dict &quot;class&quot; &quot;active&quot; &quot;aria-current&quot; &quot;page&quot;) }}&#10;    {{- else if $page.HasMenuCurrent .Menu .}}&#10;      {{- $attrs = merge $attrs (dict &quot;class&quot; &quot;ancestor&quot; &quot;aria-current&quot; &quot;true&quot;) }}&#10;    {{- end }}&#10;    {{- $name := .Name }}&#10;    {{- with .Identifier }}&#10;      {{- with T . }}&#10;        {{- $name = . }}&#10;      {{- end }}&#10;    {{- end }}&#10;    &lt;li&gt;&#10;      &lt;a&#10;        {{- range $k, $v := $attrs }}&#10;          {{- with $v }}&#10;            {{- printf &quot; %s=%q&quot; $k $v | safeHTMLAttr }}&#10;          {{- end }}&#10;        {{- end -}}&#10;      &gt;{{ $name }}&lt;/a&gt;&#10;      {{- with .Children }}&#10;        &lt;ul&gt;&#10;          {{- partial &quot;inline/menu/walk.html&quot; (dict &quot;page&quot; $page &quot;menuEntries&quot; .) }}&#10;        &lt;/ul&gt;&#10;      {{- end }}&#10;    &lt;/li&gt;&#10;  {{- end }}&#10;{{- end }}&#10;" />
              <option name="updatedContent" value="{{- /*&#10;Renders a menu for the given menu ID.&#10;&#10;@context {page} page The current page.&#10;@context {string} menuID The menu ID.&#10;&#10;@example: {{ partial &quot;menu.html&quot; (dict &quot;menuID&quot; &quot;main&quot; &quot;page&quot; .) }}&#10;*/}}&#10;&#10;{{- $page := .page }}&#10;{{- $menuID := .menuID }}&#10;&#10;{{- with index site.Menus $menuID }}&#10;  &lt;ul style=&quot;display: flex; flex-direction: column; gap: 1rem; list-style: none; margin: 0; padding: 0 1rem;&quot;&gt;&#10;    {{- partial &quot;inline/menu/walk.html&quot; (dict &quot;page&quot; $page &quot;menuEntries&quot; .) }}&#10;  &lt;/ul&gt;&#10;{{- end }}&#10;&#10;{{- define &quot;_partials/inline/menu/walk.html&quot; }}&#10;  {{- $page := .page }}&#10;  {{- range .menuEntries }}&#10;    {{- $attrs := dict &quot;href&quot; .URL }}&#10;    {{- if $page.IsMenuCurrent .Menu . }}&#10;      {{- $attrs = merge $attrs (dict &quot;class&quot; &quot;active&quot; &quot;aria-current&quot; &quot;page&quot;) }}&#10;    {{- else if $page.HasMenuCurrent .Menu .}}&#10;      {{- $attrs = merge $attrs (dict &quot;class&quot; &quot;ancestor&quot; &quot;aria-current&quot; &quot;true&quot;) }}&#10;    {{- end }}&#10;    {{- $name := .Name }}&#10;    {{- with .Identifier }}&#10;      {{- with T . }}&#10;        {{- $name = . }}&#10;      {{- end }}&#10;    {{- end }}&#10;    &lt;li&gt;&#10;      &lt;a&#10;        {{- range $k, $v := $attrs }}&#10;          {{- with $v }}&#10;            {{- printf &quot; %s=%q&quot; $k $v | safeHTMLAttr }}&#10;          {{- end }}&#10;        {{- end -}}&#10;        style=&quot;text-decoration:none;color:#222;font-weight:500;padding:0.5rem 0;border-left:2px solid transparent;transition:border 0.2s;&quot;&#10;        onmouseover=&quot;this.style.borderLeftColor='#0077ff'&quot;&#10;        onmouseout=&quot;this.style.borderLeftColor='transparent'&quot;&#10;      &gt;{{ $name }}&lt;/a&gt;&#10;      {{- with .Children }}&#10;        &lt;ul style=&quot;padding-left: 1rem;&quot;&gt;&#10;          {{- partial &quot;inline/menu/walk.html&quot; (dict &quot;page&quot; $page &quot;menuEntries&quot; .) }}&#10;        &lt;/ul&gt;&#10;      {{- end }}&#10;    &lt;/li&gt;&#10;  {{- end }}&#10;{{- end }}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>