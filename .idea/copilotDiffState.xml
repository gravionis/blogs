<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/content/posts/ai/1_nlp_basics.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/ai/1_nlp_basics.md" />
              <option name="originalContent" value="---&#10;title: NLP, LLM &amp; Generative AI  &#10;date: 2024-08-11&#10;tags: [&quot;Chatbots&quot;, &quot;RASA&quot;, &quot;ChatGPT&quot;, &quot;BERT&quot;, &quot;Transformers&quot;, &quot;Prompt Engineering&quot;]&#10;image : &quot;/img/posts/generative-ai-intro.jpeg&quot;&#10;Description  : &quot;Generative AI with NLP LLM: &#10;&quot;&#10;---&#10;&#10;# 1. Introduction&#10;---&#10;## NLP Tasks&#10;- **Language Modelling** : Predict the next word based on the sequence of words that already occurs in a given language. Application: speech recognition, OCR Translation etc.&#10;- **Text classification** : Assigning a text into one of the known categories based on content. Application: Email spam, sentiment analysis etc&#10;- **Information Extraction** : Extracting important information from a text. Application: extracting user's intent from input text, calendar etc.&#10;- **Information Retrieval** : Finding data based on user query. Application: Used in search engine.&#10;- **Conversational Agent** : A Dialogue system that can converse in a human language. Application: Siri, Alexa etc.&#10;- **Text Summarization** : Short summary of longer documents by retaining the important information. Application: Summary report generation from social media information.&#10;- **Question Answering** : Automatically answer questions posted in Natural Language. Application: Answering a user query based on data from a database.&#10;- **Machine Translation** : Converting a piece of text from one to another language. Application: Google transalator&#10;- **Topic Modelling** : Uncover the topical structure of large collection of text. Application: Text Mining&#10;&#10;## Understanding Human language and its building blocks&#10;- **Language**: words used in a Structured and conventional way and used to convey an idea by speech, writing or gesture.&#10;- **Linguistics**: Scientific study of a language and its structure. Study of language grammer, syntax and phonitics.&#10;  - Building Blocks:&#10;    - Phonemes: smallest unit of speech &amp; sound. English language has 44 of them. Applications: Speech to text transcriptions and text to speech conversations.&#10;    - morphemes and lexemes: Applications: Tokenization, Stemming, lemmatization, word embedding, parts of speech tagging.&#10;       - morphemes: smallest unit of a word. not all morphemes are words but the prefixes and suffixes are. e.g. 'multi' in multistory.&#10;       - lexemes: basic building block of a language. dictionary entries are lexems. lexemes are built on basic form e.g. walk, walking, walked.&#10; - **Syntax**: arragnement of words in a sentence. Representation of sentence is done using parse tree. Entity Extraction and relation extraction.&#10;    - syntax - phrases and sentences&#10;    - context - meaning&#10;    - Syntax Parse Tree:&#10;      ![](/blogs/img/posts/syntax-parse-tree.png)&#10;      - NP - noun phrase&#10;      - VP - verb phrase&#10;      - PP - prepositional phrase&#10;      - S - sentence at the highest level.&#10;- **Context**: words and sentences that surround any part of discourse and that helps determine the meaning. Application: Sarcasm detection, summarization, topic modelling. Made up of:&#10;  - semantics: direct meaning &#10;  - pragmatics: adds world knowledge and external knowledge.&#10;&#10;## Challenges of NLP&#10;- Ambiguity: two or more meanings of a single passage. e.g. we saw her duck. Common knowledge assumptions. e.g he says Sun rises in the west (assumption that a preson knows sun rises in the east)&#10;- Creativity&#10;&#10;---&#10;# 2. Pipeline of NLP&#10;---&#10;## NLP Pipeline&#10;Step by step processing of text is known as NLP Pipeline:&#10;- Data collection (scrapy)&#10;- Text Cleaning &#10;- Pre-processing (stemming and lemmetization)&#10;- Feature engineering (one hot encoding, bag of words technique)&#10;- Modeling&#10;- Evaluation&#10;- Deployment&#10;- Monitoring&#10;---&#10;&#10;---&#10;## NLTK library&#10;NLTK library is most commonly used NLP library. Common text pre-processing steps in NLP: &#10;  - Tokenization: breaking up text into smaller pieces called tokens. &#10;  - Stemming&#10;  - Lemmatization&#10;  - Word Embedding&#10;  - Parts of speech tagging&#10;  - Stop Word removal&#10;  - Word Sence disambiguation&#10;  - Named Entity Recognition (NER)&#10;    &#10;### Tokenization: breaking up text into smaller pieces called tokens. &#10;- 3 types of tokenizers in NLTK&#10;  - word_tokenize()&#10;  - wordpunct_tokenize()&#10;  - sent_tokenize()&#10;- when a tokenization is performed, we get individual tokens. sometimes it is necessary to group multiple tokens into 1.&#10;  - Unigrams: &quot;Steve&quot; &quot;went&quot; &quot;to&quot; &quot;school&quot;&#10;  - Bigrams: tokens of two consequtive words in a sentence; &quot;Steve went&quot; &quot;went to&quot; &quot;to school&quot;&#10;  - Trigrams: tokens of 3; &quot;Steve went to&quot; &quot;went to school&quot;&#10;  - Ngrams: tokens of n&#10;&#10;Setting the stage for tokenization:&#10;```python&#10;import nltk&#10;nltk.download('punkt')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens = nltk.word_tokenize(text)&#10;list(nltk.bigrams(ml_tokens)) # or trigrams&#10;```&#10;### Parts of speech tagging &amp; Stop words removal&#10;- Parts of speech tagging: process of marking words as corresponding to parts of speech, based on both definition and context.&#10;  - e.g. I like(Verb) to read(Verb) books&#10;  - this is helpful in understanding the context in which a word is used.&#10;- stopwords&#10;  - e.g. a, the, is, are&#10;  - not adding any important information, which can be elimiated.&#10;&#10;```python&#10;ml_tokens=nltk.word_tokenize(&quot;Jerry eats a banana&quot;)&#10;nltk.download(&quot;averaged_perceptron_tagger&quot;) # needs to be downloaded for tagging.&#10;for token in ml_tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;This outputs&#10;```python&#10;[('Jerry', 'NN')]&#10;[('eats', 'NNS')]&#10;[('a', 'DT')]&#10;[('banana', 'NN')]&#10;```&#10;pos_tag is a very basic version of the library, see how Jerry and eats is NNS - its tagged as a single term and categorized it as a Noun. In real life we use pos_tag from spacial library or transformers.&#10;&#10;### Regular expression tokenizer&#10;```python&#10;from nltk.tokenize import RegexpTokenizer&#10;sent = &quot;Jerry eats a banana&quot;&#10;reg_tokenizer = RegexpTokenizer('(?u)\W+|\$[\d\.]+|\S+')&#10;tokens = reg_tokenizer.tokenize(sent)&#10;for token in tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;&#10;```python&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens=nltk.word_tokenize(text)&#10;filtered_data = [w for w in ml_tokens if not w in stop_words]&#10;filtered_data&#10;```&#10;&#10;### Stemming, Lemmatization&#10;#### Stemming&#10;Reducing a word or part of a word to its stem or root form. It lowers the inflection (process we do inorder to modify the word in order to communicate mini-gramatical categories like tensors, voices, aspect, gender, mood etc. added to communicate to other person) of words into their root form. This is a pre-processing activity.&#10;Using the same word in different inflected forms in a text can lead to redundancy in natural language processing tasks. By reducing inflection, we decrease the number of unique words that machine learning models need to process.&#10;&#10;**Example 1**&#10;* Without Inflection: Original sentence: &quot;She runs every day, and they are running in the park while he ran yesterday.&quot;&#10;Inflected forms: runs, running, ran&#10;* With Reduced Inflection:Simplified sentence: &quot;She run every day, and they run in the park while he run yesterday.&quot; In this simplified version, we use &quot;run&quot; for all forms.&#10;* Impact: Original sentence has three different inflected forms, which can create redundancy for a natural language processing model.&#10;Simplified sentence reduces the variety of words, making it easier for the model to analyze the core action (running) without getting bogged down by different forms.&#10;&#10;**Example 2**&#10;&#10;* after stemming Generate → Generat also Generation → Generat&#10;* Stemming can create non-dictionary forms (like &quot;generat&quot;). It's important to note that in stemming, the goal is to reduce words to their root form, which might not always be a valid dictionary word. The main purpose of stemming is to reduce data redundancy by grouping related words together. The primary aim is to reduce the variety of word forms to improve processing efficiency and analysis.&#10;&#10;**Uses:**&#10;- SEO&#10;- Text mining&#10;- Web-search&#10;- Indexing&#10;- Tagging.&#10;&#10;**4 Types of Stemming Algorithms:**&#10;* Porter Stemmer: Martin Porter invented it and Original Stemmer algorithm. Ease of use and rapid. &#10;* Snowball Stemmer: Also invented by same guy. more presise than porter stemmer.&#10;* Lancaster Stemmer: Sometimes does over stemming, sometimes non linguistic or meaningless. &#10;* Regex Stemmer: morphological affixes.&#10;&#10;```python&#10;from nltk.stem import PorterStemmer&#10;porter = PorterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {porter.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gener&#10;# generation -&gt; gener&#10;# genorously -&gt; genor&#10;# generate -&gt; gener&#10;from nltk.stem import SnowballStemmer&#10;snowball = SnowballStemmer(language='english')&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {snowball.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generat&#10;# genorously -&gt; genor&#10;# generate -&gt; generat&#10;from nltk.stem import LancasterStemmer&#10;lancaster = LancasterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lancaster.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gen&#10;# generation -&gt; gen&#10;# genorously -&gt; gen&#10;# generate -&gt; gen&#10;from nltk.stem import RegexpStemmer&#10;regex = RegexpStemmer('ing|s$|able$',min=4)&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {regex.stem(word)}&quot;)&#10;#Output&#10;# generous -&gt; generou&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;#### Lemmatization&#10;Converting the words into root word using Parts of Speech (POS) tag as well as context as a base. Similar to stemming but brings context to the words and the result is a word in the dictionary. &#10;* Applications e.g. search engine and compacting&#10;**Example:**&#10;* eats → eat&#10;* ate → eat&#10;* ate → eat&#10;* eating → eat&#10;&#10;```python&#10;import nltk&#10;from nltk.stem import WordNetLemmatizer&#10;nltk.download('wordnet')&#10;lemma = WordNetLemmatizer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lemma.lemmatize(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;&#10;# Named Entity Recognition&#10;* First step in the information extraction&#10;* NER seeks to locate and classify named entities into pre-defined categories such as names of person, Organization, location etc. e.g. Modi, America, Apple Inc, Tesla&#10;&#10;## Challenges:&#10;- Word sense disambiguiation: method by which meaning of the word is determined from the context it is used.&#10;- Example: bark, cinnamon bark or sound made by dog is bark.&#10;- when the two sentences passed to the algorithm word sense disambiguiation comes into picture, it removes the ambiguity. &#10;&#10;## Application:&#10;* Text mining&#10;* Information extraction&#10;* used alongside with Lexicography&#10;* Information retrieval process&#10;&#10;## Word Sence disambiguation:&#10;**Lesk Algorithm:** based on the idea that words in each region will have a similar meaning.&#10;```python&#10;from nltk.wsd import lesk&#10;from nltk.tokenize import word_tokenize&#10;nltk.download('punkt')&#10;a1=lesk(word_tokenize('The building has a device to jam the signal'), 'jam')&#10;print(a1, a1.definition())&#10;a2=lesk(word_tokenize('I am stuck in a traffic jam'), 'jam')&#10;print(a2, a2.definition())&#10;a3=lesk(word_tokenize('I like to eat jam with bread'), 'jam')&#10;print(a3, a3.definition())&#10;#Output&#10;# Synset('jamming.n.01') deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems&#10;# Synset('jam.v.05') get stuck and immobilized&#10;# Synset('jam.v.06') crowd or pack to capacity --&gt; Somehow this isn't coming correct&#10;```&#10;&#10;## Named Entity Recognition&#10;```python&#10;nltk.download('averaged_perceptron_tagger')&#10;nltk.download('maxent_ne_chunker')&#10;nltk.download('words')&#10;text=&quot;Apple is an American company based out of California&quot;&#10;for w in nltk.word_tokenize(text):&#10;  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))):&#10;    if hasattr(chunk, 'label'):&#10;      print(chunk.label(), ' '.join(c[0] for c in chunk))&#10;# Output - GPE stands for Geo political entity&#10;# GPE Apple&#10;# GPE American&#10;# GPE California&#10;``` &#10;# spaCy Library&#10;spaCy is a free open source library for advaned Natural Language Processing in python for production use. NLTK was for research purpose. spaCy is for production use. Can handle and process large volume of text.&#10;## Features&#10;- Tokenization &#10;- Parts of Speech Tagging - word types of tokens, like verb or noun.&#10;- Dependency Parsing&#10;- Lemmatization&#10;- Sentence Boundary Detection (SBD) - finding and segmenting individual sentences.&#10;- Named Entity Recognition&#10;- Entity Linking (EL)- Disambiguating texual entities to unique identifiers ina knowledge base.&#10;- Similarity - comparing words, text apans and documents and how similar they are to each other.&#10;- Text Classification - assigning caterfores or labels to a whole document or parts of it.&#10;- Rule based Matching - finding sequence of token based on their texts and linguistic annotations, similar to regular expressions.&#10;- Training - updating and improving a statstical models predictions&#10;- Serialization - Saving objects to files or byte string.&#10;```python&#10;import spacy&#10;nlp = spacy.load(&quot;en_core_web_sm&quot;) # verson of spacy library - english small model&#10;doc = nlp(&quot;Apple is looking at buying U.K startup for $1 billion&quot;) # by default the spacy applies tagger, parser, ner&#10;&#10;for token in doc:&#10;  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)&#10;&#10;# Output&#10;# Apple Apple PROPN NNP nsubj Xxxxx True False&#10;# is be AUX VBZ aux xx True True&#10;# looking look VERB VBG ROOT xxxx True False&#10;# at at ADP IN prep xx True True&#10;# buying buy VERB VBG pcomp xxxx True False&#10;# U.K U.K PROPN NNP dobj X.X False False&#10;# startup startup VERB VB dep xxxx True False&#10;# for for ADP IN prep xxx True True&#10;# $ $ SYM $ quantmod $ False False&#10;# 1 1 NUM CD compound d False False&#10;# billion billion NUM CD pobj xxxx True False&#10;```&#10;* in the above by default the spacy applies tagger, parser, ner. The steps however can be added or replaced.&#10;![](https://spacy.io/images/pipeline.svg)&#10;&#10;* first step is tokenization&#10;![](https://spacy.io/images/tokenization.svg)&#10;&#10;```python&#10;text=&quot;Mission impossible is one of the best movies I have watched. I love it.&quot;&#10;print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(&quot;text&quot;, &quot;lemmatization&quot;, &quot;partofspeech&quot;, &quot;TAG&quot;, &quot;DEP&quot;, &quot;SHAPE&quot;, &quot;ALPHA&quot;, &quot;STOP&quot;))&#10;doc = nlp(text)&#10;for token in doc:&#10;  print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop))&#10;&#10;# text      |lemmatization  |partofspeech   |TAG       |DEP       |SHAPE     |ALPHA     |STOP      &#10;# Mission   |mission        |NOUN           |NN        |nsubj     |Xxxxx     |         1|         0&#10;# impossible|impossible     |ADJ            |JJ        |amod      |xxxx      |         1|         0&#10;# is        |be             |AUX            |VBZ       |ROOT      |xx        |         1|         1&#10;# one       |one            |NUM            |CD        |attr      |xxx       |         1|         1&#10;# of        |of             |ADP            |IN        |prep      |xx        |         1|         1&#10;# the       |the            |DET            |DT        |det       |xxx       |         1|         1&#10;# best      |good           |ADJ            |JJS       |amod      |xxxx      |         1|         0&#10;# movies    |movie          |NOUN           |NNS       |pobj      |xxxx      |         1|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# have      |have           |AUX            |VBP       |aux       |xxxx      |         1|         1&#10;# watched   |watch          |VERB           |VBN       |relcl     |xxxx      |         1|         0&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# love      |love           |VERB           |VBP       |ROOT      |xxxx      |         1|         0&#10;# it        |it             |PRON           |PRP       |dobj      |xx        |         1|         1&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I you do not understand sonething&#10;print(spacy.explain('nsubj')) #nominal subject&#10;print(spacy.explain('pobj')) #object of preposition&#10;# print entities&#10;```&#10;* Extracting the Named Entities&#10;```python&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia&quot;&#10;doc = nlp(text)&#10;for token in doc.ents:&#10;  print(token)&#10;# Output&#10;# Narendra Modi&#10;# India&#10;# Asia&#10;```&#10;&#10;* If you want to see a colourful version of the named entities then,&#10;```python&#10;from spacy import displacy&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia which embraces Machine Learning&quot;&#10;doc=nlp(text)&#10;displacy.render(docs=doc, style=&quot;ent&quot;,jupyter=True)&#10;spacy.explain('GPE') #Geo Political Entity&#10;```&#10;&#10;# NLP Text Vectorization&#10;Convertion of raw text into numerical form is called Text Vectorization. Machine learning expects text in numerical form. This is also called Feature Extraction.&#10;Many ways of achieving feature extraction:&#10;1. One Hot Encoding&#10;2. Count Vectorizer&#10;3. TF-IDF&#10;4. Word Embeddings&#10;&#10;## One Hot Encoding&#10;Every word including symbols are written in the vector form. This vector will only have 0 &amp; 1s. each word is written or encoded as a one hot vector, each word will have different vector representation. example:&#10;&#10;| Color  | Red | Blue | Green |&#10;|--------|-----|------|-------|&#10;| Red    |  1  |  0   |   0   |&#10;| Blue   |  0  |  1   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;| Red    |  1  |  0   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;&#10;```python&#10;corpus = ['dog eats meat','man eats meat']&#10;from sklearn.preprocessing import OneHotEncoder&#10;one_hot = OneHotEncoder()&#10;all_in_one = [indi.split() for indi in corpus]&#10;one_hot.fit_transform(all_in_one).toarray()&#10;#Output&#10;# [['dog', 'eats', 'meat'], ['man', 'eats', 'meat']]&#10;# array([[1., 0., 1., 1.],&#10;#        [0., 1., 1., 1.]])&#10;```&#10;&#10;we generally dont use the scikitlearn onehotencoding directly as it's mainly for structured data not for unstructured data.&#10;&#10;### Disadvantages&#10;* Size of the one hot encoding is propotional to the size of the vocabulary.&#10;* Sparse representation of data&#10;* Insufficent in storing, computing and learning from data.&#10;* No sequence of words is considered and is ignored.&#10;* If words outside the vocabulary exists there is no way to deal with it.&#10;* Word context is not considered in the representation.&#10;&#10;## Bag of Words technique (BoW)&#10;NLP pipeline has multiple steps as mentioned above. This step comes in the feature engineering step. Classical text represenation technique. Representation of the text under the consideration of bag of words. Text is characterised by a unique set of words. e.g. movie was bad; movie was excellent. This is characterised by the unique set of words not based on where it occurs in the sentence. so if the word bad it will be in one bag and excellent it will be in a different bag.&#10;&#10;**Application:** Sentiment analysis (positive and negative sentiments). Harry potter was good, a movie was good - they are classified into the same bag.&#10;&#10;### Write your own Bow Representation&#10;```python&#10;# if you are adventrous and dont want to use the Count Vectorizer.&#10;import pandas as pd&#10;import re&#10;t1 = &quot;dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower()).split()]&#10;&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;&#10;def bow(all, sentences):&#10;  results = []&#10;  for sentence in sentences:&#10;    result = {word: 0 for word in all}&#10;    for word in sentence:&#10;      result[word] = 1&#10;    results.append(result)&#10;  print(pd.DataFrame(results))&#10;&#10;bow(all_words, sentences)&#10;# Output&#10;# dog  eats  meat  everyday  man  once  in  a  while  rarely&#10;# 0    1     1     1         1    0     0   0  0      0       0&#10;# 1    0     1     1         0    1     1   1  1      1       0&#10;# 2    1     1     0         0    1     0   0  0      0       1&#10;```&#10;### Disadvantages:&#10;* Size of the vector increases with the size of the vocabulary&#10;* Sparsity (property of being scattered) is still an issue.&#10;* Does not capture the similarity between words (not context aware). 'I eat', 'I ate', 'I ran' Bag of Words Vectors for all the three documents will be equally apart - in layman terms - 'eat and ran' and 'eat and ate' will be same distance apart.&#10;&#10;&#10;```python&#10;# use the countvectorize or just write your own python code after finding the unique words&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;man eats meat once in a while.&quot;&#10;t3 = &quot;man eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;# vectorizer = CountVectorizer(binary=True) --&gt; use this for sentiment analysis  &#10;vectorizer = CountVectorizer()  &#10;X = vectorizer.fit_transform([t1, t2, t3])&#10;print(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;pd.DataFrame(bag_of_words, columns=feature_names)&#10;# Output&#10;# dog&#9;eats&#9;everyday&#9;in&#9;man&#9;meat&#9;once&#9;rarely&#9;while&#10;# 0&#9;5&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;1&#9;0&#10;```&#10;```Note: ``` vectorizer = CountVectorizer(**binary=True**)``` if you dont want actual counts but just 1s and 0s. This is a technique used specific to sentiment classification&#10;&#10;&#10;Now even if you want it as a unigram, bigram and trigram thats also possible.&#10;```python&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] &#10;unique_words = set(all_words)&#10;vectorizer = CountVectorizer(ngram_range=(1,3)) # See here &lt;--&#10;X = vectorizer.fit_transform(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;print(&quot;Feature Names (Vocabulary):&quot;, feature_names)&#10;print(&quot;Bag of Words Representation:&quot;)&#10;pd.DataFrame(bag_of_words)&#10;#Output&#10;# Feature Names (Vocabulary): ['dog' 'dog dog' 'dog dog dog' 'dog dog eats' 'dog eats' 'dog eats meat'&#10;#  'dog rarely' 'eats' 'eats dog' 'eats dog rarely' 'eats meat'&#10;#  'eats meat everyday' 'eats meat once' 'everyday' 'in' 'in while' 'man'&#10;#  'man eats' 'man eats dog' 'man eats meat' 'meat' 'meat everyday'&#10;#  'meat once' 'meat once in' 'once' 'once in' 'once in while' 'rarely'&#10;#  'while']&#10;# Bag of Words Representation:&#10;# 0&#9;1&#9;2&#9;3&#9;4&#9;5&#9;6&#9;7&#9;8&#9;9&#9;...&#9;19&#9;20&#9;21&#9;22&#9;23&#9;24&#9;25&#9;26&#9;27&#9;28&#10;# 0&#9;5&#9;4&#9;3&#9;1&#9;1&#9;1&#9;0&#9;1&#9;0&#9;0&#9;...&#9;0&#9;1&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#9;0&#9;...&#9;1&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;1&#9;1&#9;1&#9;...&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#10;# 3 rows × 29 columns&#10;```&#10;### Pros and Cons&#10;- has the ability to capture the context and word order information in the form of n-grams&#10;- Documents  having the same ngrams will have vectors closer to each other in euclidean space as compared to documents with different ngrams.&#10;- As n increased the dimensionlity (sparsity) increases&#10;- issue related to out of vocabulary problem exists&#10;&#10;## TF-IDF &#10;- a word most repeated in one document but not in any other documents are considered more  important. Stop words however dont fall into this category. &#10;- Term Frequency (TF) * Inverse Document Frequency (IDF)&#10;- quantify a word in a set of documents.&#10;- importance of words in the given context is represented here.&#10;&#10;**Terminology**&#10;t - term&#10;d - document (set of words)&#10;N - count of corpus&#10;corpus - the total document set.&#10;e.g. 'This Dress is so beautiful' - how is the computer to know that the important words here are dress and beautiful? thats where TF*IDF shines.&#10;&#10;* TF - number of times a particular word appears in a sentence.&#10;e.g. Sun rises in East; frequency of Sun - 1/4&#10;* IDF - Dress is beautiful; is isn't adding any importance. stop words needs to be weightage reduced particularly when these words are used more freqently it's importance will increase. IDF measures the informativeness of term t. it will be low for stop words. inverse document frequency ```formula: idf(t) = log(N/(df+1))```&#10;&#10;```&#10;IDF(word)=log10(total number of documents/ (1+number of documents containing the word))&#10;```&#10;&#10;hence, ```TF-IDF formula: tf-idf(t,d) = tf(t,d) * log(N/(df+1)) ```&#10;where, **N** - total number of documents in the corpus &amp; **df** - number of document with term t.&#10; e.g. lets say sentences: &#10;&#10; ```python&#10; import math&#10;s1='man eats pizza'&#10;s2='dog eats food'&#10;s3='ant eats pizza'&#10;# for man in s1 → tf = 1/3 &#10;# idf = log₂(3/1) &#10;tf = 1/3 &#10;idf = math.log(3/2)&#10;tf_idf = tf *  idf&#10;print(tf_idf) # 0.13515503603605478&#10;# for eats in s1 → tf = 1/3 &#10;tf = 1/3&#10;idf = math.log(3/4)&#10;tf_idf = tf*idf&#10;print(tf_idf) #-0.09589402415059363&#10;# hence eats is not a very important word.&#10;```&#10;## tf-idf hands on&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;freq1 = {}&#10;freq2 = {}&#10;for word in vocabulary:&#10;  freq1[word] = word_dict1[word]/len(first_sent)&#10;  freq2[word] = word_dict2[word]/len(second_sent)&#10;&#10;pd.DataFrame([freq1, freq2])&#10;```&#10;## implement the tf-idf using scikit&#10;it is supposed to be something like this. Below isn't fully working need to check why.&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data machine science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;def calculateTF(doc):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateIDF(docs):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateTFIDF(tfBagOfWords, idfs):&#10;  print(idfs)&#10;  tfIdf = {}&#10;  for word, value in tfBagOfWords.items():&#10;    tfIdf[word] = value*idfs[word]&#10;  return tfIdf&#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;pd.DataFrame([&#10;    calculateTFIDF(calculateTF(word_dict1), calculateIDF([word_dict1, word_dict2])),&#10;    calculateTFIDF(calculateTF(word_dict2), calculateIDF([word_dict1, word_dict2]))&#10;    ])&#10;```&#10;&#10;## implement using sklearn&#10;```python&#10;import sklearn&#10;import pandas as pd&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;vec = TfidfVectorizer()&#10;result = vec.fit_transform([first_sent, second_sent])&#10;pd.DataFrame(result.toarray(), columns=vec.get_feature_names_out())&#10;# Output&#10;# amazing&#9;an&#9;career&#9;current&#9;data&#9;deep&#9;in&#9;is&#9;learning&#9;machine&#9;of&#9;science&#9;subset&#9;the&#9;world&#10;# 0&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.230768&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.324336&#10;# 1&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.243956&#9;0.685743&#9;0.342871&#9;0.342871&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.000000&#10;```&#10;&#10;## Pros and cons of the tf-idf technique&#10;### Advantages&#10;- use this to calculate the similarity between two texts using similarity measures like Cosing similarity/Euclidean distance&#10;- has application in text classification, information retrieval etc.&#10;- better than earlier methods.&#10;### Disadvantages&#10;- high dimentionality&#10;- They are still discrete representation of units of text, hence unable to capture relation between words&#10;- sparse and high dimension&#10;- cannot handle OOV (Out of Vocabulary) words.&#10;---&#10;# TBC&#10;---&#10;https://www.youtube.com/watch?v=tFHeUSJAYbE&amp;list=PLz-ep5RbHosU2hnz5ejezwaYpdMutMVB0&#10;# Large Language Models (LLMs)&#10;is a type of Language Model. Quantatively it is the number of model parameters vary from 10 to 100 billion parameters per model. Qualitatively also called emergent properties starts emerging - properties in large language model that do not appear in small language models, e.g. zero shot learning - capability of a model to complete a task it is not explicitely trained to do.&#10;&#10;In the earlier days the model was trained using supervised learning we use thousands if not millions of examples - but with LLMs we use self-supervised learning. Train a very large model in a very large corpus of data. In self-supervised learning doesn't require manual labelling of each example. The labels or the targets of the model defined from the inherent structure of the data itself.&#10;&#10;One of the popular way of doing this is &quot;next word prediction paradigm&quot;. There is not just one word but many that can go after ***listen to your....***. What the llm would do is to use probablistic distribution of next word given the previous word. in the above example the words could be heart or gut or body or parents etc.. each with different probability distribution. Its essentially trained on a large set of data with so many examples of corpus of data - so it can statistically predict the next set of data. Important thing is the context matters - if for example we add the word ***don't*** in front of ***listen to your...***, the probably distribution will entirely change.&#10;&#10;Autoregression Task formula: ***P(tn | tn-1,..., tn-m)*** P(tn) given n tokens.&#10;&#10;This is how LLMs like chatgpt works.&#10;# 3 levels of Using LLMs&#10;- Level 1: Prompt Engineering&#10;  - using LLM out of the box - not changing any model parameters. Two ways to do this &#10;    - using an agent like chatgpt&#10;    - using open AI API or hugging face tranformers library: help to interact with LLMs programmatically using python for example. Pay per api call in case of open API. Hugging face transformer library is an open source option, you can run the models locally in this case so no need to send your proprietary data into 3rd party or open ai.&#10;- Level 2: Model Fine Tuning&#10;  - adjusting model parameters for a particular tasks.&#10;  - steps&#10;    - Step 1: pre-trained models are obtained. (usually trained by self supervised learning). in this step the base model is learning useful representations for a wide variety of tasks.&#10;    - Step 2: update model parameters given task-specific examples (trained by supervised learning or even reinforcement learning).e.g. chatgpt, the model we use here is a fine tuned model learnt by reinforcement learning. Some techniques is lora or low range adaptation. another technique is reinforcement learning based on human feedback (RLHF).&#10;    - Step 3: Deploy the fine tuned large language model.&#10;- Level 3: Build your own.&#10;  - This is only for 1% of all usecases. &#10;  - One example usecase: in a large company we dont want to use open source models where security is a concern, dont want to send data to 3rd party via an API. &#10;  - Another usecase is you want to create your own model and commercialize it.&#10;  - At a high level steps are:&#10;    - get the data or corpus.&#10;    - pre process and refine it &#10;    - model training&#10;    - pre trained llm.&#10;    - then go to step 2.&#10;&#10;## Connecting to AI using API, Programmatically&#10;### OpenAIs Python API&#10;It's similar to chatGPT but with Python. In both we pass a request and use the language modelling to predict the next word. Apart from the difference in the web interface in chatgpt and here programmatically some differences are as follows. most of the below aren't possible with chatgpt but programmatically possible with openai python.&#10;1) Customizable System message: Message or prompt or a set of instructions that help define the tone, personality and functionality of the model during a conversation. This helps model how to respond to user input and what constraints to follow. I customized the message in chatgpt first to give back sarcastic answers.&#10;![](/blogs/img/posts/chatgpt-customized-system-message.png)&#10;![](/blogs/img/posts/chatgpt-customized-system-message-output.png)&#10;Then i changed the message to give negative and dark response. This time the results were entirely opposite.&#10;![](/blogs/img/posts/chatgpt-customized-system-message-dark.png)&#10;2) Adjust input parameter &#10;  - max response length: response length sent back by model&#10;  - number of responses: (number of outputs you may want to programmatically select one of the response e.g.)&#10;  - temperature: randomness of response generated by the model.&#10;3) Process image and other types&#10;4) extact helful word embeddings for downstream tasks&#10;5) input audio for transcription and tranlations&#10;6) model fine tuning functionality.&#10;7) with chatgpt can only use GPT 3.5 or 4, with openai several other models are available read: https://platform.openai.com/docs/models&#10;### Costing:&#10;Tokens &amp; Pricing:&#10;same as tokenization above a given text is converted and represented as numbers. Pricing is based on the tokens, bigger prompts will incur larger costs. To use we have to get the Secret key to make API calls.&#10;&#10;```python&#10;import openai&#10;from openai import OpenAI&#10;from sk import openai_key # my own file with a variable openai_key='sk-proj-4D1ID8ZeQ...'&#10;&#10;client = OpenAI(api_key=openai_key)&#10;response = client.chat.completions.create(&#10;    model=&quot;gpt-3.5-turbo&quot;,&#10;    max_tokens=2,&#10;    temperature=2, # degree of randomness, 0 is predictable.&#10;    n=3,  &#10;    messages=[&#10;        {&#10;            &quot;role&quot;: &quot;user&quot;, &#10;            &quot;content&quot;: &quot;where there is a will there is a &quot;&#10;        }&#10;    ]&#10;)&#10;&#10;for idx, choice in enumerate(response.choices):&#10;    print(f&quot;Response {idx+1}: {choice['message']['content']}&quot;)&#10;&#10;# Output &#10;# Response 1: way.&#10;# Response 2: plan.&#10;# Response 3: chance.&#10;# note that 2 tokens - 'way' and '.'&#10;```&#10;## Hugging face Transformer library&#10;Major hub for open source Machine learning (ML) like Dockerhub for docker. It has models, dataset (its own data used to train models) and spaces (for building and deploying machine learning applications).&#10;&#10;### Transformers library&#10;Downloading and training machine learning models in python. Like NLP, computer vision, audio processing etc. E.g. for sentiment analysis - find the model that does sentiment analysis classification task then you have to take raw text convert into numerical value that is then passed to the model; finally decode the numerical output of the output to get the label of the text. This can be done easily in the transformers library using a pipeline function. &#10;other things that can be done &#10;- sentiment analysis&#10;- summarization&#10;- translation&#10;- question-answering&#10;- feature extraction &#10;- text generation etc.&#10;&#10;```python&#10;! pip install transformers&#10;from transformers import pipeline&#10;sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;)&#10;# sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;, model=&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;)&#10;texts = [&#10;    &quot;&quot;&quot;One is that the mining giant's shares are pushing higher this morning.&#10;        In early trade, the Big Australian's shares are 1.5% higher to $45.74.&#10;        This means that the BHP share price is now up 13% over the past two weeks.&quot;&quot;&quot;&#10;]&#10;results = sentiment_pipeline(texts)&#10;for text, result in zip(texts, results):&#10;    print(f&quot;Text: {text}\nSentiment: {result['label']}, Score: {result['score']}\n&quot;)&#10;```&#10;**Question**: How does it decide if a text is positive or negative without perception?&#10;### signup and logininto huggingface&#10;- lookup for transformer tag and select a model. Then you will check also for pytorch tag. This is because hugging face also supports models which aren't just compatible with pytorch and transformers but also others.&#10;- The train button on the right will have options like Amazon Sagemaker, NVIDIA NDX Cloud, AutoTrain which will help jump start the model finetuning part.&#10;-  &#10;### Getting started&#10;to get started copy the [hf-env.yml](https://github.com/ShawhinT/YouTube-Blog/blob/26dff2786a7d64620e5e7dd71fcd51a416aad1db/LLMs/hugging-face/hf-env.yml) file into your code repository.&#10;&#10;```bash&#10;conda env create --file hf-env.yml&#10;```&#10;another example for text-classification&#10;```python&#10;from transformers import pipeline&#10;classifier = pipeline(task=&quot;text-classification&quot;, model=&quot;SamLowe/roberta-base-go_emotions&quot;, top_k=None)&#10;sentences = [&quot;I am not having a great day&quot;]&#10;model_outputs = classifier(sentences)&#10;print(model_outputs[0])&#10;# Output&#10;# [{'label': 'disappointment', 'score': 0.4666951894760132}, {'label': 'sadness', 'score': 0.39849498867988586}, {'label': 'annoyance', 'score': 0.06806593388319016}, {'label': 'neutral', 'score': 0.05703023821115494}, {'label': 'disapproval', 'score': 0.044239308685064316}, {'label': 'nervousness', 'score': 0.014850745908915997}, {'label': 'realization', 'score': 0.014059904962778091}, {'label': 'approval', 'score': 0.0112674655392766}, {'label': 'joy', 'score': 0.006303396541625261}, {'label': 'remorse', 'score': 0.006221492309123278}, {'label': 'caring', 'score': 0.006029403302818537}, {'label': 'embarrassment', 'score': 0.0052654859609901905}, {'label': 'anger', 'score': 0.004981426056474447}, {'label': 'disgust', 'score': 0.004259029403328896}, {'label': 'grief', 'score': 0.0040021371096372604}, {'label': 'confusion', 'score': 0.003382918192073703}, {'label': 'relief', 'score': 0.0031405005138367414}, {'label': 'desire', 'score': 0.00282747158780694}, {'label': 'admiration', 'score': 0.002815794898197055}, {'label': 'fear', 'score': 0.002707520266994834}, {'label': 'optimism', 'score': 0.0026164911687374115}, {'label': 'love', 'score': 0.0024883910082280636}, {'label': 'excitement', 'score': 0.0024494787212461233}, {'label': 'curiosity', 'score': 0.0023743617348372936}, {'label': 'amusement', 'score': 0.001746696187183261}, {'label': 'surprise', 'score': 0.0014529851032420993}, {'label': 'gratitude', 'score': 0.0006464761681854725}, {'label': 'pride', 'score': 0.00055424973834306}]&#10;```&#10;yet another example for summarization&#10;```python&#10;from transformers import pipeline&#10;summarizer = pipeline(&quot;summarization&quot;, model=&quot;Falconsai/text_summarization&quot;)&#10;ARTICLE = &quot;&quot;&quot; &#10;Hugging Face: Revolutionizing Natural Language Processing&#10;Introduction&#10;In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.&#10;The Birth of Hugging Face&#10;Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name &quot;Hugging Face&quot; was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.&#10;Transformative Innovations&#10;Hugging Face is best known for its open-source contributions, particularly the &quot;Transformers&quot; library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.&#10;Key Contributions:&#10;1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.&#10;2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.&#10;3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.&#10;Democratizing AI&#10;Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.&#10;By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.&#10;Industry Adoption&#10;The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.&#10;Future Directions&#10;Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.&#10;Conclusion&#10;Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.&#10;&quot;&quot;&quot;&#10;print(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))&#10;&gt;&gt;&gt; [{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name &quot;Hugging Face&quot; was chosen to reflect the company\'s mission of making AI models more accessible and friendly to humans .'}]&#10;```&#10;Other transformers like ```Falconsai/text_summarization``` to use is the ```facebook/bart-large-cnn``` for text summarization.&#10;&#10;finally, you can chain together multiple objects for example first do a text summarization and then do a sentiment analysis. &#10;Another interesting task is conversational text. For this we can use the ```facebook/blenderbot-400M-distill```. There is supposed to be a class called ```Conversation``` (also imported from transformers) which is supposed to be a container for conversation. &#10;```python&#10;from transformers import pipeline&#10;&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;conversation_history = &quot;Hello, how are you?&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;&#10;# Continue the conversation&#10;conversation_history += f&quot; {response[0]['generated_text']}&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;```&#10;There is a library called **Gradio** to make it conversational. Gradio is very similar to streamlit. &#10;```python&#10;import gradio as gr&#10;from transformers import pipeline&#10;&#10;# Load the chatbot model&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;&#10;# Function to handle chatbot conversation&#10;def respond(user_input, history=[]):&#10;    # Add the user input to the conversation history&#10;    history = history or []&#10;    history.append(f&quot;User: {user_input}&quot;)&#10;    print(history)&#10;    # Generate a response&#10;    response = chatbot(user_input)&#10;    bot_reply = response[0]['generated_text']&#10;    print(bot_reply)&#10;&#10;    # Add the bot reply to the history&#10;    history.append(f&quot;Bot: {bot_reply}&quot;)&#10;    &#10;    # Return the entire conversation history as a string&#10;    return &quot;\n&quot;.join(history), history&#10;&#10;# Create the Gradio interface&#10;demo = gr.Interface(&#10;    fn=respond,  # The function that processes input&#10;    inputs=[gr.Textbox(label=&quot;Your Message here:&quot;), gr.State([])],  # Input is a message and conversation history&#10;    outputs=[gr.Textbox(label=&quot;Response here:&quot;), gr.State([])],  # Output is updated conversation and history&#10;    title=&quot;AI Chatbot&quot;&#10;)&#10;&#10;# Launch the interface&#10;demo.launch()&#10;```&#10;![](/blogs/img/posts/gradio-initial.png)&#10;you can post this in hugging face spaces or [hf.co/spaces](hf.co/spaces). They allow to create ML applications and host it here.&#10;example of this [Llama chatbot](https://huggingface.co/spaces/huggingface-projects/llama-3.2-vision-11B).&#10;* Go to hf.co/spaces and click on create new space.&#10;* follow the instructions to clone the repo and push your code.&#10;&#10;# Prompt Engineering&#10;## What is Prompt Engineering&#10;Prompt engineering refers to the process of designing and refining the input (or &quot;prompt&quot;) given to an AI language model, like GPT, to produce desired outputs. It's kind of the future of computer programming in Natural Language. Language models are not designed to peform a task, all that it does is to predict the next token, thus you can trick the model into solving your problem.&#10;Example of a prompt:&#10;```&#10;---&#10;&#10;**Prompt:**&#10;&#10;You are an intelligent system that processes natural language queries and selects the most relevant SQL query from a given list. Based on the user's question, match the correct SQL query that will retrieve the desired information from a database.&#10;&#10;**Input:**&#10;&#10;- **User Query (NLP):** The user asks a question in natural language, describing the data they want from the database.&#10;- **SQL Queries List:** A list of SQL queries is provided as possible answers.&#10;&#10;**Task:**&#10;&#10;- Analyze the user's natural language question.&#10;- Select the most appropriate SQL query from the list that best answers the user's question.&#10;&#10;**Example:**&#10;&#10;- **User Query:** &quot;What are the names and email addresses of all customers who made a purchase in the last 30 days?&quot;&#10;- **SQL Queries List:**&#10;    1. `SELECT * FROM customers WHERE purchase_date &gt; '2023-09-01';`&#10;    2. `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;    3. `SELECT id, name FROM orders WHERE status = 'complete';`&#10;    4. `SELECT email FROM customers WHERE created_at &gt; NOW() - INTERVAL 1 YEAR;`&#10;&#10;**Expected Output:**&#10;&#10;- The system should select query 2: `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;```&#10;## Two ways of implementing Prompt Enginner&#10;* Easy way - using an Agent like ChatGPT. You can't really use it to integrate it into another app.&#10;* Programmatically integrate using python or similar.&#10;&#10;## 7 Tricks for prompt engineering&#10;1. Be Descriptive - give a context around the problem&#10;2. Give Examples&#10;3. Use Structured Text&#10;    ```&#10;    give me the recipe for making chocolate cookies, give it in the format&#10;    **Title**: Chocolate Cookie Recipe&#10;    **Description**: .......&#10;    ```&#10;5. Chain of Thoughts&#10;    ```&#10;    Make me a resume for a job application at Google.&#10;    Step 1: Write an objective&#10;    Step 2: Write an introduction about my overall work experience. they are...&#10;    Step 3: Write in detail each experience.&#10;    Step 4: Summary and conclusion.&#10;    ```&#10;6. Chatbot personas: &#10;    ```&#10;    Act as an travel guide who knows everything about Sydney. Make me a travel itenaryfor weekend in Sydney in your Aussie Accent.&#10;    ```&#10;7. Flipped Approach:&#10;    The generic response might not be of interest to you hence we have depend on a conversational model. This is useful when you dont know what exactly you want. e.g.&#10;    ```&#10;    I want you to ask me questions to help me come up with an LLM based application idea. Ask me one question at a time to keep things conversational..&#10;    ```&#10;8. Reflective, Review and Refine&#10;&#10;## ChatGPT v/s GPT3.0&#10;ChatGPT is a finetuned model - easy to get useful responses, however with GPT 3.0 that isn't the case and more work is to be done on prompt engineering side - it just does work prediction. &#10;&#10;## LangChain&#10;LangChain is a framework designed to help developers build applications that leverage language models (like GPT) more effectively by integrating them with other tools, data sources, and workflows. It simplifies the process of creating applications that combine various natural language processing tasks with external data, APIs, and user interactions.&#10;```shell&#10;pip install langchain&#10;pip install langchain-community langchain-core&#10;pip install huggingface_hub&#10;```&#10;&#10;```python&#10;from langchain import HuggingFaceHub # or use openai&#10;from langchain.prompts import PromptTemplate&#10;from langchain.chains import LLMChain&#10;import os&#10;&#10;os.environ['HUGGINGFACEHUB_API_TOKEN'] = '&lt;your hf token&gt;'&#10;hugging_face_llm = HuggingFaceHub(repo_id=&quot;google/flan-t5-base&quot;, model_kwargs={&quot;temperature&quot;: 0.5})&#10;&#10;prompt_template = PromptTemplate(&#10;    input_variables=[&quot;question&quot;],&#10;    template=&quot;&quot;&quot;You are the teacher, and you are running a surprise test to see who are the attentive kids. &#10;    The questions will be in the form &#10;    :\nQuestion: {question}&quot;&quot;&quot;&#10;)&#10;&#10;qa_chain = LLMChain(llm=hugging_face_llm, prompt=prompt_template)&#10;question = &quot;What is the capital of France?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;question = &quot;What is the name of indias PM?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;# Output&#10;# Paris&#10;# Narendra Modi&#10;```&#10;## Model fine Tuning&#10;A smaller fine tuned model can outperform a larger base model. This involves taking an existing or pre-trained model like GPT 3 for a specific usecase like ChatGPT (GPT-3.5-turbo).&#10;3 ways to fine tune:&#10;1. Self Supervised learning &#10;    - you get the Training Corpus of data can cater to your usecase.&#10;    - you then use this corpus of text and you train the model in a self supervise way.&#10;2. Supervised Learning &#10;    - here you have a set of inputs and outputs e.g. feed in who is the 35th president of the US? and output JFK.&#10;    - So having these question answer pairs we can train the model how to answer questions.&#10;    - One way of doing this is via prompt templates.&#10;      ```text&#10;        Please answer the following questions&#10;        Q: {Question}&#10;        A: {Answer}&#10;      ```&#10;    -  Through this process we could translate the training data set to a series of prompts and generate a training corpus and go back to the self supervised process.&#10;3. Reinforcement Learning -&#10;    - Supervised Fine tunning, two steps:&#10;        - curating your training dataset&#10;        - Fine tuning the model.&#10;        - done in 5 steps:&#10;          1. Choose fine tuning task. it could be anything e.g.&#10;              - could be text summarization&#10;              - could be text generation&#10;              - text/binary classification what ever you want to do..&#10;          2. Prepare training dataset.&#10;              - e.g. if text summarization then the input/output pairs of text in desired summarization generate a training corpus using for e.g. a prompt template &#10;          3. Choose a base model.&#10;              - lots of foundantal llms for e.g. or fine tuned llms.&#10;              - use this as the starting point.&#10;          4. Fine-tune model via supervised learning&#10;              - There are 3 different options:&#10;                  1. retrain all the parameters: here we tweak all the parameters, the computation cost is very very very high.&#10;                  2. transfer learning: here we freeze most of the parameters only fine tune the head. cheaper than full retaining all the parameters.&#10;                  3. Parameter Efficient Fine Tuning (PEFT): here we freeze all the weights or parameters instead of most. Instead we augment the model with additional parameters that are trainable. Advantage is we can fine tune the model with a relatively small set of model parameters as against the above approaches.&#10;                  - One of the ways to do this is LoRA (Low Rank Adaptation). In short fine tune model by adding new trainable parameters.&#10;                  ![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*GmCISYhd-JLqHNEvAQU1tQ.png)&#10;                  the first component here h(x) = Wox is what looks like a model without LoRA. Wo has weights that are all trainable. her Wo is a d by k (dxk) matrix with d*k trainable parameters. e.g. d=1000, k=1000 Wo is a 1,000,000 trainable parameters. But with LoRA the ΔWx, another weight matrix with the same shape as Wo. &#10;&#10;                  To simplify things lets just represent ΔW as a product of two terms B and A (BA) hence we can represent ΔW in terms of a 2 one dimentional arrays or vectors A and B, which essentially generates the new h(x).&#10;&#10;                  In this case Wox itself if frozen but B and A are trainable. hence in the above context d = 1000 and k = 1000 hence (d*r)+(r*k) for intrensic rank or r = 2 which translates into 4000 trainable parameters as against the million parameters.&#10;          5. Evaluate the model performance.&#10;    - Train Reward model&#10;        - generating a score for language models completetion. highscore for correct answer and low score for an incorrect answer.&#10;        - start with a prompt pass it into supervised fine tuned model. this you do multiple times and then assign human labels and then use the ranking to train the rewards model. &#10;    - Reinforcement learning with Favourite algorithm&#10;        - example in the case of ChatGPT it uses PPO or Proximal Policy Optimization&#10;        - you give the prompt and pass it into supervised fine tuned model and pass it back to reward model. The reward model then will give feedback to the finetuned model, this is how you update the model parameters. &#10;## Practical Supervised Fine tunning&#10;**First thing first**&#10;These are some of the classes and it's uses.&#10;| Class                                      | Description                                                             |&#10;|--------------------------------------------|-------------------------------------------------------------------------|&#10;| `AutoModel`                                | Automatically loads a pre-trained model for various tasks.             |&#10;| `AutoModelForSequenceClassification`      | Loads a model specifically for sequence classification tasks.           |&#10;| `AutoModelForTokenClassification`         | Loads a model for token classification tasks (e.g., NER).              |&#10;| `AutoModelForQuestionAnswering`           | Loads a model for question answering tasks.                             |&#10;| `AutoModelForCausalLM`                    | Loads a model for causal language modeling tasks (e.g., text generation).|&#10;| `AutoModelForMaskedLM`                    | Loads a model for masked language modeling tasks.                      |&#10;| `AutoModelForImageClassification`         | Loads a model for image classification tasks.                          |&#10;| `AutoTokenizer`                            | Automatically loads a tokenizer corresponding to a pre-trained model.  |&#10;| `AutoFeatureExtractor`                     | Loads a feature extractor for models that require image preprocessing.  |&#10;| `AutoConfig`                               | Loads configuration settings for a model.                              |&#10;| `AutoPipeline`                             | Automatically creates a pipeline for various tasks using a model.     |&#10;| `AutoModelForSpeechSeq2Seq`               | Loads a model for speech-to-text sequence generation tasks.            |&#10;| `AutoModelForAudioClassification`         | Loads a model for audio classification tasks.                          |&#10;| `AutoModelForSeq2SeqLM`                   | Loads a model for sequence-to-sequence tasks (e.g., translation).     |&#10;| `AutoModelForImageSegmentation`           | Loads a model for image segmentation tasks.                            |&#10;| `AutoModelForImageToText`                 | Loads a model for image-to-text generation tasks.                      |&#10;| `AutoModelForTextToImage`                 | Loads a model for text-to-image generation tasks.                      |&#10;| `AutoModelForTextClassification`          | A more general class for text classification tasks.                   |&#10;| `AutoModelForConversational`               | Loads a model designed for conversational tasks.                       |&#10;&#10;&#10;**ChatGPT generated code for Sentiment analysis and then Finetuning using LoRA.**&#10;you can upload your dataset into huggingface like in here.&#10;![](/blogs/img/posts/huggingface-dataset-shawhin.png)&#10;you can access it using &#10;&#10;```python&#10;import torch&#10;from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments&#10;from peft import get_peft_model, LoraConfig, TaskType&#10;&#10;# Load the tokenizer and base model&#10;model_name = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;&#10;tokenizer = AutoTokenizer.from_pretrained(model_name)&#10;model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)&#10;&#10;# Define LoRA configuration&#10;lora_config = LoraConfig(&#10;    r=4,                   # Intrinsic rank&#10;    lora_alpha=32,        # Scaling factor&#10;    lora_dropout=0.01,    # Dropout rate&#10;    target_modules=[&quot;q_lin&quot;],  # Target modules for LoRA&#10;    task_type=TaskType.SEQ_CLS&#10;)&#10;&#10;# Wrap the model with LoRA&#10;lora_model = get_peft_model(model, lora_config)&#10;&#10;# Example training data&#10;train_texts = [&#10;    &quot;It was good.&quot;,&#10;    &quot;Not a fan, don't recommend.&quot;,&#10;    &quot;Better than the first one.&quot;,&#10;    &quot;This is not worth watching even once.&quot;,&#10;    &quot;This one is a pass.&quot;&#10;]&#10;train_labels = [1, 0, 1, 0, 0]  # Example labels corresponding to the texts&#10;&#10;# Example evaluation data&#10;eval_texts = [&#10;    &quot;A fantastic experience!&quot;,&#10;    &quot;Horrible movie, would not watch again.&quot;,&#10;]&#10;eval_labels = [1, 0]  # Example labels for evaluation&#10;&#10;# Tokenize the training and evaluation data&#10;train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;&#10;# Create PyTorch datasets&#10;class SentimentDataset(torch.utils.data.Dataset):&#10;    def __init__(self, encodings, labels):&#10;        self.encodings = encodings&#10;        self.labels = labels&#10;&#10;    def __getitem__(self, idx):&#10;        item = {key: val[idx] for key, val in self.encodings.items()}&#10;        item['labels'] = torch.tensor(self.labels[idx])&#10;        return item&#10;&#10;    def __len__(self):&#10;        return len(self.labels)&#10;&#10;# Create datasets&#10;train_dataset = SentimentDataset(train_encodings, train_labels)&#10;eval_dataset = SentimentDataset(eval_encodings, eval_labels)&#10;&#10;# Define training arguments&#10;training_args = TrainingArguments(&#10;    output_dir=&quot;./lora_finetuned_model&quot;,&#10;    evaluation_strategy=&quot;epoch&quot;,&#10;    learning_rate=5e-5,&#10;    per_device_train_batch_size=2,&#10;    per_device_eval_batch_size=2,  # Add eval batch size&#10;    num_train_epochs=3,&#10;    weight_decay=0.01,&#10;)&#10;&#10;# Define Trainer&#10;trainer = Trainer(&#10;    model=lora_model,&#10;    args=training_args,&#10;    train_dataset=train_dataset,&#10;    eval_dataset=eval_dataset,  # Provide eval dataset&#10;)&#10;&#10;# Fine-tune the model&#10;trainer.train()&#10;&#10;# Save the fine-tuned model&#10;trainer.save_model(&quot;./lora_finetuned_model&quot;)&#10;&#10;&#10;eval_texts = [&#10;    &quot;An absolutely stunning film! The visuals were breathtaking, and the storyline kept me engaged the entire time.&quot;,&#10;    &quot;I was really disappointed with this film. The plot was weak and the characters were poorly developed.&quot;,&#10;    &quot;A heartwarming story that left me in tears. The performances were phenomenal and truly captured the essence of the characters.&quot;,&#10;    &quot;A boring and predictable movie that dragged on for too long. I wouldn't recommend it to anyone.&quot;,&#10;    &quot;This movie exceeded my expectations! The plot twists were fantastic, and the acting was top-notch. Highly recommend!&quot;,&#10;    &quot;The special effects couldn't save this film. It was a chore to sit through, and I found myself looking at my watch constantly.&quot;,&#10;    &quot;An inspiring tale that resonates on many levels. The direction was exceptional, and the soundtrack was unforgettable.&quot;,&#10;    &quot;An absolute disaster! The acting was cringe-worthy and the story made no sense whatsoever.&quot;,&#10;    &quot;A brilliant blend of action and comedy. I couldn't stop laughing, and the action scenes were exhilarating!&quot;,&#10;    &quot;I expected much more from this film. It felt like a cash grab with no real substance or originality.&quot;&#10;]&#10;&#10;# Get predictions&#10;predicted_sentiments = predict_sentiment(eval_texts)&#10;&#10;# Print the results&#10;for text, sentiment in zip(eval_texts, predicted_sentiments):&#10;    print(f&quot;Text: \&quot;{text}\&quot; - Sentiment: {sentiment}&quot;)&#10;```" />
              <option name="updatedContent" value="---&#10;title: NLP, LLM &amp; Generative AI  &#10;date: 2024-08-11&#10;tags: [&quot;Chatbots&quot;, &quot;RASA&quot;, &quot;ChatGPT&quot;, &quot;BERT&quot;, &quot;Transformers&quot;, &quot;Prompt Engineering&quot;]&#10;image : &quot;/img/posts/generative-ai-intro.jpeg&quot;&#10;Description  : &quot;Generative AI with NLP LLM: &#10;&quot;&#10;---&#10;&#10;Natural Language Processing (NLP) is a foundational field in artificial intelligence that enables computers to understand, interpret, and generate human language. With the rapid evolution of large language models (LLMs) and generative AI, NLP has become central to applications such as chatbots, virtual assistants, sentiment analysis, and automated translation. This document provides a practical overview of NLP concepts, pipelines, and tools, including hands-on examples with libraries like NLTK and spaCy. It also explores the rise of LLMs, prompt engineering, and modern frameworks that are shaping the future of language-based AI systems.&#10;&#10;# 1. Introduction&#10;---&#10;## NLP Tasks&#10;- **Language Modelling** : Predict the next word based on the sequence of words that already occurs in a given language. Application: speech recognition, OCR Translation etc.&#10;- **Text classification** : Assigning a text into one of the known categories based on content. Application: Email spam, sentiment analysis etc&#10;- **Information Extraction** : Extracting important information from a text. Application: extracting user's intent from input text, calendar etc.&#10;- **Information Retrieval** : Finding data based on user query. Application: Used in search engine.&#10;- **Conversational Agent** : A Dialogue system that can converse in a human language. Application: Siri, Alexa etc.&#10;- **Text Summarization** : Short summary of longer documents by retaining the important information. Application: Summary report generation from social media information.&#10;- **Question Answering** : Automatically answer questions posted in Natural Language. Application: Answering a user query based on data from a database.&#10;- **Machine Translation** : Converting a piece of text from one to another language. Application: Google transalator&#10;- **Topic Modelling** : Uncover the topical structure of large collection of text. Application: Text Mining&#10;&#10;## Understanding Human language and its building blocks&#10;- **Language**: words used in a Structured and conventional way and used to convey an idea by speech, writing or gesture.&#10;- **Linguistics**: Scientific study of a language and its structure. Study of language grammer, syntax and phonitics.&#10;  - Building Blocks:&#10;    - Phonemes: smallest unit of speech &amp; sound. English language has 44 of them. Applications: Speech to text transcriptions and text to speech conversations.&#10;    - morphemes and lexemes: Applications: Tokenization, Stemming, lemmatization, word embedding, parts of speech tagging.&#10;       - morphemes: smallest unit of a word. not all morphemes are words but the prefixes and suffixes are. e.g. 'multi' in multistory.&#10;       - lexemes: basic building block of a language. dictionary entries are lexems. lexemes are built on basic form e.g. walk, walking, walked.&#10; - **Syntax**: arragnement of words in a sentence. Representation of sentence is done using parse tree. Entity Extraction and relation extraction.&#10;    - syntax - phrases and sentences&#10;    - context - meaning&#10;    - Syntax Parse Tree:&#10;      ![](/blogs/img/posts/syntax-parse-tree.png)&#10;      - NP - noun phrase&#10;      - VP - verb phrase&#10;      - PP - prepositional phrase&#10;      - S - sentence at the highest level.&#10;- **Context**: words and sentences that surround any part of discourse and that helps determine the meaning. Application: Sarcasm detection, summarization, topic modelling. Made up of:&#10;  - semantics: direct meaning &#10;  - pragmatics: adds world knowledge and external knowledge.&#10;&#10;## Challenges of NLP&#10;- Ambiguity: two or more meanings of a single passage. e.g. we saw her duck. Common knowledge assumptions. e.g he says Sun rises in the west (assumption that a preson knows sun rises in the east)&#10;- Creativity&#10;&#10;---&#10;# 2. Pipeline of NLP&#10;---&#10;## NLP Pipeline&#10;Step by step processing of text is known as NLP Pipeline:&#10;- Data collection (scrapy)&#10;- Text Cleaning &#10;- Pre-processing (stemming and lemmetization)&#10;- Feature engineering (one hot encoding, bag of words technique)&#10;- Modeling&#10;- Evaluation&#10;- Deployment&#10;- Monitoring&#10;---&#10;&#10;---&#10;## NLTK library&#10;NLTK library is most commonly used NLP library. Common text pre-processing steps in NLP: &#10;  - Tokenization: breaking up text into smaller pieces called tokens. &#10;  - Stemming&#10;  - Lemmatization&#10;  - Word Embedding&#10;  - Parts of speech tagging&#10;  - Stop Word removal&#10;  - Word Sence disambiguation&#10;  - Named Entity Recognition (NER)&#10;    &#10;### Tokenization: breaking up text into smaller pieces called tokens. &#10;- 3 types of tokenizers in NLTK&#10;  - word_tokenize()&#10;  - wordpunct_tokenize()&#10;  - sent_tokenize()&#10;- when a tokenization is performed, we get individual tokens. sometimes it is necessary to group multiple tokens into 1.&#10;  - Unigrams: &quot;Steve&quot; &quot;went&quot; &quot;to&quot; &quot;school&quot;&#10;  - Bigrams: tokens of two consequtive words in a sentence; &quot;Steve went&quot; &quot;went to&quot; &quot;to school&quot;&#10;  - Trigrams: tokens of 3; &quot;Steve went to&quot; &quot;went to school&quot;&#10;  - Ngrams: tokens of n&#10;&#10;Setting the stage for tokenization:&#10;```python&#10;import nltk&#10;nltk.download('punkt')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens = nltk.word_tokenize(text)&#10;list(nltk.bigrams(ml_tokens)) # or trigrams&#10;```&#10;### Parts of speech tagging &amp; Stop words removal&#10;- Parts of speech tagging: process of marking words as corresponding to parts of speech, based on both definition and context.&#10;  - e.g. I like(Verb) to read(Verb) books&#10;  - this is helpful in understanding the context in which a word is used.&#10;- stopwords&#10;  - e.g. a, the, is, are&#10;  - not adding any important information, which can be elimiated.&#10;&#10;```python&#10;ml_tokens=nltk.word_tokenize(&quot;Jerry eats a banana&quot;)&#10;nltk.download(&quot;averaged_perceptron_tagger&quot;) # needs to be downloaded for tagging.&#10;for token in ml_tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;This outputs&#10;```python&#10;[('Jerry', 'NN')]&#10;[('eats', 'NNS')]&#10;[('a', 'DT')]&#10;[('banana', 'NN')]&#10;```&#10;pos_tag is a very basic version of the library, see how Jerry and eats is NNS - its tagged as a single term and categorized it as a Noun. In real life we use pos_tag from spacial library or transformers.&#10;&#10;### Regular expression tokenizer&#10;```python&#10;from nltk.tokenize import RegexpTokenizer&#10;sent = &quot;Jerry eats a banana&quot;&#10;reg_tokenizer = RegexpTokenizer('(?u)\W+|\$[\d\.]+|\S+')&#10;tokens = reg_tokenizer.tokenize(sent)&#10;for token in tokens:&#10;  print(nltk.pos_tag([token]))&#10;```&#10;&#10;```python&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;text=&quot;In a world where technological advancements continue to redefine the boundaries of what is possible, the rapid integration of artificial intelligence, machine learning, and data-driven decision-making processes across industries ranging from healthcare, finance, and entertainment to education, agriculture, and manufacturing has opened up a plethora of opportunities for businesses, governments, and individuals to not only optimize their operations but also drive innovation in ways that were previously unimaginable, thus creating an ecosystem where collaboration between humans and machines can lead to transformative solutions that address complex global challenges such as climate change, poverty, and public health crises, while also ensuring that ethical considerations, regulatory frameworks, and the need for transparency remain at the forefront of this new era of technological evolution.&quot;&#10;ml_tokens=nltk.word_tokenize(text)&#10;filtered_data = [w for w in ml_tokens if not w in stop_words]&#10;filtered_data&#10;```&#10;&#10;### Stemming, Lemmatization&#10;#### Stemming&#10;Reducing a word or part of a word to its stem or root form. It lowers the inflection (process we do inorder to modify the word in order to communicate mini-gramatical categories like tensors, voices, aspect, gender, mood etc. added to communicate to other person) of words into their root form. This is a pre-processing activity.&#10;Using the same word in different inflected forms in a text can lead to redundancy in natural language processing tasks. By reducing inflection, we decrease the number of unique words that machine learning models need to process.&#10;&#10;**Example 1**&#10;* Without Inflection: Original sentence: &quot;She runs every day, and they are running in the park while he ran yesterday.&quot;&#10;Inflected forms: runs, running, ran&#10;* With Reduced Inflection:Simplified sentence: &quot;She run every day, and they run in the park while he run yesterday.&quot; In this simplified version, we use &quot;run&quot; for all forms.&#10;* Impact: Original sentence has three different inflected forms, which can create redundancy for a natural language processing model.&#10;Simplified sentence reduces the variety of words, making it easier for the model to analyze the core action (running) without getting bogged down by different forms.&#10;&#10;**Example 2**&#10;&#10;* after stemming Generate → Generat also Generation → Generat&#10;* Stemming can create non-dictionary forms (like &quot;generat&quot;). It's important to note that in stemming, the goal is to reduce words to their root form, which might not always be a valid dictionary word. The main purpose of stemming is to reduce data redundancy by grouping related words together. The primary aim is to reduce the number of unique words that machine learning models need to process.&#10;&#10;**Uses:**&#10;- SEO&#10;- Text mining&#10;- Web-search&#10;- Indexing&#10;- Tagging.&#10;&#10;**4 Types of Stemming Algorithms:**&#10;* Porter Stemmer: Martin Porter invented it and Original Stemmer algorithm. Ease of use and rapid. &#10;* Snowball Stemmer: Also invented by same guy. more presise than porter stemmer.&#10;* Lancaster Stemmer: Sometimes does over stemming, sometimes non linguistic or meaningless. &#10;* Regex Stemmer: morphological affixes.&#10;&#10;```python&#10;from nltk.stem import PorterStemmer&#10;porter = PorterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {porter.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gener&#10;# generation -&gt; gener&#10;# genorously -&gt; genor&#10;# generate -&gt; gener&#10;from nltk.stem import SnowballStemmer&#10;snowball = SnowballStemmer(language='english')&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {snowball.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generat&#10;# genorously -&gt; genor&#10;# generate -&gt; generat&#10;from nltk.stem import LancasterStemmer&#10;lancaster = LancasterStemmer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lancaster.stem(word)}&quot;)&#10;# Output&#10;# generous -&gt; gen&#10;# generation -&gt; gen&#10;# genorously -&gt; gen&#10;# generate -&gt; gen&#10;from nltk.stem import RegexpStemmer&#10;regex = RegexpStemmer('ing|s$|able$',min=4)&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {regex.stem(word)}&quot;)&#10;#Output&#10;# generous -&gt; generou&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;#### Lemmatization&#10;Converting the words into root word using Parts of Speech (POS) tag as well as context as a base. Similar to stemming but brings context to the words and the result is a word in the dictionary. &#10;* Applications e.g. search engine and compacting&#10;**Example:**&#10;* eats → eat&#10;* ate → eat&#10;* ate → eat&#10;* eating → eat&#10;&#10;```python&#10;import nltk&#10;from nltk.stem import WordNetLemmatizer&#10;nltk.download('wordnet')&#10;lemma = WordNetLemmatizer()&#10;words = ['generous', 'generation', 'genorously','generate']&#10;for word in words:&#10;  print(f&quot;{word} -&gt; {lemma.lemmatize(word)}&quot;)&#10;# Output&#10;# generous -&gt; generous&#10;# generation -&gt; generation&#10;# genorously -&gt; genorously&#10;# generate -&gt; generate&#10;```&#10;&#10;# Named Entity Recognition&#10;* First step in the information extraction&#10;* NER seeks to locate and classify named entities into pre-defined categories such as names of person, Organization, location etc. e.g. Modi, America, Apple Inc, Tesla&#10;&#10;## Challenges:&#10;- Word sense disambiguiation: method by which meaning of the word is determined from the context it is used.&#10;- Example: bark, cinnamon bark or sound made by dog is bark.&#10;- when the two sentences passed to the algorithm word sense disambiguiation comes into picture, it removes the ambiguity. &#10;&#10;## Application:&#10;* Text mining&#10;* Information extraction&#10;* used alongside with Lexicography&#10;* Information retrieval process&#10;&#10;## Word Sence disambiguation:&#10;**Lesk Algorithm:** based on the idea that words in each region will have a similar meaning.&#10;```python&#10;from nltk.wsd import lesk&#10;from nltk.tokenize import word_tokenize&#10;nltk.download('punkt')&#10;a1=lesk(word_tokenize('The building has a device to jam the signal'), 'jam')&#10;print(a1, a1.definition())&#10;a2=lesk(word_tokenize('I am stuck in a traffic jam'), 'jam')&#10;print(a2, a2.definition())&#10;a3=lesk(word_tokenize('I like to eat jam with bread'), 'jam')&#10;print(a3, a3.definition())&#10;#Output&#10;# Synset('jamming.n.01') deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems&#10;# Synset('jam.v.05') get stuck and immobilized&#10;# Synset('jam.v.06') crowd or pack to capacity --&gt; Somehow this isn't coming correct&#10;```&#10;&#10;## Named Entity Recognition&#10;```python&#10;nltk.download('averaged_perceptron_tagger')&#10;nltk.download('maxent_ne_chunker')&#10;nltk.download('words')&#10;text=&quot;Apple is an American company based out of California&quot;&#10;for w in nltk.word_tokenize(text):&#10;  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))):&#10;    if hasattr(chunk, 'label'):&#10;      print(chunk.label(), ' '.join(c[0] for c in chunk))&#10;# Output - GPE stands for Geo political entity&#10;# GPE Apple&#10;# GPE American&#10;# GPE California&#10;``` &#10;# spaCy Library&#10;spaCy is a free open source library for advaned Natural Language Processing in python for production use. NLTK was for research purpose. spaCy is for production use. Can handle and process large volume of text.&#10;## Features&#10;- Tokenization &#10;- Parts of Speech Tagging - word types of tokens, like verb or noun.&#10;- Dependency Parsing&#10;- Lemmatization&#10;- Sentence Boundary Detection (SBD) - finding and segmenting individual sentences.&#10;- Named Entity Recognition&#10;- Entity Linking (EL)- Disambiguating texual entities to unique identifiers ina knowledge base.&#10;- Similarity - comparing words, text apans and documents and how similar they are to each other.&#10;- Text Classification - assigning caterfores or labels to a whole document or parts of it.&#10;- Rule based Matching - finding sequence of token based on their texts and linguistic annotations, similar to regular expressions.&#10;- Training - updating and improving a statstical models predictions&#10;- Serialization - Saving objects to files or byte string.&#10;```python&#10;import spacy&#10;nlp = spacy.load(&quot;en_core_web_sm&quot;) # verson of spacy library - english small model&#10;doc = nlp(&quot;Apple is looking at buying U.K startup for $1 billion&quot;) # by default the spacy applies tagger, parser, ner&#10;&#10;for token in doc:&#10;  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)&#10;&#10;# Output&#10;# Apple Apple PROPN NNP nsubj Xxxxx True False&#10;# is be AUX VBZ aux xx True True&#10;# looking look VERB VBG ROOT xxxx True False&#10;# at at ADP IN prep xx True True&#10;# buying buy VERB VBG pcomp xxxx True False&#10;# U.K U.K PROPN NNP dobj X.X False False&#10;# startup startup VERB VB dep xxxx True False&#10;# for for ADP IN prep xxx True True&#10;# $ $ SYM $ quantmod $ False False&#10;# 1 1 NUM CD compound d False False&#10;# billion billion NUM CD pobj xxxx True False&#10;```&#10;* in the above by default the spacy applies tagger, parser, ner. The steps however can be added or replaced.&#10;![](https://spacy.io/images/pipeline.svg)&#10;&#10;* first step is tokenization&#10;![](https://spacy.io/images/tokenization.svg)&#10;&#10;```python&#10;text=&quot;Mission impossible is one of the best movies I have watched. I love it.&quot;&#10;print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(&quot;text&quot;, &quot;lemmatization&quot;, &quot;partofspeech&quot;, &quot;TAG&quot;, &quot;DEP&quot;, &quot;SHAPE&quot;, &quot;ALPHA&quot;, &quot;STOP&quot;))&#10;doc = nlp(text)&#10;for token in doc:&#10;  print(&quot;{:10}|{:15}|{:15}|{:10}|{:10}|{:10}|{:10}|{:10}&quot;.format(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop))&#10;&#10;# text      |lemmatization  |partofspeech   |TAG       |DEP       |SHAPE     |ALPHA     |STOP      &#10;# Mission   |mission        |NOUN           |NN        |nsubj     |Xxxxx     |         1|         0&#10;# impossible|impossible     |ADJ            |JJ        |amod      |xxxx      |         1|         0&#10;# is        |be             |AUX            |VBZ       |ROOT      |xx        |         1|         1&#10;# one       |one            |NUM            |CD        |attr      |xxx       |         1|         1&#10;# of        |of             |ADP            |IN        |prep      |xx        |         1|         1&#10;# the       |the            |DET            |DT        |det       |xxx       |         1|         1&#10;# best      |good           |ADJ            |JJS       |amod      |xxxx      |         1|         0&#10;# movies    |movie          |NOUN           |NNS       |pobj      |xxxx      |         1|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# have      |have           |AUX            |VBP       |aux       |xxxx      |         1|         1&#10;# watched   |watch          |VERB           |VBN       |relcl     |xxxx      |         1|         0&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I         |I              |PRON           |PRP       |nsubj     |X         |         1|         1&#10;# love      |love           |VERB           |VBP       |ROOT      |xxxx      |         1|         0&#10;# it        |it             |PRON           |PRP       |dobj      |xx        |         1|         1&#10;# .         |.              |PUNCT          |.         |punct     |.         |         0|         0&#10;# I you do not understand sonething&#10;print(spacy.explain('nsubj')) #nominal subject&#10;print(spacy.explain('pobj')) #object of preposition&#10;# print entities&#10;```&#10;* Extracting the Named Entities&#10;```python&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia&quot;&#10;doc = nlp(text)&#10;for token in doc.ents:&#10;  print(token)&#10;# Output&#10;# Narendra Modi&#10;# India&#10;# Asia&#10;```&#10;&#10;* If you want to see a colourful version of the named entities then,&#10;```python&#10;from spacy import displacy&#10;text=&quot;Narendra Modi is the PM of India which is a country in the continent of Asia which embraces Machine Learning&quot;&#10;doc=nlp(text)&#10;displacy.render(docs=doc, style=&quot;ent&quot;,jupyter=True)&#10;spacy.explain('GPE') #Geo Political Entity&#10;```&#10;&#10;# NLP Text Vectorization&#10;Convertion of raw text into numerical form is called Text Vectorization. Machine learning expects text in numerical form. This is also called Feature Extraction.&#10;Many ways of achieving feature extraction:&#10;1. One Hot Encoding&#10;2. Count Vectorizer&#10;3. TF-IDF&#10;4. Word Embeddings&#10;&#10;## One Hot Encoding&#10;Every word including symbols are written in the vector form. This vector will only have 0 &amp; 1s. each word is written or encoded as a one hot vector, each word will have different vector representation. example:&#10;&#10;| Color  | Red | Blue | Green |&#10;|--------|-----|------|-------|&#10;| Red    |  1  |  0   |   0   |&#10;| Blue   |  0  |  1   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;| Red    |  1  |  0   |   0   |&#10;| Green  |  0  |  0   |   1   |&#10;&#10;```python&#10;corpus = ['dog eats meat','man eats meat']&#10;from sklearn.preprocessing import OneHotEncoder&#10;one_hot = OneHotEncoder()&#10;all_in_one = [indi.split() for indi in corpus]&#10;one_hot.fit_transform(all_in_one).toarray()&#10;#Output&#10;# [['dog', 'eats', 'meat'], ['man', 'eats', 'meat']]&#10;# array([[1., 0., 1., 1.],&#10;#        [0., 1., 1., 1.]])&#10;```&#10;&#10;we generally dont use the scikitlearn onehotencoding directly as it's mainly for structured data not for unstructured data.&#10;&#10;### Disadvantages&#10;* Size of the one hot encoding is propotional to the size of the vocabulary.&#10;* Sparse representation of data&#10;* Insufficent in storing, computing and learning from data.&#10;* No sequence of words is considered and is ignored.&#10;* If words outside the vocabulary exists there is no way to deal with it.&#10;* Word context is not considered in the representation.&#10;&#10;## Bag of Words technique (BoW)&#10;NLP pipeline has multiple steps as mentioned above. This step comes in the feature engineering step. Classical text represenation technique. Representation of the text under the consideration of bag of words. Text is characterised by a unique set of words. e.g. movie was bad; movie was excellent. This is characterised by the unique set of words not based on where it occurs in the sentence. so if the word bad it will be in one bag and excellent it will be in a different bag.&#10;&#10;**Application:** Sentiment analysis (positive and negative sentiments). Harry potter was good, a movie was good - they are classified into the same bag.&#10;&#10;### Write your own Bow Representation&#10;```python&#10;# if you are adventrous and dont want to use the Count Vectorizer.&#10;import pandas as pd&#10;import re&#10;t1 = &quot;dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()).split(), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower()).split()]&#10;&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;&#10;def bow(all, sentences):&#10;  results = []&#10;  for sentence in sentences:&#10;    result = {word: 0 for word in all}&#10;    for word in sentence:&#10;      result[word] = 1&#10;    results.append(result)&#10;  print(pd.DataFrame(results))&#10;&#10;bow(all_words, sentences)&#10;# Output&#10;# dog  eats  meat  everyday  man  once  in  a  while  rarely&#10;# 0    1     1     1         1    0     0   0  0      0       0&#10;# 1    0     1     1         0    1     1   1  1      1       0&#10;# 2    1     1     0         0    1     0   0  0      0       1&#10;```&#10;### Disadvantages:&#10;* Size of the vector increases with the size of the vocabulary&#10;* Sparsity (property of being scattered) is still an issue.&#10;* Does not capture the similarity between words (not context aware). 'I eat', 'I ate', 'I ran' Bag of Words Vectors for all the three documents will be equally apart - in layman terms - 'eat and ran' and 'eat and ate' will be same distance apart.&#10;&#10;&#10;```python&#10;# use the countvectorize or just write your own python code after finding the unique words&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;man eats meat once in a while.&quot;&#10;t3 = &quot;man eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] # return variable - then first for.. then second for&#10;unique_words = set(all_words)&#10;# vectorizer = CountVectorizer(binary=True) --&gt; use this for sentiment analysis  &#10;vectorizer = CountVectorizer()  &#10;X = vectorizer.fit_transform([t1, t2, t3])&#10;print(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;pd.DataFrame(bag_of_words, columns=feature_names)&#10;# Output&#10;# dog&#9;eats&#9;everyday&#9;in&#9;man&#9;meat&#9;once&#9;rarely&#9;while&#10;# 0&#9;5&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;1&#9;0&#9;0&#9;1&#9;0&#9;0&#9;1&#9;0&#10;```&#10;```Note: ``` vectorizer = CountVectorizer(**binary=True**)``` if you dont want actual counts but just 1s and 0s. This is a technique used specific to sentiment classification&#10;&#10;&#10;Now even if you want it as a unigram, bigram and trigram thats also possible.&#10;```python&#10;from sklearn.feature_extraction.text import CountVectorizer&#10;import re&#10;import pandas as pd&#10;t1 = &quot;dog dog dog dog, dog eats meat everyday!&quot;&#10;t2 = &quot;mAn eats meat once in a while.&quot;&#10;t3 = &quot;man, eaTs dog rarely!!!&quot;&#10;sentences = [re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t1.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t2.lower()), &#10;             re.sub(r&quot;[^a-zA-Z0-9]&quot;, &quot; &quot;, t3.lower())]&#10;all_words = [word for words in sentences for word in words] &#10;unique_words = set(all_words)&#10;vectorizer = CountVectorizer(ngram_range=(1,3)) # See here &lt;--&#10;X = vectorizer.fit_transform(sentences)&#10;bag_of_words = X.toarray()&#10;feature_names = vectorizer.get_feature_names_out()&#10;print(&quot;Feature Names (Vocabulary):&quot;, feature_names)&#10;print(&quot;Bag of Words Representation:&quot;)&#10;pd.DataFrame(bag_of_words)&#10;#Output&#10;# Feature Names (Vocabulary): ['dog' 'dog dog' 'dog dog dog' 'dog dog eats' 'dog eats' 'dog eats meat'&#10;#  'dog rarely' 'eats' 'eats dog' 'eats dog rarely' 'eats meat'&#10;#  'eats meat everyday' 'eats meat once' 'everyday' 'in' 'in while' 'man'&#10;#  'man eats' 'man eats dog' 'man eats meat' 'meat' 'meat everyday'&#10;#  'meat once' 'meat once in' 'once' 'once in' 'once in while' 'rarely'&#10;#  'while']&#10;# Bag of Words Representation:&#10;# 0&#9;1&#9;2&#9;3&#9;4&#9;5&#9;6&#9;7&#9;8&#9;9&#9;...&#9;19&#9;20&#9;21&#9;22&#9;23&#9;24&#9;25&#9;26&#9;27&#9;28&#10;# 0&#9;5&#9;4&#9;3&#9;1&#9;1&#9;1&#9;0&#9;1&#9;0&#9;0&#9;...&#9;0&#9;1&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#10;# 1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#9;0&#9;...&#9;1&#9;1&#9;0&#9;1&#9;1&#9;1&#9;1&#9;1&#9;0&#9;1&#10;# 2&#9;1&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;1&#9;1&#9;1&#9;...&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;0&#9;1&#9;0&#10;# 3 rows × 29 columns&#10;```&#10;### Pros and Cons&#10;- has the ability to capture the context and word order information in the form of n-grams&#10;- Documents  having the same ngrams will have vectors closer to each other in euclidean space as compared to documents with different ngrams.&#10;- As n increased the dimensionlity (sparsity) increases&#10;- issue related to out of vocabulary problem exists&#10;&#10;## TF-IDF &#10;- a word most repeated in one document but not in any other documents are considered more  important. Stop words however dont fall into this category. &#10;- Term Frequency (TF) * Inverse Document Frequency (IDF)&#10;- quantify a word in a set of documents.&#10;- importance of words in the given context is represented here.&#10;&#10;**Terminology**&#10;t - term&#10;d - document (set of words)&#10;N - count of corpus&#10;corpus - the total document set.&#10;e.g. 'This Dress is so beautiful' - how is the computer to know that the important words here are dress and beautiful? thats where TF*IDF shines.&#10;&#10;* TF - number of times a particular word appears in a sentence.&#10;e.g. Sun rises in East; frequency of Sun - 1/4&#10;* IDF - Dress is beautiful; is isn't adding any importance. stop words needs to be weightage reduced particularly when these words are used more freqently it's importance will increase. IDF measures the informativeness of term t. it will be low for stop words. inverse document frequency ```formula: idf(t) = log(N/(df+1))```&#10;&#10;```&#10;IDF(word)=log10(total number of documents/ (1+number of documents containing the word))&#10;```&#10;&#10;hence, ```TF-IDF formula: tf-idf(t,d) = tf(t,d) * log(N/(df+1)) ```&#10;where, **N** - total number of documents in the corpus &amp; **df** - number of document with term t.&#10; e.g. lets say sentences: &#10;&#10; ```python&#10; import math&#10;s1='man eats pizza'&#10;s2='dog eats food'&#10;s3='ant eats pizza'&#10;# for man in s1 → tf = 1/3 &#10;# idf = log₂(3/1) &#10;tf = 1/3 &#10;idf = math.log(3/2)&#10;tf_idf = tf *  idf&#10;print(tf_idf) # 0.13515503603605478&#10;# for eats in s1 → tf = 1/3 &#10;tf = 1/3&#10;idf = math.log(3/4)&#10;tf_idf = tf*idf&#10;print(tf_idf) #-0.09589402415059363&#10;# hence eats is not a very important word.&#10;```&#10;## tf-idf hands on&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;freq1 = {}&#10;freq2 = {}&#10;for word in vocabulary:&#10;  freq1[word] = word_dict1[word]/len(first_sent)&#10;  freq2[word] = word_dict2[word]/len(second_sent)&#10;&#10;pd.DataFrame([freq1, freq2])&#10;```&#10;## implement the tf-idf using scikit&#10;it is supposed to be something like this. Below isn't fully working need to check why.&#10;```python&#10;import pandas as pd&#10;import math&#10;import sklearn&#10;import nltk&#10;from nltk.corpus import stopwords&#10;nltk.download('stopwords')&#10;stop_words = stopwords.words('english')&#10;first_sent = &quot;Data machine science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;first_sent = [word for word in first_sent.split() if word not in stop_words]&#10;second_sent = [word for word in second_sent.split() if word not in stop_words]&#10;vocabulary = set(first_sent).union(set(second_sent))&#10;word_dict1 = dict.fromkeys(vocabulary, 0)&#10;word_dict2 = dict.fromkeys(vocabulary, 0)&#10;for word in first_sent:&#10;  word_dict1[word] += 1&#10;for word in second_sent:&#10;  word_dict2[word] += 1&#10;# Count Vectorization representation.&#10;df = pd.DataFrame([word_dict1,word_dict2]) &#10;&#10;def calculateTF(doc):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateIDF(docs):&#10;  # To be implemented&#10;  pass&#10;&#10;def calculateTFIDF(tfBagOfWords, idfs):&#10;  print(idfs)&#10;  tfIdf = {}&#10;  for word, value in tfBagOfWords.items():&#10;    tfIdf[word] = value*idfs[word]&#10;  return tfIdf&#10;&#10;# Term Frequency - number of occurances of the word/total number of words&#10;pd.DataFrame([&#10;    calculateTFIDF(calculateTF(word_dict1), calculateIDF([word_dict1, word_dict2])),&#10;    calculateTFIDF(calculateTF(word_dict2), calculateIDF([word_dict1, word_dict2]))&#10;    ])&#10;```&#10;&#10;## implement using sklearn&#10;```python&#10;import sklearn&#10;import pandas as pd&#10;from sklearn.feature_extraction.text import TfidfVectorizer&#10;first_sent = &quot;Data science is an amazing career in the current world&quot;&#10;second_sent = &quot;Deep learning is a subset of machine learning&quot;&#10;vec = TfidfVectorizer()&#10;result = vec.fit_transform([first_sent, second_sent])&#10;pd.DataFrame(result.toarray(), columns=vec.get_feature_names_out())&#10;# Output&#10;# amazing&#9;an&#9;career&#9;current&#9;data&#9;deep&#9;in&#9;is&#9;learning&#9;machine&#9;of&#9;science&#9;subset&#9;the&#9;world&#10;# 0&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.230768&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.324336&#9;0.000000&#9;0.324336&#9;0.324336&#10;# 1&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.243956&#9;0.685743&#9;0.342871&#9;0.342871&#9;0.000000&#9;0.342871&#9;0.000000&#9;0.000000&#10;```&#10;&#10;## Pros and cons of the tf-idf technique&#10;### Advantages&#10;- use this to calculate the similarity between two texts using similarity measures like Cosing similarity/Euclidean distance&#10;- has application in text classification, information retrieval etc.&#10;- better than earlier methods.&#10;### Disadvantages&#10;- high dimentionality&#10;- They are still discrete representation of units of text, hence unable to capture relation between words&#10;- sparse and high dimension&#10;- cannot handle OOV (Out of Vocabulary) words.&#10;---&#10;# TBC&#10;---&#10;https://www.youtube.com/watch?v=tFHeUSJAYbE&amp;list=PLz-ep5RbHosU2hnz5ejezwaYpdMutMVB0&#10;# Large Language Models (LLMs)&#10;is a type of Language Model. Quantatively it is the number of model parameters vary from 10 to 100 billion parameters per model. Qualitatively also called emergent properties starts emerging - properties in large language model that do not appear in small language models, e.g. zero shot learning - capability of a model to complete a task it is not explicitely trained to do.&#10;&#10;In the earlier days the model was trained using supervised learning we use thousands if not millions of examples - but with LLMs we use self-supervised learning. Train a very large model in a very large corpus of data. In self-supervised learning doesn't require manual labelling of each example. The labels or the targets of the model defined from the inherent structure of the data itself.&#10;&#10;One of the popular way of doing this is &quot;next word prediction paradigm&quot;. There is not just one word but many that can go after ***listen to your....***. What the llm would do is to use probablistic distribution of next word given the previous word. in the above example the words could be heart or gut or body or parents etc.. each with different probability distribution. Its essentially trained on a large set of data with so many examples of corpus of data - so it can statistically predict the next set of data. Important thing is the context matters - if for example we add the word ***don't*** in front of ***listen to your...***, the probably distribution will entirely change.&#10;&#10;Autoregression Task formula: ***P(tn | tn-1,..., tn-m)*** P(tn) given n tokens.&#10;&#10;This is how LLMs like chatgpt works.&#10;# 3 levels of Using LLMs&#10;- Level 1: Prompt Engineering&#10;  - using LLM out of the box - not changing any model parameters. Two ways to do this &#10;    - using an agent like chatgpt&#10;    - using open AI API or hugging face tranformers library: help to interact with LLMs programmatically using python for example. Pay per api call in case of open API. Hugging face transformer library is an open source option, you can run the models locally in this case so no need to send your proprietary data into 3rd party or open ai.&#10;- Level 2: Model Fine Tuning&#10;  - adjusting model parameters for a particular tasks.&#10;  - steps&#10;    - Step 1: pre-trained models are obtained. (usually trained by self supervised learning). in this step the base model is learning useful representations for a wide variety of tasks.&#10;    - Step 2: update model parameters given task-specific examples (trained by supervised learning or even reinforcement learning).e.g. chatgpt, the model we use here is a fine tuned model learnt by reinforcement learning. Some techniques is lora or low range adaptation. another technique is reinforcement learning based on human feedback (RLHF).&#10;    - Step 3: Deploy the fine tuned large language model.&#10;- Level 3: Build your own.&#10;  - This is only for 1% of all usecases. &#10;  - One example usecase: in a large company we dont want to use open source models where security is a concern, dont want to send data to 3rd party via an API. &#10;  - Another usecase is you want to create your own model and commercialize it.&#10;  - At a high level steps are:&#10;    - get the data or corpus.&#10;    - pre process and refine it &#10;    - model training&#10;    - pre trained llm.&#10;    - then go to step 2.&#10;&#10;## Connecting to AI using API, Programmatically&#10;### OpenAIs Python API&#10;It's similar to chatGPT but with Python. In both we pass a request and use the language modelling to predict the next word. Apart from the difference in the web interface in chatgpt and here programmatically some differences are as follows. most of the below aren't possible with chatgpt but programmatically possible with openai python.&#10;1) Customizable System message: Message or prompt or a set of instructions that help define the tone, personality and functionality of the model during a conversation. This helps model how to respond to user input and what constraints to follow. I customized the message in chatgpt first to give back sarcastic answers.&#10;![](/blogs/img/posts/chatgpt-customized-system-message.png)&#10;![](/blogs/img/posts/chatgpt-customized-system-message-output.png)&#10;Then i changed the message to give negative and dark response. This time the results were entirely opposite.&#10;![](/blogs/img/posts/chatgpt-customized-system-message-dark.png)&#10;2) Adjust input parameter &#10;  - max response length: response length sent back by model&#10;  - number of responses: (number of outputs you may want to programmatically select one of the response e.g.)&#10;  - temperature: randomness of response generated by the model.&#10;3) Process image and other types&#10;4) extact helful word embeddings for downstream tasks&#10;5) input audio for transcription and tranlations&#10;6) model fine tuning functionality.&#10;7) with chatgpt can only use GPT 3.5 or 4, with openai several other models are available read: https://platform.openai.com/docs/models&#10;### Costing:&#10;Tokens &amp; Pricing:&#10;same as tokenization above a given text is converted and represented as numbers. Pricing is based on the tokens, bigger prompts will incur larger costs. To use we have to get the Secret key to make API calls.&#10;&#10;```python&#10;import openai&#10;from openai import OpenAI&#10;from sk import openai_key # my own file with a variable openai_key='sk-proj-4D1ID8ZeQ...'&#10;&#10;client = OpenAI(api_key=openai_key)&#10;response = client.chat.completions.create(&#10;    model=&quot;gpt-3.5-turbo&quot;,&#10;    max_tokens=2,&#10;    temperature=2, # degree of randomness, 0 is predictable.&#10;    n=3,  &#10;    messages=[&#10;        {&#10;            &quot;role&quot;: &quot;user&quot;, &#10;            &quot;content&quot;: &quot;where there is a will there is a &quot;&#10;        }&#10;    ]&#10;)&#10;&#10;for idx, choice in enumerate(response.choices):&#10;    print(f&quot;Response {idx+1}: {choice['message']['content']}&quot;)&#10;&#10;# Output &#10;# Response 1: way.&#10;# Response 2: plan.&#10;# Response 3: chance.&#10;# note that 2 tokens - 'way' and '.'&#10;```&#10;## Hugging face Transformer library&#10;Major hub for open source Machine learning (ML) like Dockerhub for docker. It has models, dataset (its own data used to train models) and spaces (for building and deploying machine learning applications).&#10;&#10;### Transformers library&#10;Downloading and training machine learning models in python. Like NLP, computer vision, audio processing etc. E.g. for sentiment analysis - find the model that does sentiment analysis classification task then you have to take raw text convert into numerical value that is then passed to the model; finally decode the numerical output of the output to get the label of the text. This can be done easily in the transformers library using a pipeline function. &#10;other things that can be done &#10;- sentiment analysis&#10;- summarization&#10;- translation&#10;- question-answering&#10;- feature extraction &#10;- text generation etc.&#10;&#10;```python&#10;! pip install transformers&#10;from transformers import pipeline&#10;sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;)&#10;# sentiment_pipeline = pipeline(task=&quot;sentiment-analysis&quot;, model=&quot;distilbert/distilbert-base-uncased-finetuned-sst-2-english&quot;)&#10;texts = [&#10;    &quot;&quot;&quot;One is that the mining giant's shares are pushing higher this morning.&#10;        In early trade, the Big Australian's shares are 1.5% higher to $45.74.&#10;        This means that the BHP share price is now up 13% over the past two weeks.&quot;&quot;&quot;&#10;]&#10;results = sentiment_pipeline(texts)&#10;for text, result in zip(texts, results):&#10;    print(f&quot;Text: {text}\nSentiment: {result['label']}, Score: {result['score']}\n&quot;)&#10;```&#10;**Question**: How does it decide if a text is positive or negative without perception?&#10;### signup and logininto huggingface&#10;- lookup for transformer tag and select a model. Then you will check also for pytorch tag. This is because hugging face also supports models which aren't just compatible with pytorch and transformers but also others.&#10;- The train button on the right will have options like Amazon Sagemaker, NVIDIA NDX Cloud, AutoTrain which will help jump start the model finetuning part.&#10;-  &#10;### Getting started&#10;to get started copy the [hf-env.yml](https://github.com/ShawhinT/YouTube-Blog/blob/26dff2786a7d64620e5e7dd71fcd51a416aad1db/LLMs/hugging-face/hf-env.yml) file into your code repository.&#10;&#10;```bash&#10;conda env create --file hf-env.yml&#10;```&#10;another example for text-classification&#10;```python&#10;from transformers import pipeline&#10;classifier = pipeline(task=&quot;text-classification&quot;, model=&quot;SamLowe/roberta-base-go_emotions&quot;, top_k=None)&#10;sentences = [&quot;I am not having a great day&quot;]&#10;model_outputs = classifier(sentences)&#10;print(model_outputs[0])&#10;# Output&#10;# [{'label': 'disappointment', 'score': 0.4666951894760132}, {'label': 'sadness', 'score': 0.39849498867988586}, {'label': 'annoyance', 'score': 0.06806593388319016}, {'label': 'neutral', 'score': 0.05703023821115494}, {'label': 'disapproval', 'score': 0.044239308685064316}, {'label': 'nervousness', 'score': 0.014850745908915997}, {'label': 'realization', 'score': 0.014059904962778091}, {'label': 'approval', 'score': 0.0112674655392766}, {'label': 'joy', 'score': 0.006303396541625261}, {'label': 'remorse', 'score': 0.006221492309123278}, {'label': 'caring', 'score': 0.006029403302818537}, {'label': 'embarrassment', 'score': 0.0052654859609901905}, {'label': 'anger', 'score': 0.004981426056474447}, {'label': 'disgust', 'score': 0.004259029403328896}, {'label': 'grief', 'score': 0.0040021371096372604}, {'label': 'confusion', 'score': 0.003382918192073703}, {'label': 'relief', 'score': 0.0031405005138367414}, {'label': 'desire', 'score': 0.00282747158780694}, {'label': 'admiration', 'score': 0.002815794898197055}, {'label': 'fear', 'score': 0.002707520266994834}, {'label': 'optimism', 'score': 0.0026164911687374115}, {'label': 'love', 'score': 0.0024883910082280636}, {'label': 'excitement', 'score': 0.0024494787212461233}, {'label': 'curiosity', 'score': 0.0023743617348372936}, {'label': 'amusement', 'score': 0.001746696187183261}, {'label': 'surprise', 'score': 0.0014529851032420993}, {'label': 'gratitude', 'score': 0.0006464761681854725}, {'label': 'pride', 'score': 0.00055424973834306}]&#10;```&#10;yet another example for summarization&#10;```python&#10;from transformers import pipeline&#10;summarizer = pipeline(&quot;summarization&quot;, model=&quot;Falconsai/text_summarization&quot;)&#10;ARTICLE = &quot;&quot;&quot; &#10;Hugging Face: Revolutionizing Natural Language Processing&#10;Introduction&#10;In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.&#10;The Birth of Hugging Face&#10;Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name &quot;Hugging Face&quot; was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.&#10;Transformative Innovations&#10;Hugging Face is best known for its open-source contributions, particularly the &quot;Transformers&quot; library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.&#10;Key Contributions:&#10;1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.&#10;2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.&#10;3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.&#10;Democratizing AI&#10;Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.&#10;By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.&#10;Industry Adoption&#10;The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.&#10;Future Directions&#10;Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.&#10;Conclusion&#10;Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.&#10;&quot;&quot;&quot;&#10;print(summarizer(ARTICLE, max_length=1000, min_length=30, do_sample=False))&#10;&gt;&gt;&gt; [{'summary_text': 'Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, the company has left an indelible mark on the industry . The name &quot;Hugging Face&quot; was chosen to reflect the company\'s mission of making AI models more accessible and friendly to humans .'}]&#10;```&#10;Other transformers like ```Falconsai/text_summarization``` to use is the ```facebook/bart-large-cnn``` for text summarization.&#10;&#10;finally, you can chain together multiple objects for example first do a text summarization and then do a sentiment analysis. &#10;Another interesting task is conversational text. For this we can use the ```facebook/blenderbot-400M-distill```. There is supposed to be a class called ```Conversation``` (also imported from transformers) which is supposed to be a container for conversation. &#10;```python&#10;from transformers import pipeline&#10;&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;conversation_history = &quot;Hello, how are you?&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;&#10;# Continue the conversation&#10;conversation_history += f&quot; {response[0]['generated_text']}&quot;&#10;response = chatbot(conversation_history)&#10;print(response)&#10;```&#10;There is a library called **Gradio** to make it conversational. Gradio is very similar to streamlit. &#10;```python&#10;import gradio as gr&#10;from transformers import pipeline&#10;&#10;# Load the chatbot model&#10;chatbot = pipeline(model=&quot;facebook/blenderbot-400M-distill&quot;)&#10;&#10;# Function to handle chatbot conversation&#10;def respond(user_input, history=[]):&#10;    # Add the user input to the conversation history&#10;    history = history or []&#10;    history.append(f&quot;User: {user_input}&quot;)&#10;    print(history)&#10;    # Generate a response&#10;    response = chatbot(user_input)&#10;    bot_reply = response[0]['generated_text']&#10;    print(bot_reply)&#10;&#10;    # Add the bot reply to the history&#10;    history.append(f&quot;Bot: {bot_reply}&quot;)&#10;    &#10;    # Return the entire conversation history as a string&#10;    return &quot;\n&quot;.join(history), history&#10;&#10;# Create the Gradio interface&#10;demo = gr.Interface(&#10;    fn=respond,  # The function that processes input&#10;    inputs=[gr.Textbox(label=&quot;Your Message here:&quot;), gr.State([])],  # Input is a message and conversation history&#10;    outputs=[gr.Textbox(label=&quot;Response here:&quot;), gr.State([])],  # Output is updated conversation and history&#10;    title=&quot;AI Chatbot&quot;&#10;)&#10;&#10;# Launch the interface&#10;demo.launch()&#10;```&#10;![](/blogs/img/posts/gradio-initial.png)&#10;you can post this in hugging face spaces or [hf.co/spaces](hf.co/spaces). They allow to create ML applications and host it here.&#10;example of this [Llama chatbot](https://huggingface.co/spaces/huggingface-projects/llama-3.2-vision-11B).&#10;* Go to hf.co/spaces and click on create new space.&#10;* follow the instructions to clone the repo and push your code.&#10;&#10;# Prompt Engineering&#10;## What is Prompt Engineering&#10;Prompt engineering refers to the process of designing and refining the input (or &quot;prompt&quot;) given to an AI language model, like GPT, to produce desired outputs. It's kind of the future of computer programming in Natural Language. Language models are not designed to peform a task, all that it does is to predict the next token, thus you can trick the model into solving your problem.&#10;Example of a prompt:&#10;```&#10;---&#10;&#10;**Prompt:**&#10;&#10;You are an intelligent system that processes natural language queries and selects the most relevant SQL query from a given list. Based on the user's question, match the correct SQL query that will retrieve the desired information from a database.&#10;&#10;**Input:**&#10;&#10;- **User Query (NLP):** The user asks a question in natural language, describing the data they want from the database.&#10;- **SQL Queries List:** A list of SQL queries is provided as possible answers.&#10;&#10;**Task:**&#10;&#10;- Analyze the user's natural language question.&#10;- Select the most appropriate SQL query from the list that best answers the user's question.&#10;&#10;**Example:**&#10;&#10;- **User Query:** &quot;What are the names and email addresses of all customers who made a purchase in the last 30 days?&quot;&#10;- **SQL Queries List:**&#10;    1. `SELECT * FROM customers WHERE purchase_date &gt; '2023-09-01';`&#10;    2. `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;    3. `SELECT id, name FROM orders WHERE status = 'complete';`&#10;    4. `SELECT email FROM customers WHERE created_at &gt; NOW() - INTERVAL 1 YEAR;`&#10;&#10;**Expected Output:**&#10;&#10;- The system should select query 2: `SELECT name, email FROM customers WHERE purchase_date &gt; NOW() - INTERVAL 30 DAY;`&#10;```&#10;## Two ways of implementing Prompt Enginner&#10;* Easy way - using an Agent like ChatGPT. You can't really use it to integrate it into another app.&#10;* Programmatically integrate using python or similar.&#10;&#10;## 7 Tricks for prompt engineering&#10;1. Be Descriptive - give a context around the problem&#10;2. Give Examples&#10;3. Use Structured Text&#10;    ```&#10;    give me the recipe for making chocolate cookies, give it in the format&#10;    **Title**: Chocolate Cookie Recipe&#10;    **Description**: .......&#10;    ```&#10;5. Chain of Thoughts&#10;    ```&#10;    Make me a resume for a job application at Google.&#10;    Step 1: Write an objective&#10;    Step 2: Write an introduction about my overall work experience. they are...&#10;    Step 3: Write in detail each experience.&#10;    Step 4: Summary and conclusion.&#10;    ```&#10;6. Chatbot personas: &#10;    ```&#10;    Act as an travel guide who knows everything about Sydney. Make me a travel itenaryfor weekend in Sydney in your Aussie Accent.&#10;    ```&#10;7. Flipped Approach:&#10;    The generic response might not be of interest to you hence we have depend on a conversational model. This is useful when you dont know what exactly you want. e.g.&#10;    ```&#10;    I want you to ask me questions to help me come up with an LLM based application idea. Ask me one question at a time to keep things conversational..&#10;    ```&#10;8. Reflective, Review and Refine&#10;&#10;## ChatGPT v/s GPT3.0&#10;ChatGPT is a finetuned model - easy to get useful responses, however with GPT 3.0 that isn't the case and more work is to be done on prompt engineering side - it just does work prediction. &#10;&#10;## LangChain&#10;LangChain is a framework designed to help developers build applications that leverage language models (like GPT) more effectively by integrating them with other tools, data sources, and workflows. It simplifies the process of creating applications that combine various natural language processing tasks with external data, APIs, and user interactions.&#10;```shell&#10;pip install langchain&#10;pip install langchain-community langchain-core&#10;pip install huggingface_hub&#10;```&#10;&#10;```python&#10;from langchain import HuggingFaceHub # or use openai&#10;from langchain.prompts import PromptTemplate&#10;from langchain.chains import LLMChain&#10;import os&#10;&#10;os.environ['HUGGINGFACEHUB_API_TOKEN'] = '&lt;your hf token&gt;'&#10;hugging_face_llm = HuggingFaceHub(repo_id=&quot;google/flan-t5-base&quot;, model_kwargs={&quot;temperature&quot;: 0.5})&#10;&#10;prompt_template = PromptTemplate(&#10;    input_variables=[&quot;question&quot;],&#10;    template=&quot;&quot;&quot;You are the teacher, and you are running a surprise test to see who are the attentive kids. &#10;    The questions will be in the form &#10;    :\nQuestion: {question}&quot;&quot;&quot;&#10;)&#10;&#10;qa_chain = LLMChain(llm=hugging_face_llm, prompt=prompt_template)&#10;question = &quot;What is the capital of France?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;question = &quot;What is the name of indias PM?&quot;&#10;response = qa_chain.run({&quot;question&quot;: question})&#10;print(response)&#10;&#10;# Output&#10;# Paris&#10;# Narendra Modi&#10;```&#10;## Model fine Tuning&#10;A smaller fine tuned model can outperform a larger base model. This involves taking an existing or pre-trained model like GPT 3 for a specific usecase like ChatGPT (GPT-3.5-turbo).&#10;3 ways to fine tune:&#10;1. Self Supervised learning &#10;    - you get the Training Corpus of data can cater to your usecase.&#10;    - you then use this corpus of text and you train the model in a self supervise way.&#10;2. Supervised Learning &#10;    - here you have a set of inputs and outputs e.g. feed in who is the 35th president of the US? and output JFK.&#10;    - So having these question answer pairs we can train the model how to answer questions.&#10;    - One way of doing this is via prompt templates.&#10;      ```text&#10;        Please answer the following questions&#10;        Q: {Question}&#10;        A: {Answer}&#10;      ```&#10;    -  Through this process we could translate the training data set to a series of prompts and generate a training corpus and go back to the self supervised process.&#10;3. Reinforcement Learning -&#10;    - Supervised Fine tunning, two steps:&#10;        - curating your training dataset&#10;        - Fine tuning the model.&#10;        - done in 5 steps:&#10;          1. Choose fine tuning task. it could be anything e.g.&#10;              - could be text summarization&#10;              - could be text generation&#10;              - text/binary classification what ever you want to do..&#10;          2. Prepare training dataset.&#10;              - e.g. if text summarization then the input/output pairs of text in desired summarization generate a training corpus using for e.g. a prompt template &#10;          3. Choose a base model.&#10;              - lots of foundantal llms for e.g. or fine tuned llms.&#10;              - use this as the starting point.&#10;          4. Fine-tune model via supervised learning&#10;              - There are 3 different options:&#10;                  1. retrain all the parameters: here we tweak all the parameters, the computation cost is very very very high.&#10;                  2. transfer learning: here we freeze most of the parameters only fine tune the head. cheaper than full retaining all the parameters.&#10;                  3. Parameter Efficient Fine Tuning (PEFT): here we freeze all the weights or parameters. Instead we augment the model with additional parameters that are trainable. Advantage is we can fine tune the model with a relatively small set of model parameters as against the above approaches.&#10;                  - One of the ways to do this is LoRA (Low Rank Adaptation). In short fine tune model by adding new trainable parameters.&#10;                  ![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*GmCISYhd-JLqHNEvAQU1tQ.png)&#10;                  the first component here h(x) = Wox is what looks like a model without LoRA. Wo has weights that are all trainable. her Wo is a d by k (dxk) matrix with d*k trainable parameters. e.g. d=1000, k=1000 Wo is a 1,000,000 trainable parameters. But with LoRA the ΔWx, another weight matrix with the same shape as Wo. &#10;&#10;                  To simplify things lets just represent ΔW as a product of two terms B and A (BA) hence we can represent ΔW in terms of a 2 one dimentional arrays or vectors A and B, which essentially generates the new h(x).&#10;&#10;                  In this case Wox itself if frozen but B and A are trainable. hence in the above context d = 1000 and k = 1000 hence (d*r)+(r*k) for intrensic rank or r = 2 which translates into 4000 trainable parameters as against the million parameters.&#10;          5. Evaluate the model performance.&#10;    - Train Reward model&#10;        - generating a score for language models completetion. highscore for correct answer and low score for an incorrect answer.&#10;        - start with a prompt pass it into supervised fine tuned model. this you do multiple times and then assign human labels and then use the ranking to train the rewards model. &#10;    - Reinforcement learning with Favourite algorithm&#10;        - example in the case of ChatGPT it uses PPO or Proximal Policy Optimization&#10;        - you give the prompt and pass it into supervised fine tuned model and pass it back to reward model. The reward model then will give feedback to the finetuned model, this is how you update the model parameters. &#10;## Practical Supervised Fine tunning&#10;**First thing first**&#10;These are some of the classes and it's uses.&#10;| Class                                      | Description                                                             |&#10;|--------------------------------------------|-------------------------------------------------------------------------|&#10;| `AutoModel`                                | Automatically loads a pre-trained model for various tasks.             |&#10;| `AutoModelForSequenceClassification`      | Loads a model specifically for sequence classification tasks.           |&#10;| `AutoModelForTokenClassification`         | Loads a model for token classification tasks (e.g., NER).              |&#10;| `AutoModelForQuestionAnswering`           | Loads a model for question answering tasks.                             |&#10;| `AutoModelForCausalLM`                    | Loads a model for causal language modeling tasks (e.g., text generation).|&#10;| `AutoModelForMaskedLM`                    | Loads a model for masked language modeling tasks.                      |&#10;| `AutoModelForImageClassification`         | Loads a model for image classification tasks.                          |&#10;| `AutoTokenizer`                            | Automatically loads a tokenizer corresponding to a pre-trained model.  |&#10;| `AutoFeatureExtractor`                     | Loads a feature extractor for models that require image preprocessing.  |&#10;| `AutoConfig`                               | Loads configuration settings for a model.                              |&#10;| `AutoPipeline`                             | Automatically creates a pipeline for various tasks using a model.     |&#10;| `AutoModelForSpeechSeq2Seq`               | Loads a model for speech-to-text sequence generation tasks.            |&#10;| `AutoModelForAudioClassification`         | Loads a model for audio classification tasks.                          |&#10;| `AutoModelForSeq2SeqLM`                   | Loads a model for sequence-to-sequence tasks (e.g., translation).     |&#10;| `AutoModelForImageSegmentation`           | Loads a model for image segmentation tasks.                            |&#10;| `AutoModelForImageToText`                 | Loads a model for image-to-text generation tasks.                      |&#10;| `AutoModelForTextToImage`                 | Loads a model for text-to-image generation tasks.                      |&#10;| `AutoModelForTextClassification`          | A more general class for text classification tasks.                   |&#10;| `AutoModelForConversational`               | Loads a model designed for conversational tasks.                       |&#10;&#10;&#10;**ChatGPT generated code for Sentiment analysis and then Finetuning using LoRA.**&#10;you can upload your dataset into huggingface like in here.&#10;![](/blogs/img/posts/huggingface-dataset-shawhin.png)&#10;you can access it using &#10;&#10;```python&#10;import torch&#10;from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments&#10;from peft import get_peft_model, LoraConfig, TaskType&#10;&#10;# Load the tokenizer and base model&#10;model_name = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;&#10;tokenizer = AutoTokenizer.from_pretrained(model_name)&#10;model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)&#10;&#10;# Define LoRA configuration&#10;lora_config = LoraConfig(&#10;    r=4,                   # Intrinsic rank&#10;    lora_alpha=32,        # Scaling factor&#10;    lora_dropout=0.01,    # Dropout rate&#10;    target_modules=[&quot;q_lin&quot;],  # Target modules for LoRA&#10;    task_type=TaskType.SEQ_CLS&#10;)&#10;&#10;# Wrap the model with LoRA&#10;lora_model = get_peft_model(model, lora_config)&#10;&#10;# Example training data&#10;train_texts = [&#10;    &quot;It was good.&quot;,&#10;    &quot;Not a fan, don't recommend.&quot;,&#10;    &quot;Better than the first one.&quot;,&#10;    &quot;This is not worth watching even once.&quot;,&#10;    &quot;This one is a pass.&quot;&#10;]&#10;train_labels = [1, 0, 1, 0, 0]  # Example labels corresponding to the texts&#10;&#10;# Example evaluation data&#10;eval_texts = [&#10;    &quot;A fantastic experience!&quot;,&#10;    &quot;Horrible movie, would not watch again.&quot;,&#10;]&#10;eval_labels = [1, 0]  # Example labels for evaluation&#10;&#10;# Tokenize the training and evaluation data&#10;train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, return_tensors=&quot;pt&quot;, max_length=512)&#10;&#10;# Create PyTorch datasets&#10;class SentimentDataset(torch.utils.data.Dataset):&#10;    def __init__(self, encodings, labels):&#10;        self.encodings = encodings&#10;        self.labels = labels&#10;&#10;    def __getitem__(self, idx):&#10;        item = {key: val[idx] for key, val in self.encodings.items()}&#10;        item['labels'] = torch.tensor(self.labels[idx])&#10;        return item&#10;&#10;    def __len__(self):&#10;        return len(self.labels)&#10;&#10;# Create datasets&#10;train_dataset = SentimentDataset(train_encodings, train_labels)&#10;eval_dataset = SentimentDataset(eval_encodings, eval_labels)&#10;&#10;# Define training arguments&#10;training_args = TrainingArguments(&#10;    output_dir=&quot;./lora_finetuned_model&quot;,&#10;    evaluation_strategy=&quot;epoch&quot;,&#10;    learning_rate=5e-5,&#10;    per_device_train_batch_size=2,&#10;    per_device_eval_batch_size=2,  # Add eval batch size&#10;    num_train_epochs=3,&#10;    weight_decay=0.01,&#10;)&#10;&#10;# Define Trainer&#10;trainer = Trainer(&#10;    model=lora_model,&#10;    args=training_args,&#10;    train_dataset=train_dataset,&#10;    eval_dataset=eval_dataset,  # Provide eval dataset&#10;)&#10;&#10;# Fine-tune the model&#10;trainer.train()&#10;&#10;# Save the fine-tuned model&#10;trainer.save_model(&quot;./lora_finetuned_model&quot;)&#10;&#10;&#10;eval_texts = [&#10;    &quot;An absolutely stunning film! The visuals were breathtaking, and the storyline kept me engaged the entire time.&quot;,&#10;    &quot;I was really disappointed with this film. The plot was weak and the characters were poorly developed.&quot;,&#10;    &quot;A heartwarming story that left me in tears. The performances were phenomenal and truly captured the essence of the characters.&quot;,&#10;    &quot;A boring and predictable movie that dragged on for too long. I wouldn't recommend it to anyone.&quot;,&#10;    &quot;This movie exceeded my expectations! The plot twists were fantastic, and the acting was top-notch. Highly recommend!&quot;,&#10;    &quot;The special effects couldn't save this film. It was a chore to sit through, and I found myself looking at my watch constantly.&quot;,&#10;    &quot;An inspiring tale that resonates on many levels. The direction was exceptional, and the soundtrack was unforgettable.&quot;,&#10;    &quot;An absolute disaster! The acting was cringe-worthy and the story made no sense whatsoever.&quot;,&#10;    &quot;A brilliant blend of action and comedy. I couldn't stop laughing, and the action scenes were exhilarating!&quot;,&#10;    &quot;I expected much more from this film. It felt like a cash grab with no real substance or originality.&quot;&#10;]&#10;&#10;# Get predictions&#10;predicted_sentiments = predict_sentiment(eval_texts)&#10;&#10;# Print the results&#10;for text, sentiment in zip(eval_texts, predicted_sentiments):&#10;    print(f&quot;Text: \&quot;{text}\&quot; - Sentiment: {sentiment}&quot;)&#10;```" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/content/posts/system_design/youtube.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/content/posts/system_design/youtube.md" />
              <option name="originalContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'Youtube System Design Interview'&#10;tags = ['Youtube', 'Interview']&#10;+++&#10;&#10;## 1. Requirements Gathering&#10;&#10;### Functional Requirements&#10;- **Video Upload**: Users can upload videos (multiple formats, sizes up to 10GB)&#10;- **Video Streaming**: Users can watch videos with adaptive bitrate streaming&#10;- **Video Search**: Search videos by title, description, tags, channel&#10;- **User Management**: Registration, authentication, profiles, subscriptions&#10;- **Social Features**: Comments, likes/dislikes, sharing, playlists&#10;- **Channel Management**: Create channels, manage content, analytics&#10;- **Content Moderation**: Automated and manual content review&#10;- **Notifications**: Subscription updates, comment replies, trending content&#10;&#10;### Non-Functional Requirements&#10;- **Scale**: 2B+ users, 500+ hours uploaded per minute, 1B+ hours watched daily&#10;- **Availability**: 99.9% uptime (CAP Theorem - prioritize Availability and Partition tolerance)&#10;- **Latency**: &#10;  - Video start time: &lt;2 seconds globally&#10;  - Search results: &lt;300ms&#10;  - Upload processing: Variable based on video size&#10;- **Consistency**: Eventually consistent for social features, strong consistency for user auth&#10;- **Storage**: Exabyte-scale video storage with global distribution&#10;- **Bandwidth**: Petabyte-scale daily traffic&#10;&#10;### Scale Estimation&#10;- **Users**: 2.7B monthly active users&#10;- **Videos**: 720,000 hours uploaded daily&#10;- **Storage**: 1PB+ new content daily&#10;- **Bandwidth**: 30PB+ daily egress traffic&#10;- **QPS**: 1M+ concurrent video streams&#10;&#10;## 2. High-Level Architecture&#10;&#10;```&#10;[CDN Layer] → [Load Balancers] → [API Gateway] → [Microservices]&#10;                                                      ↓&#10;[Message Queue] ← [Video Processing Pipeline] ← [Object Storage]&#10;                                                      ↓&#10;[Search Engine] ← [Metadata Services] → [Analytics Pipeline]&#10;```&#10;&#10;### Core Components&#10;1. **API Gateway** - Request routing, authentication, rate limiting&#10;2. **Video Upload Service** - Handle video ingestion and initial processing&#10;3. **Video Processing Pipeline** - Transcoding, thumbnail generation, ML analysis&#10;4. **Video Streaming Service** - Adaptive bitrate delivery&#10;5. **Metadata Service** - Video information, user data, social interactions&#10;6. **Search Service** - Video discovery and recommendation&#10;7. **User Service** - Authentication, profiles, subscriptions&#10;8. **Analytics Service** - View tracking, performance metrics&#10;9. **Notification Service** - Real-time updates and alerts&#10;&#10;## 2a. Microservice Decomposition &amp; Hexagonal Architecture (Chris Richardson)&#10;&#10;### Decomposition Strategies (from &quot;Microservices Patterns&quot;)&#10;- **By Business Capability**: Decompose services around business domains (e.g., Video Management, User Management, Social Interactions, Analytics).&#10;- **By Subdomain (DDD)**: Identify core, supporting, and generic subdomains (e.g., Video Processing as core, Notification as supporting).&#10;- **By Transaction Boundary**: Services should own their data and transactional boundaries (e.g., Video Upload and Processing as separate services).&#10;- **By Team Ownership**: Align services with team boundaries for independent delivery.&#10;&#10;### Hexagonal Architecture (Ports &amp; Adapters)&#10;- **Service Core**: Business logic is isolated from external systems.&#10;- **Ports**: Define interfaces for driving (API, UI) and driven (DB, messaging, external APIs) adapters.&#10;- **Adapters**: Implement ports for REST, gRPC, Kafka, databases, etc.&#10;- **Benefits**: Improves testability, flexibility, and separation of concerns.&#10;&#10;#### Example: Video Upload Service (Hexagonal)&#10;- **Core**: Handles upload validation, metadata extraction, and orchestration.&#10;- **Inbound Adapter**: REST API for receiving uploads.&#10;- **Outbound Adapters**: Kafka producer for events, S3 adapter for storage, DB adapter for metadata.&#10;&#10;### Additional Patterns from the Book&#10;- **API Composition**: Aggregate data from multiple services for UI.&#10;- **Database per Service**: Each service owns its schema.&#10;- **Saga Pattern**: Manage distributed transactions (e.g., video upload workflow).&#10;- **CQRS**: Separate read/write models for scalability.&#10;- **Event Sourcing**: Persist state changes as events for auditability.&#10;&#10;### References&#10;- Chris Richardson, &quot;Microservices Patterns&quot; (Manning)&#10;- https://microservices.io&#10;&#10;## 3. Scale Cube Application for 10x Growth&#10;&#10;### X-Axis Scaling (Horizontal Duplication)&#10;- **Load Balancers**: Deploy multiple tiers (L4/L7) with auto-scaling&#10;- **API Gateway Clusters**: Regional deployment with intelligent routing&#10;- **Microservice Replicas**: Auto-scaling based on CPU, memory, and queue depth&#10;- **Database Read Replicas**: Multiple read-only instances per region&#10;&#10;### Y-Axis Scaling (Functional Decomposition)&#10;- **Service Decomposition**:&#10;  - Upload Service → Video Ingestion + Metadata Extraction + Storage&#10;  - User Service → Auth + Profile + Subscription + Preferences&#10;  - Social Service → Comments + Likes + Sharing + Community&#10;- **Database Decomposition**: Separate DBs for videos, users, analytics, social&#10;- **Event-Driven Architecture**: Loose coupling via message queues&#10;&#10;### Z-Axis Scaling (Data Partitioning)&#10;- **Video Sharding**: By video ID hash, geographic region, or creator&#10;- **User Sharding**: By user ID hash or geographic region&#10;- **Temporal Sharding**: Hot data (recent) vs cold data (archived)&#10;- **Content-Based Sharding**: By video category, language, or popularity&#10;&#10;## 4. Microservices Design Patterns&#10;&#10;### Service Patterns&#10;- **API Gateway Pattern**: Single entry point with cross-cutting concerns&#10;- **Service Registry &amp; Discovery**: Consul/Eureka for service location&#10;- **Circuit Breaker**: Hystrix for fault tolerance and cascading failure prevention&#10;- **Bulkhead**: Resource isolation between services&#10;- **Retry with Exponential Backoff**: Resilient inter-service communication&#10;&#10;### Data Patterns&#10;- **Database per Service**: Each microservice owns its data&#10;- **Saga Pattern**: Distributed transactions for video upload workflow&#10;- **CQRS**: Separate read/write models for video metadata and analytics&#10;- **Event Sourcing**: Audit trail for user actions and video lifecycle&#10;&#10;### Communication Patterns&#10;- **Asynchronous Messaging**: Kafka for video processing pipeline&#10;- **Request-Response**: HTTP/gRPC for real-time user interactions&#10;- **Publish-Subscribe**: Event notifications for subscriptions&#10;- **Message Routing**: Content-based routing for different video types&#10;&#10;- **Hexagonal Architecture**: Each service is designed using ports and adapters, isolating business logic from infrastructure.&#10;- **Decomposition by Business Capability**: Services are split by domain, following DDD and team boundaries.&#10;- **Saga Pattern**: Used for workflows like video upload and processing.&#10;- **CQRS &amp; Event Sourcing**: Applied for scalability and auditability.&#10;&#10;## 5. Event-Driven Architecture (EDA)&#10;&#10;### Event Streaming Platform&#10;```&#10;Video Upload → [Event Producer] → [Kafka Topics] → [Event Consumers] → Processing Services&#10;```&#10;&#10;### Core Events&#10;- **VideoUploadedEvent**: Triggers transcoding pipeline&#10;- **VideoProcessedEvent**: Updates metadata and makes video available&#10;- **UserActionEvent**: Likes, comments, views for recommendation engine&#10;- **SubscriptionEvent**: Channel subscription/unsubscription&#10;- **ModerationEvent**: Content review results&#10;&#10;### Event Patterns&#10;- **Event Sourcing**: Store all state changes as events&#10;- **CQRS**: Separate command and query responsibility&#10;- **Event Choreography**: Services react to events autonomously&#10;- **Event Orchestration**: Central coordinator for complex workflows&#10;&#10;## 6. CAP Theorem Considerations&#10;&#10;### Design Decisions&#10;- **Partition Tolerance**: Always required in distributed system&#10;- **Availability vs Consistency Trade-offs**:&#10;  - **AP Systems**: Video streaming, comments, likes (eventual consistency)&#10;  - **CP Systems**: User authentication, payment processing&#10;  - **CA Systems**: Single-region components with strong consistency&#10;&#10;### Implementation Strategy&#10;- **Multi-Region Deployment**: Handle network partitions&#10;- **Eventual Consistency**: Social features can tolerate temporary inconsistency&#10;- **Strong Consistency**: Critical operations like user authentication&#10;- **Conflict Resolution**: Last-writer-wins, vector clocks for concurrent updates&#10;&#10;## 7. Storage Architecture&#10;&#10;### Video Storage&#10;- **Object Storage**: S3/GCS for raw and processed video files&#10;- **CDN**: CloudFront/CloudFlare for global content delivery&#10;- **Storage Tiers**: Hot (recent), warm (popular), cold (archived)&#10;- **Compression**: AV1 codec for 30% bandwidth savings&#10;&#10;### Metadata Storage&#10;- **Relational Database**: PostgreSQL for structured data (users, videos)&#10;- **Document Database**: MongoDB for flexible schemas (comments, analytics)&#10;- **Graph Database**: Neo4j for social relationships and recommendations&#10;- **Cache Layer**: Redis for frequently accessed data&#10;&#10;### Search Index&#10;- **Elasticsearch**: Full-text search for videos, channels, playlists&#10;- **Vector Database**: Pinecone for ML-based video recommendations&#10;- **Real-time Indexing**: Stream processing for immediate search availability&#10;&#10;## 8. Video Processing Pipeline&#10;&#10;### Processing Stages&#10;1. **Ingestion**: Upload validation, virus scanning, metadata extraction&#10;2. **Transcoding**: Multiple resolutions, formats, and bitrates&#10;3. **AI Processing**: Content analysis, thumbnail generation, closed captions&#10;4. **Quality Check**: Automated quality assessment and optimization&#10;5. **Distribution**: CDN upload and cache warming&#10;&#10;### Technologies&#10;- **Message Queue**: Apache Kafka for pipeline orchestration&#10;- **Container Orchestration**: Kubernetes for scalable processing&#10;- **Workflow Engine**: Apache Airflow for complex processing workflows&#10;- **ML Platform**: TensorFlow Serving for content analysis&#10;&#10;## 9. Scaling Strategies for 10x Growth&#10;&#10;### Infrastructure Scaling&#10;- **Multi-Cloud**: AWS, GCP, Azure for redundancy and cost optimization&#10;- **Edge Computing**: Process videos closer to users&#10;- **Serverless**: Lambda/Cloud Functions for variable workloads&#10;- **Auto-scaling**: Predictive scaling based on usage patterns&#10;&#10;### Performance Optimization&#10;- **Caching Strategy**: &#10;  - L1: Browser cache (static content)&#10;  - L2: CDN cache (popular videos)&#10;  - L3: Application cache (metadata)&#10;  - L4: Database cache (query results)&#10;&#10;### Data Management&#10;- **Data Archiving**: Move old content to cheaper storage tiers&#10;- **Data Compression**: Advanced codecs and compression algorithms&#10;- **Smart Prefetching**: ML-based content prediction and caching&#10;- **Geographic Optimization**: Content placement based on user location&#10;&#10;## 10. Monitoring and Observability&#10;&#10;### Metrics&#10;- **Golden Signals**: Latency, traffic, errors, saturation&#10;- **Business Metrics**: Video start failures, buffering ratio, user engagement&#10;- **Infrastructure Metrics**: CPU, memory, network, storage utilization&#10;&#10;### Tools&#10;- **Monitoring**: Prometheus, Grafana, DataDog&#10;- **Logging**: ELK Stack (Elasticsearch, Logstash, Kibana)&#10;- **Tracing**: Jaeger, Zipkin for distributed tracing&#10;- **Alerting**: PagerDuty for incident management&#10;&#10;## 11. Security Considerations&#10;&#10;### Content Security&#10;- **DRM**: Widevine, FairPlay for premium content protection&#10;- **Content Filtering**: ML-based inappropriate content detection&#10;- **Access Control**: JWT tokens, OAuth 2.0, rate limiting&#10;&#10;### Infrastructure Security&#10;- **Network Security**: VPC, security groups, WAF&#10;- **Encryption**: TLS in transit, AES-256 at rest&#10;- **Secrets Management**: HashiCorp Vault, AWS Secrets Manager&#10;- **Compliance**: GDPR, COPPA, regional data protection laws&#10;&#10;## 12. Cost Optimization&#10;&#10;### Storage Optimization&#10;- **Intelligent Tiering**: Automatic movement between storage classes&#10;- **Deduplication**: Remove duplicate video segments&#10;- **Compression**: Advanced codecs (AV1, H.265) for bandwidth savings&#10;- **Regional Optimization**: Store content closer to primary audience&#10;&#10;### Compute Optimization&#10;- **Spot Instances**: Use for batch processing jobs&#10;- **Right-sizing**: ML-based instance size recommendations&#10;- **Reserved Capacity**: Long-term commitments for predictable workloads&#10;- **Serverless**: Pay-per-use for variable workloads&#10;&#10;## 13. Disaster Recovery and Business Continuity&#10;&#10;### Backup Strategy&#10;- **Multi-Region Replication**: Critical data replicated across regions&#10;- **Point-in-Time Recovery**: Database snapshots and transaction logs&#10;- **Content Backup**: Multiple copies of popular content&#10;&#10;### Recovery Procedures&#10;- **RTO (Recovery Time Objective)**: 15 minutes for critical services&#10;- **RPO (Recovery Point Objective)**: 5 minutes for user data&#10;- **Failover Automation**: Automated traffic rerouting during outages&#10;- **Chaos Engineering**: Regular disaster simulations&#10;&#10;## 14. Key Principles and Laws Applied&#10;&#10;### Performance Laws&#10;- **Little's Law**: Queue length = arrival rate × response time&#10;- **Amdahl's Law**: Parallel processing limitations&#10;- **Universal Scalability Law**: Overhead of coordination in distributed systems&#10;&#10;### Design Principles&#10;- **Single Responsibility**: Each service has one clear purpose&#10;- **Open/Closed**: Services open for extension, closed for modification&#10;- **Dependency Inversion**: Depend on abstractions, not concretions&#10;- **Fail Fast**: Immediate error detection and reporting&#10;&#10;### Reliability Patterns&#10;- **Bulkhead**: Isolate resources to prevent cascading failures&#10;- **Circuit Breaker**: Prevent calls to failing services&#10;- **Timeout**: Set maximum wait times for all operations&#10;- **Idempotency**: Safe to retry operations multiple times&#10;&#10;### Additional Laws and Principles&#10;- **Murphy's Law**: &quot;Anything that can go wrong will go wrong.&quot; Design for failure and recovery.&#10;- **Conway's Law**: System design mirrors the communication structure of the organization.&#10;- **Occam's Razor**: Prefer the simplest solution that works.&#10;- **Robustness Principle (Postel's Law)**: &quot;Be conservative in what you send, be liberal in what you accept.&quot;&#10;- **Law of Demeter**: Minimize coupling by only interacting with immediate collaborators.&#10;- **Hofstadter's Law**: &quot;It always takes longer than you expect, even when you take into account Hofstadter's Law.&quot;&#10;- **Pareto Principle (80/20 Rule)**: 80% of effects come from 20% of causes; optimize for the critical path.&#10;- **Peter Principle**: In hierarchical organizations, people tend to be promoted to their level of incompetence (impacts team/org design).&#10;- **Gall's Law**: A complex system that works is invariably found to have evolved from a simple system that worked.&#10;&#10;## 15. Database Design&#10;&#10;### User Service Database&#10;```sql&#10;-- Users table&#10;CREATE TABLE users (&#10;    user_id BIGINT PRIMARY KEY,&#10;    username VARCHAR(50) UNIQUE NOT NULL,&#10;    email VARCHAR(255) UNIQUE NOT NULL,&#10;    password_hash VARCHAR(255) NOT NULL,&#10;    created_at TIMESTAMP DEFAULT NOW(),&#10;    last_login TIMESTAMP&#10;);&#10;&#10;-- Channels table&#10;CREATE TABLE channels (&#10;    channel_id BIGINT PRIMARY KEY,&#10;    user_id BIGINT REFERENCES users(user_id),&#10;    channel_name VARCHAR(100) NOT NULL,&#10;    description TEXT,&#10;    subscriber_count BIGINT DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;&#10;-- Subscriptions table (sharded by user_id)&#10;CREATE TABLE subscriptions (&#10;    subscription_id BIGINT PRIMARY KEY,&#10;    subscriber_id BIGINT REFERENCES users(user_id),&#10;    channel_id BIGINT REFERENCES channels(channel_id),&#10;    subscribed_at TIMESTAMP DEFAULT NOW(),&#10;    UNIQUE(subscriber_id, channel_id)&#10;);&#10;```&#10;&#10;### Video Service Database&#10;```sql&#10;-- Videos table (sharded by video_id hash)&#10;CREATE TABLE videos (&#10;    video_id BIGINT PRIMARY KEY,&#10;    channel_id BIGINT NOT NULL,&#10;    title VARCHAR(255) NOT NULL,&#10;    description TEXT,&#10;    duration INTEGER, -- in seconds&#10;    view_count BIGINT DEFAULT 0,&#10;    like_count BIGINT DEFAULT 0,&#10;    dislike_count BIGINT DEFAULT 0,&#10;    upload_time TIMESTAMP DEFAULT NOW(),&#10;    processing_status ENUM('uploading', 'processing', 'ready', 'failed'),&#10;    visibility ENUM('public', 'private', 'unlisted')&#10;);&#10;&#10;-- Video metadata table&#10;CREATE TABLE video_metadata (&#10;    video_id BIGINT PRIMARY KEY REFERENCES videos(video_id),&#10;    file_size BIGINT,&#10;    codec VARCHAR(50),&#10;    resolution VARCHAR(20),&#10;    bitrate INTEGER,&#10;    thumbnail_url VARCHAR(500),&#10;    tags TEXT[] -- PostgreSQL array for tags&#10;);&#10;&#10;-- Comments table (sharded by video_id)&#10;CREATE TABLE comments (&#10;    comment_id BIGINT PRIMARY KEY,&#10;    video_id BIGINT NOT NULL,&#10;    user_id BIGINT NOT NULL,&#10;    parent_comment_id BIGINT, -- for replies&#10;    content TEXT NOT NULL,&#10;    like_count INTEGER DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;```&#10;&#10;## 16. API Design&#10;&#10;### REST API Endpoints&#10;&#10;#### Video Operations&#10;```http&#10;# Upload video&#10;POST /api/v1/videos&#10;Content-Type: multipart/form-data&#10;&#10;# Get video details&#10;GET /api/v1/videos/{videoId}&#10;&#10;# Update video metadata&#10;PUT /api/v1/videos/{videoId}&#10;&#10;# Delete video&#10;DELETE /api/v1/videos/{videoId}&#10;&#10;# Search videos&#10;GET /api/v1/videos/search?q={query}&amp;limit={limit}&amp;offset={offset}&#10;&#10;# Get trending videos&#10;GET /api/v1/videos/trending?category={category}&amp;region={region}&#10;```&#10;&#10;#### User Operations&#10;```http&#10;# User registration&#10;POST /api/v1/users/register&#10;&#10;# User login&#10;POST /api/v1/users/login&#10;&#10;# Get user profile&#10;GET /api/v1/users/{userId}&#10;&#10;# Subscribe to channel&#10;POST /api/v1/users/{userId}/subscriptions/{channelId}&#10;&#10;# Get user subscriptions&#10;GET /api/v1/users/{userId}/subscriptions&#10;```&#10;&#10;#### Social Operations&#10;```http&#10;# Like/Unlike video&#10;POST /api/v1/videos/{videoId}/like&#10;DELETE /api/v1/videos/{videoId}/like&#10;&#10;# Add comment&#10;POST /api/v1/videos/{videoId}/comments&#10;&#10;# Get comments&#10;GET /api/v1/videos/{videoId}/comments?limit={limit}&amp;offset={offset}&#10;&#10;# Reply to comment&#10;POST /api/v1/comments/{commentId}/replies&#10;```&#10;&#10;### GraphQL Schema (Alternative)&#10;```graphql&#10;type Video {&#10;  id: ID!&#10;  title: String!&#10;  description: String&#10;  duration: Int!&#10;  viewCount: Int!&#10;  likeCount: Int!&#10;  uploadTime: String!&#10;  channel: Channel!&#10;  comments(first: Int, after: String): CommentConnection&#10;}&#10;&#10;type Channel {&#10;  id: ID!&#10;  name: String!&#10;  subscriberCount: Int!&#10;  videos(first: Int, after: String): VideoConnection&#10;}&#10;&#10;type Query {&#10;  video(id: ID!): Video&#10;  searchVideos(query: String!, first: Int, after: String): VideoConnection&#10;  trendingVideos(category: String, region: String): [Video!]!&#10;}&#10;&#10;type Mutation {&#10;  uploadVideo(input: VideoInput!): Video&#10;  likeVideo(videoId: ID!): Video&#10;  addComment(videoId: ID!, content: String!): Comment&#10;}&#10;```&#10;&#10;## 17. Caching Strategy&#10;&#10;### Multi-Level Caching&#10;```yaml&#10;# Level 1: Browser Cache&#10;- Static assets: 1 year&#10;- Video thumbnails: 1 week&#10;- API responses: 5 minutes&#10;&#10;# Level 2: CDN Cache (CloudFlare/CloudFront)&#10;- Video segments: 1 day&#10;- Thumbnails: 1 week&#10;- API responses: 1 minute&#10;&#10;# Level 3: Application Cache (Redis)&#10;- Popular video metadata: 1 hour&#10;- User sessions: 24 hours&#10;- Search results: 15 minutes&#10;- Trending videos: 30 minutes&#10;&#10;# Level 4: Database Query Cache&#10;- Complex analytics queries: 5 minutes&#10;- User profile data: 30 minutes&#10;- Channel information: 1 hour&#10;```&#10;&#10;### Cache Invalidation Strategy&#10;- **Time-based**: TTL for most cached data&#10;- **Event-based**: Invalidate on video updates, user actions&#10;- **Version-based**: Cache keys include version numbers&#10;- **Write-through**: Update cache and database simultaneously&#10;- **Write-behind**: Async cache updates for non-critical data&#10;&#10;## 18. Message Queue Architecture&#10;&#10;### Kafka Topic Design&#10;```yaml&#10;# Video Processing Topics&#10;video-upload-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 7 days&#10;&#10;video-transcoding-jobs:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 3 days&#10;&#10;video-ready-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# User Activity Topics&#10;user-view-events:&#10;  partitions: 200&#10;  replication-factor: 3&#10;  retention: 90 days&#10;&#10;user-interaction-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# Notification Topics&#10;subscription-notifications:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 7 days&#10;```&#10;&#10;### Event Schema (Avro)&#10;```json&#10;{&#10;  &quot;type&quot;: &quot;record&quot;,&#10;  &quot;name&quot;: &quot;VideoUploadEvent&quot;,&#10;  &quot;fields&quot;: [&#10;    {&quot;name&quot;: &quot;videoId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;userId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;channelId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;filename&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;fileSize&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;contentType&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;uploadTimestamp&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;metadata&quot;, &quot;type&quot;: {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;string&quot;}}&#10;  ]&#10;}&#10;```&#10;&#10;## 19. Load Balancing Strategy&#10;&#10;### Geographic Load Balancing&#10;```yaml&#10;# DNS-based routing&#10;Global Load Balancer:&#10;  - US-East: 40% traffic&#10;  - US-West: 20% traffic&#10;  - Europe: 25% traffic&#10;  - Asia-Pacific: 15% traffic&#10;&#10;# Regional Load Balancers&#10;Regional LB (L7):&#10;  - Path-based routing: /api/upload → Upload Service&#10;  - Header-based routing: User-Agent → Mobile/Web Service&#10;  - Weighted routing: Canary deployments&#10;&#10;# Service Load Balancers (L4)&#10;Service Discovery:&#10;  - Health checks every 30 seconds&#10;  - Circuit breaker: 5 failures in 60 seconds&#10;  - Load balancing algorithms: Weighted round-robin&#10;```&#10;&#10;### Auto-scaling Configuration&#10;```yaml&#10;# Horizontal Pod Autoscaler (Kubernetes)&#10;Video Upload Service:&#10;  minReplicas: 10&#10;  maxReplicas: 100&#10;  targetCPUUtilization: 70%&#10;  targetMemoryUtilization: 80%&#10;  scaleUpStabilization: 60s&#10;  scaleDownStabilization: 300s&#10;&#10;Video Streaming Service:&#10;  minReplicas: 50&#10;  maxReplicas: 500&#10;  targetCPUUtilization: 60%&#10;  customMetrics:&#10;    - concurrent_streams_per_pod: 1000&#10;```&#10;&#10;## 20. Monitoring and Alerting&#10;&#10;### Key Metrics Dashboard&#10;```yaml&#10;# Golden Signals&#10;Latency:&#10;  - Video start time: P50, P95, P99&#10;  - API response time: P50, P95, P99&#10;  - Upload processing time: P50, P95, P99&#10;&#10;Traffic:&#10;  - Requests per second by endpoint&#10;  - Concurrent video streams&#10;  - Upload requests per minute&#10;&#10;Errors:&#10;  - HTTP error rates (4xx, 5xx)&#10;  - Video processing failures&#10;  - Database connection errors&#10;&#10;Saturation:&#10;  - CPU utilization across services&#10;  - Memory usage patterns&#10;  - Queue depth in Kafka topics&#10;  - Storage utilization&#10;```&#10;&#10;### Alert Rules&#10;```yaml&#10;# Critical Alerts (PagerDuty)&#10;Video Start Failure Rate &gt; 1%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;API Error Rate &gt; 5%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;Database Connection Pool &gt; 90%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;# Warning Alerts (Slack)&#10;Upload Processing Time &gt; P95:&#10;  severity: warning&#10;  notification: 5-minute delay&#10;&#10;CDN Cache Hit Rate &lt; 85%:&#10;  severity: warning&#10;  notification: 10-minute delay&#10;```&#10;&#10;## Conclusion&#10;&#10;This comprehensive YouTube system design demonstrates how to architect a massive-scale video platform that can handle billions of users and petabytes of content. The design incorporates modern distributed systems principles, microservices architecture, event-driven patterns, and advanced scaling techniques.&#10;&#10;The 10x scaling strategy leverages the Scale Cube dimensions, applies CAP theorem principles strategically, and utilizes cutting-edge technologies to ensure the platform remains performant, reliable, and cost-effective as it grows. The architecture is designed to be resilient, observable, and maintainable while providing an excellent user experience globally." />
              <option name="updatedContent" value="+++&#10;date = '2025-05-03T12:44:47+10:00'&#10;draft = false&#10;title = 'Youtube System Design Interview'&#10;tags = ['Youtube', 'Interview']&#10;+++&#10;&#10;Designing a large-scale video sharing platform like YouTube is a classic and challenging system design interview problem. The requirements go far beyond simple video upload and playback: the system must support billions of users, petabytes of data, and millions of concurrent streams, all while delivering a seamless user experience across the globe. This necessitates a deep understanding of distributed systems, scalability, reliability, and cost optimization.&#10;&#10;At its core, such a platform must efficiently handle video ingestion, transcoding, storage, adaptive streaming, search, recommendations, social interactions, and analytics. Each of these domains presents unique technical challenges, from managing exabyte-scale storage and high-throughput data pipelines to ensuring low-latency video delivery and robust content moderation. The architecture must be resilient to failures, support rapid feature evolution, and maintain strong security and compliance standards.&#10;&#10;Modern best practices advocate for a microservices-based approach, as detailed in Chris Richardson’s &quot;Microservices Patterns&quot;. This involves decomposing the system by business capability and subdomain, aligning services with team boundaries, and applying patterns such as hexagonal architecture (ports and adapters), API composition, database-per-service, and event-driven workflows (sagas, CQRS, event sourcing). These strategies enable independent scaling, deployment, and evolution of different system components.&#10;&#10;Additionally, the design must account for fundamental distributed systems laws and principles, including the CAP theorem, Murphy’s Law, Conway’s Law, and the Universal Scalability Law. Applying these principles helps guide trade-offs between consistency, availability, and partition tolerance, and ensures the system is robust in the face of inevitable failures and organizational constraints.&#10;&#10;This document provides a comprehensive, step-by-step breakdown of how to architect a YouTube-scale system, covering requirements, high-level architecture, scaling strategies, microservices patterns, storage, processing pipelines, monitoring, security, and more. The goal is to demonstrate a practical, modern approach to building and scaling a global video platform.&#10;&#10;## 1. Requirements Gathering&#10;&#10;### Functional Requirements&#10;- **Video Upload**: Users can upload videos (multiple formats, sizes up to 10GB)&#10;- **Video Streaming**: Users can watch videos with adaptive bitrate streaming&#10;- **Video Search**: Search videos by title, description, tags, channel&#10;- **User Management**: Registration, authentication, profiles, subscriptions&#10;- **Social Features**: Comments, likes/dislikes, sharing, playlists&#10;- **Channel Management**: Create channels, manage content, analytics&#10;- **Content Moderation**: Automated and manual content review&#10;- **Notifications**: Subscription updates, comment replies, trending content&#10;&#10;### Non-Functional Requirements&#10;- **Scale**: 2B+ users, 500+ hours uploaded per minute, 1B+ hours watched daily&#10;- **Availability**: 99.9% uptime (CAP Theorem - prioritize Availability and Partition tolerance)&#10;- **Latency**: &#10;  - Video start time: &lt;2 seconds globally&#10;  - Search results: &lt;300ms&#10;  - Upload processing: Variable based on video size&#10;- **Consistency**: Eventually consistent for social features, strong consistency for user auth&#10;- **Storage**: Exabyte-scale video storage with global distribution&#10;- **Bandwidth**: Petabyte-scale daily traffic&#10;&#10;### Scale Estimation&#10;- **Users**: 2.7B monthly active users&#10;- **Videos**: 720,000 hours uploaded daily&#10;- **Storage**: 1PB+ new content daily&#10;- **Bandwidth**: 30PB+ daily egress traffic&#10;- **QPS**: 1M+ concurrent video streams&#10;&#10;## 2. High-Level Architecture&#10;&#10;```&#10;[CDN Layer] → [Load Balancers] → [API Gateway] → [Microservices]&#10;                                                      ↓&#10;[Message Queue] ← [Video Processing Pipeline] ← [Object Storage]&#10;                                                      ↓&#10;[Search Engine] ← [Metadata Services] → [Analytics Pipeline]&#10;```&#10;&#10;### Core Components&#10;1. **API Gateway** - Request routing, authentication, rate limiting&#10;2. **Video Upload Service** - Handle video ingestion and initial processing&#10;3. **Video Processing Pipeline** - Transcoding, thumbnail generation, ML analysis&#10;4. **Video Streaming Service** - Adaptive bitrate delivery&#10;5. **Metadata Service** - Video information, user data, social interactions&#10;6. **Search Service** - Video discovery and recommendation&#10;7. **User Service** - Authentication, profiles, subscriptions&#10;8. **Analytics Service** - View tracking, performance metrics&#10;9. **Notification Service** - Real-time updates and alerts&#10;&#10;## 2a. Microservice Decomposition &amp; Hexagonal Architecture (Chris Richardson)&#10;&#10;### Decomposition Strategies (from &quot;Microservices Patterns&quot;)&#10;- **By Business Capability**: Decompose services around business domains (e.g., Video Management, User Management, Social Interactions, Analytics).&#10;- **By Subdomain (DDD)**: Identify core, supporting, and generic subdomains (e.g., Video Processing as core, Notification as supporting).&#10;- **By Transaction Boundary**: Services should own their data and transactional boundaries (e.g., Video Upload and Processing as separate services).&#10;- **By Team Ownership**: Align services with team boundaries for independent delivery.&#10;&#10;### Hexagonal Architecture (Ports &amp; Adapters)&#10;- **Service Core**: Business logic is isolated from external systems.&#10;- **Ports**: Define interfaces for driving (API, UI) and driven (DB, messaging, external APIs) adapters.&#10;- **Adapters**: Implement ports for REST, gRPC, Kafka, databases, etc.&#10;- **Benefits**: Improves testability, flexibility, and separation of concerns.&#10;&#10;#### Example: Video Upload Service (Hexagonal)&#10;- **Core**: Handles upload validation, metadata extraction, and orchestration.&#10;- **Inbound Adapter**: REST API for receiving uploads.&#10;- **Outbound Adapters**: Kafka producer for events, S3 adapter for storage, DB adapter for metadata.&#10;&#10;### Additional Patterns from the Book&#10;- **API Composition**: Aggregate data from multiple services for UI.&#10;- **Database per Service**: Each service owns its schema.&#10;- **Saga Pattern**: Manage distributed transactions (e.g., video upload workflow).&#10;- **CQRS**: Separate read/write models for scalability.&#10;- **Event Sourcing**: Persist state changes as events for auditability.&#10;&#10;### References&#10;- Chris Richardson, &quot;Microservices Patterns&quot; (Manning)&#10;- https://microservices.io&#10;&#10;## 3. Scale Cube Application for 10x Growth&#10;&#10;### X-Axis Scaling (Horizontal Duplication)&#10;- **Load Balancers**: Deploy multiple tiers (L4/L7) with auto-scaling&#10;- **API Gateway Clusters**: Regional deployment with intelligent routing&#10;- **Microservice Replicas**: Auto-scaling based on CPU, memory, and queue depth&#10;- **Database Read Replicas**: Multiple read-only instances per region&#10;&#10;### Y-Axis Scaling (Functional Decomposition)&#10;- **Service Decomposition**:&#10;  - Upload Service → Video Ingestion + Metadata Extraction + Storage&#10;  - User Service → Auth + Profile + Subscription + Preferences&#10;  - Social Service → Comments + Likes + Sharing + Community&#10;- **Database Decomposition**: Separate DBs for videos, users, analytics, social&#10;- **Event-Driven Architecture**: Loose coupling via message queues&#10;&#10;### Z-Axis Scaling (Data Partitioning)&#10;- **Video Sharding**: By video ID hash, geographic region, or creator&#10;- **User Sharding**: By user ID hash or geographic region&#10;- **Temporal Sharding**: Hot data (recent) vs cold data (archived)&#10;- **Content-Based Sharding**: By video category, language, or popularity&#10;&#10;## 4. Microservices Design Patterns&#10;&#10;### Service Patterns&#10;- **API Gateway Pattern**: Single entry point with cross-cutting concerns&#10;- **Service Registry &amp; Discovery**: Consul/Eureka for service location&#10;- **Circuit Breaker**: Hystrix for fault tolerance and cascading failure prevention&#10;- **Bulkhead**: Resource isolation between services&#10;- **Retry with Exponential Backoff**: Resilient inter-service communication&#10;&#10;### Data Patterns&#10;- **Database per Service**: Each microservice owns its data&#10;- **Saga Pattern**: Distributed transactions for video upload workflow&#10;- **CQRS**: Separate read/write models for video metadata and analytics&#10;- **Event Sourcing**: Audit trail for user actions and video lifecycle&#10;&#10;### Communication Patterns&#10;- **Asynchronous Messaging**: Kafka for video processing pipeline&#10;- **Request-Response**: HTTP/gRPC for real-time user interactions&#10;- **Publish-Subscribe**: Event notifications for subscriptions&#10;- **Message Routing**: Content-based routing for different video types&#10;&#10;- **Hexagonal Architecture**: Each service is designed using ports and adapters, isolating business logic from infrastructure.&#10;- **Decomposition by Business Capability**: Services are split by domain, following DDD and team boundaries.&#10;- **Saga Pattern**: Used for workflows like video upload and processing.&#10;- **CQRS &amp; Event Sourcing**: Applied for scalability and auditability.&#10;&#10;## 5. Event-Driven Architecture (EDA)&#10;&#10;### Event Streaming Platform&#10;```&#10;Video Upload → [Event Producer] → [Kafka Topics] → [Event Consumers] → Processing Services&#10;```&#10;&#10;### Core Events&#10;- **VideoUploadedEvent**: Triggers transcoding pipeline&#10;- **VideoProcessedEvent**: Updates metadata and makes video available&#10;- **UserActionEvent**: Likes, comments, views for recommendation engine&#10;- **SubscriptionEvent**: Channel subscription/unsubscription&#10;- **ModerationEvent**: Content review results&#10;&#10;### Event Patterns&#10;- **Event Sourcing**: Store all state changes as events&#10;- **CQRS**: Separate command and query responsibility&#10;- **Event Choreography**: Services react to events autonomously&#10;- **Event Orchestration**: Central coordinator for complex workflows&#10;&#10;## 6. CAP Theorem Considerations&#10;&#10;### Design Decisions&#10;- **Partition Tolerance**: Always required in distributed system&#10;- **Availability vs Consistency Trade-offs**:&#10;  - **AP Systems**: Video streaming, comments, likes (eventual consistency)&#10;  - **CP Systems**: User authentication, payment processing&#10;  - **CA Systems**: Single-region components with strong consistency&#10;&#10;### Implementation Strategy&#10;- **Multi-Region Deployment**: Handle network partitions&#10;- **Eventual Consistency**: Social features can tolerate temporary inconsistency&#10;- **Strong Consistency**: Critical operations like user authentication&#10;- **Conflict Resolution**: Last-writer-wins, vector clocks for concurrent updates&#10;&#10;## 7. Storage Architecture&#10;&#10;### Video Storage&#10;- **Object Storage**: S3/GCS for raw and processed video files&#10;- **CDN**: CloudFront/CloudFlare for global content delivery&#10;- **Storage Tiers**: Hot (recent), warm (popular), cold (archived)&#10;- **Compression**: AV1 codec for 30% bandwidth savings&#10;&#10;### Metadata Storage&#10;- **Relational Database**: PostgreSQL for structured data (users, videos)&#10;- **Document Database**: MongoDB for flexible schemas (comments, analytics)&#10;- **Graph Database**: Neo4j for social relationships and recommendations&#10;- **Cache Layer**: Redis for frequently accessed data&#10;&#10;### Search Index&#10;- **Elasticsearch**: Full-text search for videos, channels, playlists&#10;- **Vector Database**: Pinecone for ML-based video recommendations&#10;- **Real-time Indexing**: Stream processing for immediate search availability&#10;&#10;## 8. Video Processing Pipeline&#10;&#10;### Processing Stages&#10;1. **Ingestion**: Upload validation, virus scanning, metadata extraction&#10;2. **Transcoding**: Multiple resolutions, formats, and bitrates&#10;3. **AI Processing**: Content analysis, thumbnail generation, closed captions&#10;4. **Quality Check**: Automated quality assessment and optimization&#10;5. **Distribution**: CDN upload and cache warming&#10;&#10;### Technologies&#10;- **Message Queue**: Apache Kafka for pipeline orchestration&#10;- **Container Orchestration**: Kubernetes for scalable processing&#10;- **Workflow Engine**: Apache Airflow for complex processing workflows&#10;- **ML Platform**: TensorFlow Serving for content analysis&#10;&#10;## 9. Scaling Strategies for 10x Growth&#10;&#10;### Infrastructure Scaling&#10;- **Multi-Cloud**: AWS, GCP, Azure for redundancy and cost optimization&#10;- **Edge Computing**: Process videos closer to users&#10;- **Serverless**: Lambda/Cloud Functions for variable workloads&#10;- **Auto-scaling**: Predictive scaling based on usage patterns&#10;&#10;### Performance Optimization&#10;- **Caching Strategy**: &#10;  - L1: Browser cache (static content)&#10;  - L2: CDN cache (popular videos)&#10;  - L3: Application cache (metadata)&#10;  - L4: Database cache (query results)&#10;&#10;### Data Management&#10;- **Data Archiving**: Move old content to cheaper storage tiers&#10;- **Data Compression**: Advanced codecs and compression algorithms&#10;- **Smart Prefetching**: ML-based content prediction and caching&#10;- **Geographic Optimization**: Content placement based on user location&#10;&#10;## 10. Monitoring and Observability&#10;&#10;### Metrics&#10;- **Golden Signals**: Latency, traffic, errors, saturation&#10;- **Business Metrics**: Video start failures, buffering ratio, user engagement&#10;- **Infrastructure Metrics**: CPU, memory, network, storage utilization&#10;&#10;### Tools&#10;- **Monitoring**: Prometheus, Grafana, DataDog&#10;- **Logging**: ELK Stack (Elasticsearch, Logstash, Kibana)&#10;- **Tracing**: Jaeger, Zipkin for distributed tracing&#10;- **Alerting**: PagerDuty for incident management&#10;&#10;## 11. Security Considerations&#10;&#10;### Content Security&#10;- **DRM**: Widevine, FairPlay for premium content protection&#10;- **Content Filtering**: ML-based inappropriate content detection&#10;- **Access Control**: JWT tokens, OAuth 2.0, rate limiting&#10;&#10;### Infrastructure Security&#10;- **Network Security**: VPC, security groups, WAF&#10;- **Encryption**: TLS in transit, AES-256 at rest&#10;- **Secrets Management**: HashiCorp Vault, AWS Secrets Manager&#10;- **Compliance**: GDPR, COPPA, regional data protection laws&#10;&#10;## 12. Cost Optimization&#10;&#10;### Storage Optimization&#10;- **Intelligent Tiering**: Automatic movement between storage classes&#10;- **Deduplication**: Remove duplicate video segments&#10;- **Compression**: Advanced codecs (AV1, H.265) for bandwidth savings&#10;- **Regional Optimization**: Store content closer to primary audience&#10;&#10;### Compute Optimization&#10;- **Spot Instances**: Use for batch processing jobs&#10;- **Right-sizing**: ML-based instance size recommendations&#10;- **Reserved Capacity**: Long-term commitments for predictable workloads&#10;- **Serverless**: Pay-per-use for variable workloads&#10;&#10;## 13. Disaster Recovery and Business Continuity&#10;&#10;### Backup Strategy&#10;- **Multi-Region Replication**: Critical data replicated across regions&#10;- **Point-in-Time Recovery**: Database snapshots and transaction logs&#10;- **Content Backup**: Multiple copies of popular content&#10;&#10;### Recovery Procedures&#10;- **RTO (Recovery Time Objective)**: 15 minutes for critical services&#10;- **RPO (Recovery Point Objective)**: 5 minutes for user data&#10;- **Failover Automation**: Automated traffic rerouting during outages&#10;- **Chaos Engineering**: Regular disaster simulations&#10;&#10;## 14. Key Principles and Laws Applied&#10;&#10;### Performance Laws&#10;- **Little's Law**: Queue length = arrival rate × response time&#10;- **Amdahl's Law**: Parallel processing limitations&#10;- **Universal Scalability Law**: Overhead of coordination in distributed systems&#10;&#10;### Design Principles&#10;- **Single Responsibility**: Each service has one clear purpose&#10;- **Open/Closed**: Services open for extension, closed for modification&#10;- **Dependency Inversion**: Depend on abstractions, not concretions&#10;- **Fail Fast**: Immediate error detection and reporting&#10;&#10;### Reliability Patterns&#10;- **Bulkhead**: Isolate resources to prevent cascading failures&#10;- **Circuit Breaker**: Prevent calls to failing services&#10;- **Timeout**: Set maximum wait times for all operations&#10;- **Idempotency**: Safe to retry operations multiple times&#10;&#10;### Additional Laws and Principles&#10;- **Murphy's Law**: &quot;Anything that can go wrong will go wrong.&quot; Design for failure and recovery.&#10;- **Conway's Law**: System design mirrors the communication structure of the organization.&#10;- **Occam's Razor**: Prefer the simplest solution that works.&#10;- **Robustness Principle (Postel's Law)**: &quot;Be conservative in what you send, be liberal in what you accept.&quot;&#10;- **Law of Demeter**: Minimize coupling by only interacting with immediate collaborators.&#10;- **Hofstadter's Law**: &quot;It always takes longer than you expect, even when you take into account Hofstadter's Law.&quot;&#10;- **Pareto Principle (80/20 Rule)**: 80% of effects come from 20% of causes; optimize for the critical path.&#10;- **Peter Principle**: In hierarchical organizations, people tend to be promoted to their level of incompetence (impacts team/org design).&#10;- **Gall's Law**: A complex system that works is invariably found to have evolved from a simple system that worked.&#10;&#10;## 15. Database Design&#10;&#10;### User Service Database&#10;```sql&#10;-- Users table&#10;CREATE TABLE users (&#10;    user_id BIGINT PRIMARY KEY,&#10;    username VARCHAR(50) UNIQUE NOT NULL,&#10;    email VARCHAR(255) UNIQUE NOT NULL,&#10;    password_hash VARCHAR(255) NOT NULL,&#10;    created_at TIMESTAMP DEFAULT NOW(),&#10;    last_login TIMESTAMP&#10;);&#10;&#10;-- Channels table&#10;CREATE TABLE channels (&#10;    channel_id BIGINT PRIMARY KEY,&#10;    user_id BIGINT REFERENCES users(user_id),&#10;    channel_name VARCHAR(100) NOT NULL,&#10;    description TEXT,&#10;    subscriber_count BIGINT DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;&#10;-- Subscriptions table (sharded by user_id)&#10;CREATE TABLE subscriptions (&#10;    subscription_id BIGINT PRIMARY KEY,&#10;    subscriber_id BIGINT REFERENCES users(user_id),&#10;    channel_id BIGINT REFERENCES channels(channel_id),&#10;    subscribed_at TIMESTAMP DEFAULT NOW(),&#10;    UNIQUE(subscriber_id, channel_id)&#10;);&#10;```&#10;&#10;### Video Service Database&#10;```sql&#10;-- Videos table (sharded by video_id hash)&#10;CREATE TABLE videos (&#10;    video_id BIGINT PRIMARY KEY,&#10;    channel_id BIGINT NOT NULL,&#10;    title VARCHAR(255) NOT NULL,&#10;    description TEXT,&#10;    duration INTEGER, -- in seconds&#10;    view_count BIGINT DEFAULT 0,&#10;    like_count BIGINT DEFAULT 0,&#10;    dislike_count BIGINT DEFAULT 0,&#10;    upload_time TIMESTAMP DEFAULT NOW(),&#10;    processing_status ENUM('uploading', 'processing', 'ready', 'failed'),&#10;    visibility ENUM('public', 'private', 'unlisted')&#10;);&#10;&#10;-- Video metadata table&#10;CREATE TABLE video_metadata (&#10;    video_id BIGINT PRIMARY KEY REFERENCES videos(video_id),&#10;    file_size BIGINT,&#10;    codec VARCHAR(50),&#10;    resolution VARCHAR(20),&#10;    bitrate INTEGER,&#10;    thumbnail_url VARCHAR(500),&#10;    tags TEXT[] -- PostgreSQL array for tags&#10;);&#10;&#10;-- Comments table (sharded by video_id)&#10;CREATE TABLE comments (&#10;    comment_id BIGINT PRIMARY KEY,&#10;    video_id BIGINT NOT NULL,&#10;    user_id BIGINT NOT NULL,&#10;    parent_comment_id BIGINT, -- for replies&#10;    content TEXT NOT NULL,&#10;    like_count INTEGER DEFAULT 0,&#10;    created_at TIMESTAMP DEFAULT NOW()&#10;);&#10;```&#10;&#10;## 16. API Design&#10;&#10;### REST API Endpoints&#10;&#10;#### Video Operations&#10;```http&#10;# Upload video&#10;POST /api/v1/videos&#10;Content-Type: multipart/form-data&#10;&#10;# Get video details&#10;GET /api/v1/videos/{videoId}&#10;&#10;# Update video metadata&#10;PUT /api/v1/videos/{videoId}&#10;&#10;# Delete video&#10;DELETE /api/v1/videos/{videoId}&#10;&#10;# Search videos&#10;GET /api/v1/videos/search?q={query}&amp;limit={limit}&amp;offset={offset}&#10;&#10;# Get trending videos&#10;GET /api/v1/videos/trending?category={category}&amp;region={region}&#10;```&#10;&#10;#### User Operations&#10;```http&#10;# User registration&#10;POST /api/v1/users/register&#10;&#10;# User login&#10;POST /api/v1/users/login&#10;&#10;# Get user profile&#10;GET /api/v1/users/{userId}&#10;&#10;# Subscribe to channel&#10;POST /api/v1/users/{userId}/subscriptions/{channelId}&#10;&#10;# Get user subscriptions&#10;GET /api/v1/users/{userId}/subscriptions&#10;```&#10;&#10;#### Social Operations&#10;```http&#10;# Like/Unlike video&#10;POST /api/v1/videos/{videoId}/like&#10;DELETE /api/v1/videos/{videoId}/like&#10;&#10;# Add comment&#10;POST /api/v1/videos/{videoId}/comments&#10;&#10;# Get comments&#10;GET /api/v1/videos/{videoId}/comments?limit={limit}&amp;offset={offset}&#10;&#10;# Reply to comment&#10;POST /api/v1/comments/{commentId}/replies&#10;```&#10;&#10;### GraphQL Schema (Alternative)&#10;```graphql&#10;type Video {&#10;  id: ID!&#10;  title: String!&#10;  description: String&#10;  duration: Int!&#10;  viewCount: Int!&#10;  likeCount: Int!&#10;  uploadTime: String!&#10;  channel: Channel!&#10;  comments(first: Int, after: String): CommentConnection&#10;}&#10;&#10;type Channel {&#10;  id: ID!&#10;  name: String!&#10;  subscriberCount: Int!&#10;  videos(first: Int, after: String): VideoConnection&#10;}&#10;&#10;type Query {&#10;  video(id: ID!): Video&#10;  searchVideos(query: String!, first: Int, after: String): VideoConnection&#10;  trendingVideos(category: String, region: String): [Video!]!&#10;}&#10;&#10;type Mutation {&#10;  uploadVideo(input: VideoInput!): Video&#10;  likeVideo(videoId: ID!): Video&#10;  addComment(videoId: ID!, content: String!): Comment&#10;}&#10;```&#10;&#10;## 17. Caching Strategy&#10;&#10;### Multi-Level Caching&#10;```yaml&#10;# Level 1: Browser Cache&#10;- Static assets: 1 year&#10;- Video thumbnails: 1 week&#10;- API responses: 5 minutes&#10;&#10;# Level 2: CDN Cache (CloudFlare/CloudFront)&#10;- Video segments: 1 day&#10;- Thumbnails: 1 week&#10;- API responses: 1 minute&#10;&#10;# Level 3: Application Cache (Redis)&#10;- Popular video metadata: 1 hour&#10;- User sessions: 24 hours&#10;- Search results: 15 minutes&#10;- Trending videos: 30 minutes&#10;&#10;# Level 4: Database Query Cache&#10;- Complex analytics queries: 5 minutes&#10;- User profile data: 30 minutes&#10;- Channel information: 1 hour&#10;```&#10;&#10;### Cache Invalidation Strategy&#10;- **Time-based**: TTL for most cached data&#10;- **Event-based**: Invalidate on video updates, user actions&#10;- **Version-based**: Cache keys include version numbers&#10;- **Write-through**: Update cache and database simultaneously&#10;- **Write-behind**: Async cache updates for non-critical data&#10;&#10;## 18. Message Queue Architecture&#10;&#10;### Kafka Topic Design&#10;```yaml&#10;# Video Processing Topics&#10;video-upload-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 7 days&#10;&#10;video-transcoding-jobs:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 3 days&#10;&#10;video-ready-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# User Activity Topics&#10;user-view-events:&#10;  partitions: 200&#10;  replication-factor: 3&#10;  retention: 90 days&#10;&#10;user-interaction-events:&#10;  partitions: 100&#10;  replication-factor: 3&#10;  retention: 30 days&#10;&#10;# Notification Topics&#10;subscription-notifications:&#10;  partitions: 50&#10;  replication-factor: 3&#10;  retention: 7 days&#10;```&#10;&#10;### Event Schema (Avro)&#10;```json&#10;{&#10;  &quot;type&quot;: &quot;record&quot;,&#10;  &quot;name&quot;: &quot;VideoUploadEvent&quot;,&#10;  &quot;fields&quot;: [&#10;    {&quot;name&quot;: &quot;videoId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;userId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;channelId&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;filename&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;fileSize&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;contentType&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;uploadTimestamp&quot;, &quot;type&quot;: &quot;long&quot;},&#10;    {&quot;name&quot;: &quot;metadata&quot;, &quot;type&quot;: {&quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;string&quot;}}&#10;  ]&#10;}&#10;```&#10;&#10;## 19. Load Balancing Strategy&#10;&#10;### Geographic Load Balancing&#10;```yaml&#10;# DNS-based routing&#10;Global Load Balancer:&#10;  - US-East: 40% traffic&#10;  - US-West: 20% traffic&#10;  - Europe: 25% traffic&#10;  - Asia-Pacific: 15% traffic&#10;&#10;# Regional Load Balancers&#10;Regional LB (L7):&#10;  - Path-based routing: /api/upload → Upload Service&#10;  - Header-based routing: User-Agent → Mobile/Web Service&#10;  - Weighted routing: Canary deployments&#10;&#10;# Service Load Balancers (L4)&#10;Service Discovery:&#10;  - Health checks every 30 seconds&#10;  - Circuit breaker: 5 failures in 60 seconds&#10;  - Load balancing algorithms: Weighted round-robin&#10;```&#10;&#10;### Auto-scaling Configuration&#10;```yaml&#10;# Horizontal Pod Autoscaler (Kubernetes)&#10;Video Upload Service:&#10;  minReplicas: 10&#10;  maxReplicas: 100&#10;  targetCPUUtilization: 70%&#10;  targetMemoryUtilization: 80%&#10;  scaleUpStabilization: 60s&#10;  scaleDownStabilization: 300s&#10;&#10;Video Streaming Service:&#10;  minReplicas: 50&#10;  maxReplicas: 500&#10;  targetCPUUtilization: 60%&#10;  customMetrics:&#10;    - concurrent_streams_per_pod: 1000&#10;```&#10;&#10;## 20. Monitoring and Alerting&#10;&#10;### Key Metrics Dashboard&#10;```yaml&#10;# Golden Signals&#10;Latency:&#10;  - Video start time: P50, P95, P99&#10;  - API response time: P50, P95, P99&#10;  - Upload processing time: P50, P95, P99&#10;&#10;Traffic:&#10;  - Requests per second by endpoint&#10;  - Concurrent video streams&#10;  - Upload requests per minute&#10;&#10;Errors:&#10;  - HTTP error rates (4xx, 5xx)&#10;  - Video processing failures&#10;  - Database connection errors&#10;&#10;Saturation:&#10;  - CPU utilization across services&#10;  - Memory usage patterns&#10;  - Queue depth in Kafka topics&#10;  - Storage utilization&#10;```&#10;&#10;### Alert Rules&#10;```yaml&#10;# Critical Alerts (PagerDuty)&#10;Video Start Failure Rate &gt; 1%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;API Error Rate &gt; 5%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;Database Connection Pool &gt; 90%:&#10;  severity: critical&#10;  notification: immediate&#10;&#10;# Warning Alerts (Slack)&#10;Upload Processing Time &gt; P95:&#10;  severity: warning&#10;  notification: 5-minute delay&#10;&#10;CDN Cache Hit Rate &lt; 85%:&#10;  severity: warning&#10;  notification: 10-minute delay&#10;```&#10;&#10;## Conclusion&#10;&#10;This comprehensive YouTube system design demonstrates how to architect a massive-scale video platform that can handle billions of users and petabytes of content. The design incorporates modern distributed systems principles, microservices architecture, event-driven patterns, and advanced scaling techniques.&#10;&#10;The 10x scaling strategy leverages the Scale Cube dimensions, applies CAP theorem principles strategically, and utilizes cutting-edge technologies to ensure the platform remains performant, reliable, and cost-effective as it grows. The architecture is designed to be resilient, observable, and maintainable while providing an excellent user experience globally." />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>